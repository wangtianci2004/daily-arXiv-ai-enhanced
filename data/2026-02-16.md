<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]
- [cs.RO](#cs.RO) [Total: 38]
- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 56]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: 本文推出了 GT-HarmBench，这是一个包含 2,009 个博弈论场景的基准测试，旨在评估前沿 AI 模型在多智能体环境下的安全风险，结果显示模型在社会协作方面存在显著的可靠性差距。


<details>
  <summary>Details</summary>
Motivation: 现有的 AI 安全基准主要侧重于单智能体评估，缺乏对多智能体环境下特有风险（如协调失败、利益冲突）的深入理解。随着 AI 系统越来越多地部署在复杂的多智能体高风险环境中，亟需一个评估其社会效益决策能力的标准化基准。

Method: 研究者构建了包含 2,009 个高风险场景的基准测试集 **GT-HarmBench**。这些场景基于经典博弈论结构（如囚徒困境、猎鹿博弈、胆小鬼博弈），并结合 MIT AI Risk Repository 中的现实风险背景。研究通过对 15 种前沿模型进行测试，评估了其在不同提示词框架（Framing）和顺序下的决策一致性，并分析了其推理模式。

Result: 1. **社交益处达成率低**：前沿模型在仅 62% 的情况下选择了对社会有益的行动，频繁导致有害后果。2. **鲁棒性差**：模型的决策对博弈论提示词的措辞（Framing）和顺序具有高度敏感性。3. **干预有效性**：通过特定的博弈论干预手段，可将社会有益结果的比例提升高达 18%。

Conclusion: GT-HarmBench 揭示了前沿 AI 模型在多智能体对齐方面存在严重的可靠性缺陷。该基准测试为研究多智能体环境下的协作、冲突及安全性提供了标准化的评估框架和代码库。

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [2] [Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation](https://arxiv.org/abs/2602.12544)
*Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Creighton Glasscock,Honglak Lee*

Main category: cs.AI

TL;DR: 本文通过引入基于约束的细粒度评估框架，实现了Web智能体高质量训练数据的自动化规模化生成，并利用部分成功轨迹显著提升了小型学生模型在复杂预订任务上的性能。


<details>
  <summary>Details</summary>
Motivation: Web智能体训练面临高质量数据获取困难的挑战，核心症结在于难以准确量化任务执行过程中的进度（轨迹评估），导致大量潜在的有用数据（如部分成功的轨迹）被浪费。

Method: 提出了一种基于约束（constraint-based）的评估框架，用于对任务进度进行细粒度评估；同时构建了名为BookingArena的新基准，涵盖20个流行网站的复杂预订任务，并通过该框架生成的增强数据训练蒸馏学生模型。

Result: 实验表明，使用该流水线训练的蒸馏学生模型在性能上优于现有的开源方法，且在模型体积更小的情况下，其表现可媲美或超越现有的商业系统。

Conclusion: 该研究有效解决了Web交互数据集生成效率低下和多样性不足的问题，通过创新的评估方法论，证明了利用细粒度反馈训练小型高效Web智能体的可行性。

Abstract: We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.

</details>


### [3] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: 本文研究了多领域可验证奖励强化学习（RLVR）的训练范式，对比了混合训练与独立训练后合并的效果，发现推理密集型任务间存在显著的协同增益，并揭示了其内在作用机制。


<details>
  <summary>Details</summary>
Motivation: 虽然可验证奖励强化学习（RLVR）在数学和编程等特定领域提升了 LLM 的推理能力，但如何高效地构建跨多领域的专家级模型仍存在挑战。目前领域内主要存在“混合多任务训练”和“分任务训练后合并”两种策略，但缺乏对这两者效果的系统性对比、分析及其内部协同机制的深入探讨。

Method: 研究者选择了数学、编程、科学和指令遵循等多个高难度领域，利用开源数据集设计了详尽的定性和定量实验。他们对比了两种主流范式：1) 混合多任务 RLVR（Mixed multi-task RLVR）；2) 独立 RLVR 后进行模型合并（Separate RLVR followed by model merging）。此外，还从权重空间几何（weight space geometry）、模型预测行为和信息约束三个维度深入分析了模型提升的内部机制。

Result: 实验发现，跨领域的 RLVR 训练几乎没有表现出相互干扰，反而在推理密集型领域（如数学和科学）之间观察到了显著的相互协同（Synergistic）效应。通过机制分析，研究揭示了这些增益在权重空间布局和预测一致性方面的体现。

Conclusion: 该研究（M2RL项目）证明了在大语言模型中开展多领域 RLVR 训练的可行性与优势。研究强调，推理密集型任务之间存在正向增益，这为构建高性能的多领域专家模型提供了重要的策略指导，即无论是通过混合训练还是模型合并，都能在不损失性能的前提下实现能力的协同提升。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [4] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: 本文提出 McDiffuSE 框架，通过蒙特卡洛树搜索（MCTS）优化掩码扩散模型的插值顺序，显著提升了数学与代码推理任务的生成性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）在处理数学和代码推理任务时，其“计划与插值”解码性能对槽位的填充顺序高度敏感，导致输出结果存在巨大的方差，限制了生成质量。

Method: 提出 McDiffuSE 框架，将槽位选择（slot selection）建模为决策过程。该方法利用蒙特卡洛树搜索（MCTS）进行前瞻（look-ahead）模拟，在正式提交生成内容前评估局部补全质量，从而在复杂的组合空间中搜索并优化插值顺序。

Result: McDiffuSE 在多个基准测试中表现优异：相比自回归模型平均提升 3.2%，相比基础插值模型提升 8.0%。特别是在 MBPP 任务上提升了 19.5%，在 MATH500 上提升了 4.9%。

Conclusion: MCTS 规划是提升掩码扩散模型生成质量的有效方法。研究强调了非顺序生成路径对性能提升的必要性，并指出在搜索过程中，调优探索常数以克服模型置信度偏见比单纯增加模拟次数更为关键。

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [5] [GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics](https://arxiv.org/abs/2602.12617)
*Modi Jin,Yiming Zhang,Boyuan Sun,Dingwen Zhang,MingMing Cheng,Qibin Hou*

Main category: cs.AI

TL;DR: 本文提出了 GeoAgent，通过引入专家标注的 GeoSeek 数据集以及定制化的地理相似度与一致性奖励机制，解决了地理定位中 AI 生成推理数据不可靠的问题，实现了更精准且符合人类逻辑的细粒度地址推断。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习（RL）的地理定位方法存在两个主要缺陷：一是过度依赖 AI 生成的思维链（CoT）数据，二是其训练策略与地理学科的固有特征相冲突，导致模型推理过程缺乏准确性和专业性。

Method: 1. 引入 **GeoSeek 数据集**：由地理专家和专业玩家标注的包含思维链（CoT）的高质量地理定位数据。
2. 提出 **地理相似度奖励（Geo-similarity reward）**：根据地理特征引导模型收敛。
3. 提出 **一致性奖励（Consistency reward）**：利用一个独立的一致性智能体（Consistency Agent）评估推理过程的完整性与逻辑一致性。

Result: 实验结果表明，GeoAgent 在多个空间粒度上的性能均优于现有的先进方法及一系列通用的多模态大模型（VLLMs），并且其生成的推理逻辑与人类专家的思维高度对齐。

Conclusion: GeoAgent 证明了通过引入专家领域知识（GeoSeek 数据集）以及设计符合地理逻辑的奖励函数（相似度与一致性奖励），可以显著提升大模型在复杂地理推理任务中的表现，使其推理逻辑更加接近人类专家。

Abstract: This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.

</details>


### [6] [AI Agents for Inventory Control: Human-LLM-OR Complementarity](https://arxiv.org/abs/2602.12631)
*Jackie Baek,Yaopeng Fu,Will Ma,Tianyi Peng*

Main category: cs.AI

TL;DR: 本文研究了运筹学算法、大语言模型（LLM）与人类在多周期库存控制中的协同作用。通过新提出的InventoryBench基准测试，证明了结合OR与LLM的方法优于单一模型，且实验表明人机协作能显著提升决策利润，并从数学和实证角度验证了AI对个体决策的广泛增益。


<details>
  <summary>Details</summary>
Motivation: 传统的运筹学（OR）算法在库存控制中依赖于僵化的建模假设，在面对需求分布偏移或缺乏上下文信息时表现不佳。尽管大语言模型（LLM）具备灵活推理能力，但如何将其有效地整合进传统决策流程中尚不明确，且人机协作在库存决策中的实际效果有待验证。

Method: 1. 构建了InventoryBench基准测试集，包含1000多个涵盖合成及真实需求数据（包含季节性、需求偏移及前置时间不确定性）的实例。
2. 开发了OR增强型LLM决策方法（OR-augmented LLM）。
3. 开展受控课堂实验，评估LLM建议对人类决策管道（Human-in-the-loop）的影响。
4. 数学化定义了“个体层面互补效应”，并推导出一个分布自由（distribution-free）的下界，用于量化从AI协作中获益的人群比例。

Result: 1. **算法性能**：OR增强型LLM方法的表现优于单纯的OR算法或单纯的LLM。
2. **协作效应**：人机协作团队获得的平均利润高于人类独立决策或AI独立决策。
3. **个体增益**：通过推导的下界及实证发现，绝大部分个体在与AI协作时都能获得决策能力的提升，这一结论修正了以往“人机协作可能降低性能”的观点。

Conclusion: LLM与传统的OR算法在库存管理中并非替代关系，而是互补关系。通过OR增强的LLM方法以及“人机协作”模式，能够有效应对复杂、多变的需求环境，提升决策效益。研究通过数学形式化证明了AI对个体决策能力的普遍增益。

Abstract: Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.
  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.
  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.

</details>


### [7] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: CogRouter 是一个让 LLM 智能体能够根据任务步骤动态调整“思考深度”的训练框架，旨在平衡复杂决策中的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 智能体在多轮决策任务中通常采用固定的认知模式（要么不思考直接输出，要么统一深度思考）。这种“一刀切”的方法在长程任务中效率低下，因为不同步骤对认知的需求不同（有些仅需常规执行，有些需战略规划）。

Method: 1. **认知层级设计**：基于 ACT-R 理论设计从直觉反应到策略规划的四个层级认知水平。
2. **两阶段训练 (CoSFT & CoPO)**：首先通过认知感知监督微调 (CoSFT) 注入层级化认知模式；随后利用认知感知策略优化 (CoPO) 通过置信度感知的优势重权进行步骤级信用分配。
3. **动态路由核心逻辑**：以最大化生成动作的置信度作为选择合适认知深度的基准。

Result: 在 ALFWorld 和 ScienceWorld 基准测试中，CogRouter (基于 Qwen2.5-7B) 达到了 82.3% 的成功率。在性能上超过了 GPT-4o (+40.3%)、OpenAI-o3 (+18.3%) 和 GRPO (+14.0%)，同时在 Token 使用量上减少了 62%。

Conclusion: CogRouter 证明了在多轮决策任务中，根据步骤需求动态调整认知深度比固定认知模式更具优势。该框架不仅显著提升了任务成功率，还极大地提高了推理效率，为构建高效、智能的自主 Agent 提供了新路径。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [8] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: 本文提出了一个针对2-SAT问题的结构化诊断基准，通过参数化生成具有特定逻辑特征的公式，揭示了LLM在逻辑推理中存在的、被传统基准测试掩盖的结构性脆弱与脆性。


<details>
  <summary>Details</summary>
Motivation: 标准的SAT基准测试往往将表面难度（如长度、措辞、子句顺序）与决定满足性的内在结构特征混淆，导致难以区分模型是基于表面模式识别还是真正的逻辑推理进行决策。需要一种能够隔离结构变量、精确定位推理瓶颈的测试方法。

Method: 构建了一个基于2-SAT问题的诊断基准，利用蕴含图（implication graph）生成五类具有可解释结构的公式：1) 矛盾环（UNSAT核心）；2) 自由变量控制（解空间大小）；3) 预设骨干（测试传播能力）；4) 桥接子句（测试顺序敏感性）；5) 对称性与重复。通过决策准确率、赋值有效性及语义保持扰动（重命名、重排序等）下的鲁棒性来评估LLM。

Result: 即使在表面统计数据（如子句数量）保持一致的情况下，针对性的逻辑结构干预也会导致LLM性能出现急剧下降。模型在面对子句重测序、变量重命名等语义保持的扰动时表现出明显的不稳定性，揭示了隐藏在聚合准确率背后的“脆弱区间”。

Conclusion: 大语言模型在逻辑推理中表现出高度的脆弱性，传统的聚合准确率指标无法有效评估其推理能力的可靠性。通过结构化、参数化的诊断工具，可以更深入地理解并揭示模型在处理特定逻辑拓扑时的失效模式。

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [9] [SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks](https://arxiv.org/abs/2602.12670)
*Xiangyi Li,Wenbo Chen,Yimin Liu,Shenghan Zheng,Xiaokun Chen,Yifeng He,Yubo Li,Bingran You,Haotian Shen,Jiankai Sun,Shuyi Wang,Qunhong Zeng,Di Wang,Xuandong Zhao,Yuanli Wang,Roey Ben Chaim,Zonglin Di,Yipeng Gao,Junwei He,Yizhuo He,Liqiang Jing,Luyang Kong,Xin Lan,Jiachen Li,Songlin Li,Yijiang Li,Yueqian Lin,Xinyi Liu,Xuanqing Liu,Haoran Lyu,Ze Ma,Bowei Wang,Runhui Wang,Tianyu Wang,Wengao Ye,Yue Zhang,Hanwen Xing,Yiqi Xue,Steven Dillmann,Han-chung Lee*

Main category: cs.AI

TL;DR: 本文提出 SkillsBench 基准，评估了结构化知识包（Agent Skills）对 LLM 智能体的影响，发现人工精选技能可显著提升性能（特别是对于小模型），但模型尚无法有效自生成这些技能。


<details>
  <summary>Details</summary>
Motivation: 尽管 Agent Skills（用于增强大模型推理能力的结构化程序性知识包）已被广泛采用，但目前缺乏标准化的评估方法来衡量这些技能是否真正提升了模型性能。

Method: 开发了 SkillsBench 基准测试，包含 11 个领域的 86 个任务，并配备确定性验证器。实验采用了对比分析法，在 7 种模型配置上针对 7,308 条轨迹测试了三种条件：无技能（No Skills）、人工精选技能（Curated Skills）和自生成技能（Self-generated Skills）。

Result: 1. 人工精选技能使平均通过率提升 16.2%，其中医疗领域提升最高（+51.9%），软件工程最低（+4.5%）。2. 自生成技能平均而言没有带来任何收益，表明模型目前无法可靠地编写其受益所需的知识。3. 包含 2-3 个模块的精简技能效果优于详尽文档。4. 具备技能的小模型性能可比肩不具备技能的大模型。

Conclusion: Agent Skills 是提升大模型智能体执行复杂任务能力的有效手段，但其有效性高度依赖于技能的质量和结构。目前的模型尚未具备可靠的“自我进化”能力（自生成技能无效），而高质量、精简的程序性知识能够缩小模型规模带来的性能差距。

Abstract: Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

</details>


### [10] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: 本文提出了 X-SYS 参考架构，旨在解决可解释 AI 在实际部署中的系统工程难题，并通过 SemanticLens 系统展示了如何构建具备高响应性、可扩展且易维护的交互式解释系统。


<details>
  <summary>Details</summary>
Motivation: 尽管 XAI 算法层出不穷，但将其部署为实际系统仍面临巨大挑战。交互式解释系统需要处理复杂的重复查询、动态演进的模型与数据以及合规性约束，目前缺乏能够连接算法与用户界面的系统级架构指导。

Method: 提出了 X-SYS 参考架构，将 XAI 视为信息系统问题。该架构定义了 **STAR**（可扩展性、可追溯性、响应性、适应性）四大质量属性，并采用了五层组件分解模型：交互界面服务（XUI）、解释服务、模型服务、数据服务以及编排与治理。该架构通过映射交互模式与系统能力，实现了前端 UI 与后端计算的解耦。

Result: 通过开发 **SemanticLens** 系统（针对视觉语言模型的语义搜索和激活引导工具）验证了 X-SYS 的有效性。实验证明，该架构通过服务边界定义实现了组件独立演进，通过离/在线分离确保了响应速度，并通过状态管理支持了系统可追溯性。

Conclusion: 将 XAI 运行化（Operationalizing）必须从信息系统工程的角度出发。X-SYS 为开发者提供了一个可复用的蓝图，使交互式解释系统能够在满足运营约束的前提下实现端到端的设计与部署。

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [11] [WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning](https://arxiv.org/abs/2602.12852)
*Junjie Wang,Zequn Xie,Dan Yang,Jie Feng,Yue Shen,Duolin Sun,Meixiu Long,Yihan Jiao,Zhehao Tan,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: WebClipper通过图剪枝技术压缩Web Agent的冗余搜索轨迹，在减少20%调用成本的同时提升了任务准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的开源Web Agent在执行复杂搜索任务时效率低下，通常伴随着冗长的工具调用轨迹、循环推理以及对无效分支的过度探索。

Method: 提出WebClipper框架，将Agent的搜索过程建模为状态图，并将其轨迹优化抽象为“最小必要有向无环图（DAG）挖掘”问题。通过剪枝技术剔除冗余步骤后，利用提纯后的轨迹对模型进行持续训练。同时，引入F-AE Score作为衡量准确率与效率平衡性的新指标。

Result: 在保持或提升准确率的同时，WebClipper成功将工具调用轮次（tool-call rounds）减少了约20%，显著提升了搜索效率。

Conclusion: WebClipper证明了通过轨迹压缩与精炼训练，可以有效进化Web Agent的搜索模式，为在复杂信息搜索任务中平衡“有效性”与“效率”提供了新的技术路径和评估标准。

Abstract: Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.

</details>


### [12] [BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2602.12876)
*Huanyao Zhang,Jiepeng Zhou,Bo Li,Bowen Zhou,Yanzhe Dan,Haishan Lu,Zhiyong Cao,Jiaoyang Chen,Yuqian Han,Zinan Sheng,Zhengwei Tao,Hao Liang,Jialong Wu,Yang Shi,Yuanpeng He,Jiaye Lin,Qintong Zhang,Guochen Yan,Runhao Zhao,Zhengpin Li,Xiaohan Yu,Lang Mei,Chong Chen,Wentao Zhang,Bin Cui*

Main category: cs.AI

TL;DR: 本文推出了 BrowseComp-$V^3$ 基准测试和 OmniSeeker 智能体框架，旨在评估和提升 MLLM 在复杂多模态网页浏览中的深度搜索能力，研究揭示了当前顶尖模型在处理此类任务时仍面临巨大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态网页浏览基准在任务复杂度、证据获取的可重复性以及评估粒度方面存在局限，无法有效衡量多模态大模型（MLLM）在开放环境下进行深度搜索和复杂推理的能力。

Method: 1. 构建 **BrowseComp-$V^3$** 基准：包含300个跨领域的高难度问题，要求多层级、跨模态的多跳推理。
2. 引入**过程评估机制**：通过专家验证的子目标驱动机制，对中间推理步骤进行细粒度分析。
3. 提出 **OmniSeeker** 框架：一种集成了网页搜索与视觉感知工具的统一多模态浏览智能体架构。

Result: 实验表明，即使是目前最先进的模型（SOTA）在 BrowseComp-$V^3$ 上的准确率也仅为 36%，显示出模型在整合跨模态证据和处理细粒度感知任务时的严重不足。

Conclusion: 当前最先进的模型在处理真实世界的多模态深度搜索任务时仍存在显著差距，尤其是在多模态信息的深度整合和细粒度感知方面存在瓶颈。

Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.

</details>


### [13] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: 本文研究了大型推理模型在多轮对抗攻击下的鲁棒性，发现其虽优于普通模型但仍存在严重漏洞，并揭示了传统置信度防御机制在推理模型上的失效现象。


<details>
  <summary>Details</summary>
Motivation: 尽管具备推理能力的大模型在复杂任务中取得了顶尖表现，但它们在多轮对抗压力下的鲁棒性尚未得到充分研究，需要明确推理过程是否能有效抵御对抗性诱导。

Method: 对9种前沿大推理模型（LRMs）在多轮对抗攻击下的表现进行了评估，通过轨迹分析（Trajectory Analysis）识别其失效模式，并测试了置信度感知响应生成（CARG）防御机制在推理模型上的有效性。

Result: 1. 推理模型优于指令微调基准，但仍存在特定漏洞，误导性建议和社交压力对其普遍有效；2. 识别出五种失效模式（自我怀疑、社交顺从、建议劫持、情绪敏感及推理疲劳），其中前两者占失败案例的50%；3. 传统的CARG防御因模型过度自信而失效，且随机置信度嵌入的效果反常地优于针对性提取。

Conclusion: 推理能力的提升并不等同于对抗鲁棒性的自动增强。由于长链推理会导致模型产生过度自信，现有的基于置信度的防御机制（如CARG）在推理模型上表现不佳，必须针对推理模型的特性进行根本性的重新设计。

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [14] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出受限假设辩论（CABA）框架，通过引入受限变量解决了传统 ABA 无法处理非基项论证的限制，实现了对标准 ABA 语义的保守泛化。


<details>
  <summary>Details</summary>
Motivation: 传统的假设辩论（ABA）框架主要基于命题原子或基项（ground）语言，这种限制使得它们无法直接处理包含变量的论证和攻击，从而限制了其在处理复杂逻辑和无限领域问题时的适用性。

Method: 通过提升（lift）现有 ABA 的局限性，提出了一种新型的受限假设辩论框架（CABA）。该方法允许组件和论证包含在可能无限的域中取值的受限变量，并基于多种非基项攻击（non-ground attacks）概念定义了非基项语义。

Result: 成功定义了 CABA 的非基项语义，并证明了该语义在处理无变量情况时能够保守地还原为标准的 ABA 语义，实现了对传统框架的兼容与扩展。

Conclusion: CABA 是对标准 ABA 框架的有效扩展和保守泛化（Conservative Generalisation），通过引入约束变量显著增强了其在复杂建模场景下的表达能力，同时保持了与原有语义的一致性。

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [15] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: 本文提出一种结合模糊规则系统（FRBS）与最优控制的无人机避障框架，通过模糊逻辑自适应调整安全约束，旨在提高决策的可解释性与计算效率，但在实验中发现了求解器工具箱的软件兼容性限制。


<details>
  <summary>Details</summary>
Motivation: 针对经典最优控制在处理不确定性时的局限性，以及航空安全关键系统中对决策可解释性的需求。旨在通过选择性激活避障更新来减少不必要的计算开销，同时确保飞行路径符合航空间隔标准和适航指南。

Method: 设计了一个三阶段的 Takagi-Sugeno-Kang (TSK) 模糊逻辑层，用于根据 FAA 和 EASA 的航空规章动态调整避障半径、紧急程度和激活决策。这些模糊推导的间隙值作为“软约束”整合进最优控制问题中，并利用 FALCON 工具箱和 IPOPT 求解器进行轨迹规划。

Result: 在单线程 MATLAB 环境下，算法迭代时间约为 2.3 秒，初步证明了准实时运行的可行性。但在实验中发现 FALCON 与 IPOPT 的最新版本存在软件兼容性故障，导致拉格朗日惩罚项始终为零，使得约束无法正确生效，这被确认为工具箱的回归问题而非建模错误。

Conclusion: 该混合架构在理论上能够结合模糊逻辑的灵活性与最优控制的精确性，具备准实时应用的潜力。然而，当前的实现受到 FALCON 与 IPOPT 求解器工具箱回归问题的制约，导致约束强制执行失效。未来研究需着重于修复软件兼容性、优化模糊隶属度函数，并扩展至更复杂的飞行器模型和随机动态环境。

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [16] [LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning](https://arxiv.org/abs/2602.12314)
*Junwoon Lee,Yulun Tian*

Main category: cs.RO

TL;DR: LatentAM 是一个基于 3D 高斯泼溅（3DGS）的在线建图框架，通过在线字典学习实现与模型无关、无需预训练的语义特征提取，支持在大规模环境中以近实时速度进行开放词汇的机器人感知。


<details>
  <summary>Details</summary>
Motivation: 传统的开放词汇感知方法通常依赖于特定 VLM 模型的解码器进行高维嵌入蒸馏，这导致模型通用性差且往往需要离线预训练。此外，在大规模场景中，如何实现高效的特征映射并保持较低的显存占用是实时机器人任务中的痛点。

Method: 1. **在线字典学习**：提出一种无需预训练且与具体模型无关的方法，将每个高斯基元与紧凑的查询向量关联。
2. **注意力机制映射**：通过可学习字典和注意力机制，将查询向量转换为近似的 VLM 嵌入。
3. **分层地图管理**：采用基于体素哈希（Voxel Hashing）的策略，将活跃的局部地图保留在 GPU 优化，全局地图索引存储在 CPU，实现内存开销受限的大规模建图。
4. **在线优化**：利用流式观测数据初始化字典，并在置信域正则化下进行在线优化，以适应不断变化的场景语义。

Result: 在公开基准和大规模自定义数据集上的实验表明，LatentAM 的特征重建保真度显著优于现有最先进（SOTA）方法，且在评估数据集上达到了 12-35 FPS 的近实时运行速度。

Conclusion: LatentAM 为开放词汇机器人感知提供了一个高效、通用且可扩展的 3D 建图方案，其“即插即用”的特性使其能够灵活适应不同的视觉语言模型，同时在计算效率和显存管理上达到了工业级的实用要求。

Abstract: We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our approach associates each Gaussian primitive with a compact query vector that can be converted into approximate VLM embeddings using an attention mechanism with a learnable dictionary. The dictionary is initialized efficiently from streaming observations and optimized online to adapt to evolving scene semantics under trust-region regularization. To scale to long trajectories and large environments, we further propose an efficient map management strategy based on voxel hashing, where optimization is restricted to an active local map on the GPU, while the global map is stored and indexed on the CPU to maintain bounded GPU memory usage. Experiments on public benchmarks and a large-scale custom dataset demonstrate that LatentAM attains significantly better feature reconstruction fidelity compared to state-of-the-art methods, while achieving near-real-time speed (12-35 FPS) on the evaluated datasets. Our project page is at: https://junwoonlee.github.io/projects/LatentAM

</details>


### [17] [ForeAct: Steering Your VLA with Efficient Visual Foresight Planning](https://arxiv.org/abs/2602.12322)
*Zhuoyang Zhang,Shang Yang,Qinghao Hu,Luke J. Huang,James Hou,Yufei Sun,Yao Lu,Song Han*

Main category: cs.RO

TL;DR: ForeAct 是一个通用的视觉预见规划器，通过实时生成未来观察图像和子任务描述来指导 VLA 模型，在不改变 VLA 架构的前提下大幅提升了复杂任务的执行成功率。


<details>
  <summary>Details</summary>
Motivation: 在开放世界中，VLA 模型难以同时兼顾高层语义推理与底层视觉运动推理。作者旨在通过提供“想象的”未来观察，降低 VLA 的推理负担，提升其在多步任务中的准确性和泛化性。

Method: 提出 ForeAct 框架，包含：1. **高效预见图像生成模块**：可在 0.33s 内生成 640×480 的未来观察图像；2. **视觉语言模型（VLM）**：负责分解任务并生成子任务描述。该框架通过在 VLA 的视觉输入中加入“预见图像”，使其专注于视觉运动推理。生成器在超过 100 万个跨具身序列上预训练以学习鲁棒的动力学规律。

Result: 在 11 个真实世界多步任务基准测试中，ForeAct 达到 87.4% 的平均成功率。相比 $\pi_0$ 基线（46.5%）绝对提升了 40.9%，相比仅使用文本子任务引导的基线（57.1%）绝对提升了 30.3%。

Conclusion: 通过引入视觉预见（Visual Foresight），将高层规划与底层动作执行解耦，能显著增强现有 VLA 模型的泛化能力和任务成功率。该框架证明了高质量未来图像预测在具身智能规划中的关键作用。

Abstract: Vision-Language-Action (VLA) models convert high-level language instructions into concrete, executable actions, a task that is especially challenging in open-world environments. We present Visual Foresight Planning (ForeAct), a general and efficient planner that guides a VLA step-by-step using imagined future observations and subtask descriptions. With an imagined future observation, the VLA can focus on visuo-motor inference rather than high-level semantic reasoning, leading to improved accuracy and generalization. Our planner comprises a highly efficient foresight image generation module that predicts a high-quality 640$\times$480 future observation from the current visual input and language instruction within only 0.33s on an H100 GPU, together with a vision-language model that reasons over the task and produces subtask descriptions for both the generator and the VLA. Importantly, state-of-the-art VLAs can integrate our planner seamlessly by simply augmenting their visual inputs, without any architectural modification. The foresight generator is pretrained on over 1 million multi-task, cross-embodiment episodes, enabling it to learn robust embodied dynamics. We evaluate our framework on a benchmark that consists of 11 diverse, multi-step real-world tasks. It achieves an average success rate of 87.4%, demonstrating a +40.9% absolute improvement over the $π_0$ baseline (46.5%) and a +30.3% absolute improvement over $π_0$ augmented with textual subtask guidance (57.1%).

</details>


### [18] [Schur-MI: Fast Mutual Information for Robotic Information Gathering](https://arxiv.org/abs/2602.12346)
*Kalvik Jakkala,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: 本文提出 Schur-MI 框架，通过舒尔补分解和计算重用技术，将高斯过程互信息的计算复杂度显著降低，实现了在机器人探测任务中高效、实时的信息路径规划。


<details>
  <summary>Details</summary>
Motivation: 互信息（MI）在机器人信息采集（RIG）中具有强理论保证，但由于涉及大量的对数行列式计算，其计算成本极高，限制了其在实时传感器部署（SP）和信息路径规划（IPP）中的应用。

Method: 提出了一种基于高斯过程（GP）的互信息计算公式 Schur-MI：(1) 利用机器人信息采集的迭代结构进行预计算，并在规划步骤中重用中间变量；(2) 采用舒尔补（Schur-complement）因子分解，将互信息的计算复杂度从候选采样点规模的立方 $O(|\mathcal{V}|^3)$ 降低到已选采样点规模的立方 $O(|\mathcal{A}|^3)$。

Result: 在真实的海床地形（bathymetry）数据集实验中，Schur-MI 相比标准 MI 方法实现了高达 12.7 倍的加速。此外，通过无人船（ASV）的自适应 IPP 现场试验验证了该方法在实际在线规划中的可行性。

Conclusion: Schur-MI 通过显著降低互信息的计算开销，使其能够应用于实时的机器人探测任务，有效弥合了信息论目标函数与实时在线规划之间的性能鸿沟。

Abstract: Mutual information (MI) is a principled and widely used objective for robotic information gathering (RIG), providing strong theoretical guarantees for sensor placement (SP) and informative path planning (IPP). However, its high computational cost, dominated by repeated log-determinant evaluations, has limited its use in real-time planning. This letter presents Schur-MI, a Gaussian process (GP) MI formulation that (i) leverages the iterative structure of RIG to precompute and reuse expensive intermediate quantities across planning steps, and (ii) uses a Schur-complement factorization to avoid large determinant computations. Together, these methods reduce the per-evaluation cost of MI from $\mathcal{O}(|\mathcal{V}|^3)$ to $\mathcal{O}(|\mathcal{A}|^3)$, where $\mathcal{V}$ and $\mathcal{A}$ denote the candidate and selected sensing locations, respectively. Experiments on real-world bathymetry datasets show that Schur-MI achieves up to a $12.7\times$ speedup over the standard MI formulation. Field trials with an autonomous surface vehicle (ASV) performing adaptive IPP further validate its practicality. By making MI computation tractable for online planning, Schur-MI helps bridge the gap between information-theoretic objectives and real-time robotic exploration.

</details>


### [19] [LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation](https://arxiv.org/abs/2602.12351)
*Yue Hu,Avery Xi,Qixin Xiao,Seth Isaacson,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: LongNav-R1 提出了一个针对 VLA 模型的多轮强化学习框架，通过将导航转化为连续对话并引入视界自适应优化，显著提升了长程视觉导航的成功率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的单轮 VLA 决策范式难以处理长程导航中的历史因果推理，且过度依赖人类演示，导致模型缺乏行为多样性并在长任务中表现僵化。

Method: 开发了 LongNav-R1 框架，将导航建模为智能体与环境间的连续多轮对话；引入了“视界自适应策略优化”（Horizon-Adaptive Policy Optimization），通过在优势估计中考虑变长视界，实现精准的时间信用分配（Temporal Credit Assignment）。

Result: 仅需 4,000 条采样轨迹，LongNav-R1 将 Qwen3-VL-2B 的导航成功率从 64.3% 提升至 73.0%，在样本效率上超越 SOTA 方法，并成功实现真实世界环境的零样本迁移。

Conclusion: 多轮强化学习（Multi-turn RL）范式能显著增强 VLA 模型在长程任务中的推理与决策能力，有效解决长序列下的行为塌陷问题，并展现出卓越的现实场景泛化性。

Abstract: This paper develops LongNav-R1, an end-to-end multi-turn reinforcement learning (RL) framework designed to optimize Visual-Language-Action (VLA) models for long-horizon navigation. Unlike existing single-turn paradigm, LongNav-R1 reformulates the navigation decision process as a continuous multi-turn conversation between the VLA policy and the embodied environment. This multi-turn RL framework offers two distinct advantages: i) it enables the agent to reason about the causal effects of historical interactions and sequential future outcomes; and ii) it allows the model to learn directly from online interactions, fostering diverse trajectory generation and avoiding the behavioral rigidity often imposed by human demonstrations. Furthermore, we introduce Horizon-Adaptive Policy Optimization. This mechanism explicitly accounts for varying horizon lengths during advantage estimation, facilitating accurate temporal credit assignment over extended sequences. Consequently, the agent develops diverse navigation behaviors and resists collapse during long-horizon tasks. Experiments on object navigation benchmarks validate the framework's efficacy: With 4,000 rollout trajectories, LongNav-R1 boosts the Qwen3-VL-2B success rate from 64.3% to 73.0%. These results demonstrate superior sample efficiency and significantly outperform state-of-the-art methods. The model's generalizability and robustness are further validated by its zero-shot performance in long-horizon real-world navigation settings. All source code will be open-sourced upon publication.

</details>


### [20] [Predicting Dynamic Map States from Limited Field-of-View Sensor Data](https://arxiv.org/abs/2602.12360)
*Knut Peterson,David Han*

Main category: cs.RO

TL;DR: 针对传感器视场受限问题，本文提出将动态传感器数据编码为融合时空信息的单张图像，并利用成熟的图像生成模型实现高精度的环境状态预测。


<details>
  <summary>Details</summary>
Motivation: 自主系统在现实部署中常因设计限制、意外遮挡或传感器故障导致视场角（FOV）受限。为确保系统安全运行，亟需一种能够在数据缺失或视野受限的情况下，根据有限的时间序列数据准确推断并预测周围环境状态的能力。

Method: 提出将动态传感器数据转化为一种简单的“单图格式”，该格式能够同时捕捉空间和时间特征；随后利用多种现有的“图像到图像”（Image-to-Image）深度学习模型，基于此类时空图像进行动态地图状态的推理与预测。

Result: 实验表明，该方法在多种多样的感知场景下均表现出色，能够利用现有的图像学习模型高精度地预测动态地图状态。

Conclusion: 该研究证明，通过巧妙的数据表示（将时空信息编码为单张图像），可以充分利用成熟的计算机视觉架构解决机器人感知中的有限视场预测挑战，具有较强的普适性和实用价值。

Abstract: When autonomous systems are deployed in real-world scenarios, sensors are often subject to limited field-of-view (FOV) constraints, either naturally through system design, or through unexpected occlusions or sensor failures. In conditions where a large FOV is unavailable, it is important to be able to infer information about the environment and predict the state of nearby surroundings based on available data to maintain safe and accurate operation. In this work, we explore the effectiveness of deep learning for dynamic map state prediction based on limited FOV time series data. We show that by representing dynamic sensor data in a simple single-image format that captures both spatial and temporal information, we can effectively use a wide variety of existing image-to-image learning models to predict map states with high accuracy in a diverse set of sensing scenarios.

</details>


### [21] [Zero-Shot Adaptation to Robot Structural Damage via Natural Language-Informed Kinodynamics Modeling](https://arxiv.org/abs/2602.12385)
*Anuj Pokhrel,Aniket Datar,Mohammad Nazeri,Francesco Cancelliere,Xuesu Xiao*

Main category: cs.RO

TL;DR: 本文提出ZLIK，通过将自然语言损伤描述融入动力学建模，利用自监督学习实现了受损机器人在复杂环境下的高精度、零样本自适应动力学预测。


<details>
  <summary>Details</summary>
Motivation: 高性能机器人在极端环境下不可避免会产生机械损伤，导致其动力学特性发生变化。由于损伤类型具有高度异构性和复杂性，难以通过传统方式量化损伤对动力学的影响。作者提出利用自然语言的描述能力来捕捉这些复杂的损伤特征。

Method: 提出ZLIK（Zero-shot Language Informed Kinodynamics）框架。该方法采用自监督学习，将损伤的自然语言语义描述与机器人的动力学行为进行关联（Grounding），从而构建数据驱动的前向动力学模型，并利用BeamNG.tech物理模拟器进行数据驱动的训练。

Result: ZLIK在处理未见过的损伤时实现了零样本自适应，动力学预测误差最高降低了81%。此外，该模型成功实现了从仿真到现实（sim-to-real）以及从全比例到1/10比例机器人的跨尺度泛化。

Conclusion: 自然语言能够有效捕捉并量化复杂的结构损伤，结合自监督学习可以显著提升自主机器人在机械受损情况下的动力学建模能力、鲁棒性以及跨场景泛化能力。

Abstract: High-performance autonomous mobile robots endure significant mechanical stress during in-the-wild operations, e.g., driving at high speeds or over rugged terrain. Although these platforms are engineered to withstand such conditions, mechanical degradation is inevitable. Structural damage manifests as consistent and notable changes in kinodynamic behavior compared to a healthy vehicle. Given the heterogeneous nature of structural failures, quantifying various damages to inform kinodynamics is challenging. We posit that natural language can describe and thus capture this variety of damages. Therefore, we propose Zero-shot Language Informed Kinodynamics (ZLIK), which employs self-supervised learning to ground semantic information of damage descriptions in kinodynamic behaviors to learn a forward kinodynamics model in a data-driven manner. Using the high-fidelity soft-body physics simulator BeamNG.tech, we collect data from a variety of structurally compromised vehicles. Our learned model achieves zero-shot adaptation to different damages with up to 81% reduction in kinodynamics error and generalizes across the sim-to-real and full-to-1/10$^{\text{th}}$ scale gaps.

</details>


### [22] [Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning](https://arxiv.org/abs/2602.12405)
*Carl Qi,Xiaojie Wang,Silong Yong,Stephen Sheng,Huitan Mao,Sriram Srinivasan,Manikantan Nambi,Amy Zhang,Yesh Dattatreya*

Main category: cs.RO

TL;DR: 本文提出 ARMOR 框架，通过多任务自精炼过程和异构监督学习，显著提升了机器人在复杂环境下的故障检测与开放式推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人故障推理方法通常将其视为闭集分类问题或依赖大量人工标注，难以处理现实世界中微妙、组合且难以穷举的故障情况，且高质量推理标签的获取成本极高。

Method: 提出 ARMOR 框架，将检测与推理表述为多任务自精炼过程（Self-refinement）。训练时利用异构监督（大规模稀疏二值标签与小规模丰富推理标注），结合离线和在线模仿学习进行优化。推理时生成多个精炼轨迹，并通过自我确定性（Self-certainty）指标筛选最优预测。

Result: ARMOR 在多种环境下均达到 SOTA 性能，故障检测率提升高达 30%，通过 LLM 模糊匹配衡量的推理得分提升高达 100%。实验证明其对异构监督具有鲁棒性，并具备处理开放式推理的能力。

Conclusion: ARMOR 证明了将故障检测与推理建模为多任务自精炼过程，并结合异构监督学习，能够有效应对现实世界中复杂且难以枚举的机器人故障，实现了超越预定义模式的开放式推理能力。

Abstract: Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. Experiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30% on failure detection rate and up to 100% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes. We provide dditional visualizations on our website: https://sites.google.com/utexas.edu/armor

</details>


### [23] [MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery](https://arxiv.org/abs/2602.12407)
*Keshara Weerasinghe,Seyed Hamid Reza Roodabeh,Andrew Hawkins,Zhaomeng Zhang,Zachary Schrader,Homa Alemzadeh*

Main category: cs.RO

TL;DR: 本文推出了MiDAS，一个开源且跨平台的非侵入式手术机器人多模态数据采集系统，能在不访问机器人内部私有接口的情况下，实现与官方数据精度相当的操作追踪和手势识别。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人辅助微创手术（RMIS）研究依赖多模态数据，但商用手术机器人的内部遥测数据（telemetry）通常受版权保护且难以获取，严重阻碍了该领域的研究进展和数据共享。

Method: 开发了集成电磁追踪、RGB-D手部追踪、脚踏板压力传感和视频捕获的非侵入式硬件系统；在Raven-II（开源）和da Vinci Xi（临床）平台上采集了住院医生的操作数据；通过相关性分析验证传感器精度，并利用手势识别任务验证其有效性。

Result: 外部传感器采集的数据与机器人内部运动学高度相关；基于MiDAS非侵入式信号实现的手势识别准确率与使用机器人内部私有数据的结果相当。

Conclusion: MiDAS为RMIS研究提供了一个可复用的多模态数据采集标准方案，并通过开源相关数据集（特别是首个高保真疝气修复缝合数据集）填补了领域空白。

Abstract: Background: Robot-assisted minimally invasive surgery (RMIS) research increasingly relies on multimodal data, yet access to proprietary robot telemetry remains a major barrier. We introduce MiDAS, an open-source, platform-agnostic system enabling time-synchronized, non-invasive multimodal data acquisition across surgical robotic platforms.
  Methods: MiDAS integrates electromagnetic and RGB-D hand tracking, foot pedal sensing, and surgical video capturing without requiring proprietary robot interfaces. We validated MiDAS on the open-source Raven-II and the clinical da Vinci Xi by collecting multimodal datasets of peg transfer and hernia repair suturing tasks performed by surgical residents. Correlation analysis and downstream gesture recognition experiments were conducted.
  Results: External hand and foot sensing closely approximated internal robot kinematics and non-invasive motion signals achieved gesture recognition performance comparable to proprietary telemetry.
  Conclusion: MiDAS enables reproducible multimodal RMIS data collection and is released with annotated datasets, including the first multimodal dataset capturing hernia repair suturing on high-fidelity simulation models.

</details>


### [24] [Control Barrier Functions with Audio Risk Awareness for Robot Safe Navigation on Construction Sites](https://arxiv.org/abs/2602.12416)
*Johannes Mootz,Reza Akhavian*

Main category: cs.RO

TL;DR: 本文提出一种融合音频感知与控制屏障函数（CBF）的机器人安全滤波器，通过实时识别工地噪声（如电钻声）来动态调整导航安全边界，确保视觉受限环境下的避障安全性。


<details>
  <summary>Details</summary>
Motivation: 建筑工地环境复杂、动态且视觉遮挡严重，导致传统视觉感知和导航方案面临挑战。与此同时，工地中丰富的音频信息在当前的机器人自主系统中尚未得到充分利用。

Method: 提出一种基于控制屏障函数（CBF）的安全滤波器。该方法包含一个轻量级实时电钻检测器（基于信号包络和周期性），并将其输出作为外部风险因子，直接用于调制CBF的避障安全余量。实验对比了圆形和目标对齐椭圆形两种CBF表达形式。

Result: 在复杂环境的仿真评估中，该CBF滤波器在所有测试中均消除了安全违规。相比于圆形CBF（成功率40.2%），目标对齐椭圆CBF有效避免了死锁，成功率提升至76.5%。

Conclusion: 将音频感知集成到基于CBF的控制器中，证明了在安全关键和动态环境下，利用多模态信息（特别是声音）增强机器人安全推理能力的有效性。

Abstract: Construction automation increasingly requires autonomous mobile robots, yet robust autonomy remains challenging on construction sites. These environments are dynamic and often visually occluded, which complicates perception and navigation. In this context, valuable information from audio sources remains underutilized in most autonomy stacks. This work presents a control barrier function (CBF)-based safety filter that provides safety guarantees for obstacle avoidance while adapting safety margins during navigation using an audio-derived risk cue. The proposed framework augments the CBF with a lightweight, real-time jackhammer detector based on signal envelope and periodicity. Its output serves as an exogenous risk that is directly enforced in the controller by modulating the barrier function. The approach is evaluated in simulation with two CBF formulations (circular and goal-aligned elliptical) with a unicycle robot navigating a cluttered construction environment. Results show that the CBF safety filter eliminates safety violations across all trials while reaching the target in 40.2% (circular) vs. 76.5% (elliptical), as the elliptical formulation better avoids deadlock. This integration of audio perception into a CBF-based controller demonstrates a pathway toward richer multimodal safety reasoning in autonomous robots for safety-critical and dynamic environments.

</details>


### [25] [An Autonomous, End-to-End, Convex-Based Framework for Close-Range Rendezvous Trajectory Design and Guidance with Hardware Testbed Validation](https://arxiv.org/abs/2602.12421)
*Minduli C. Wijayatunga,Julian Guinane,Nathan D. Wallace,Xiaofeng Wu*

Main category: cs.RO

TL;DR: 本文提出并验证了CORTEX框架，这是一种结合深度学习感知与凸优化引导的自主卫星交会对接系统，能够实现高精度、实时的轨迹规划，并有效应对各类系统故障与环境不确定性。


<details>
  <summary>Details</summary>
Motivation: 自主卫星在轨服务要求在满足严苛安全与运行约束的同时，具备星上实时计算能力，并能应对感知、执行器及动力学模型的不确定性与故障。

Method: 提出CORTEX（基于凸优化的交会对接轨迹执行）框架，其核心组成部分包括：(1) 集成深度学习的视觉感知流水线；(2) 基于凸优化的实时轨迹设计与引导算法；(3) 具备参考轨迹重生成及“中止至安全轨道”（abort-to-safe-orbit）逻辑的鲁棒控制策略。验证手段涵盖了Basilisk高保真仿真环境、光学导航测试平台以及平面气浮台硬件在环（HITL）实验。

Result: 在极端工况的蒙特卡洛仿真中，CORTEX实现了36.85±44.46 mm的末端位置误差和1.25±2.26 mm/s的速度误差。在包含故障注入（发动机失效、传感器故障）的18次气浮台硬件实验中，末端位置和速度误差分别控制在8.09±5.29 mm和2.23±1.72 mm/s，验证了系统的端到端引导性能与鲁棒性。

Conclusion: CORTEX框架证明了在存在动力学不确定性、传感器故障和发动机失效的复杂环境下，通过集成深度学习感知与凸优化引导，能够实现高精度、实时且具备容错能力的自主卫星近距离交会对接。

Abstract: Autonomous satellite servicing missions must execute close-range rendezvous under stringent safety and operational constraints while remaining computationally tractable for onboard use and robust to uncertainty in sensing, actuation, and dynamics. This paper presents CORTEX (Convex Optimization for Rendezvous Trajectory Execution), an autonomous, perception-enabled, real-time trajectory design and guidance framework for close-range rendezvous. CORTEX integrates a deep-learning perception pipeline with convex-optimisation-based trajectory design and guidance, including reference regeneration and abort-to-safe-orbit logic to recover from large deviations caused by sensor faults and engine failures.
  CORTEX is validated in high-fidelity software simulation and hardware-in-the-loop experiments. The software pipeline (Basilisk) models high-fidelity relative dynamics, realistic thruster execution, perception, and attitude control. Hardware testing uses (i) an optical navigation testbed to assess perception-to-estimation performance and (ii) a planar air-bearing testbed to evaluate the end-to-end guidance loop under representative actuation and subsystem effects. A Monte-Carlo campaign in simulation includes initial-state uncertainty, thrust-magnitude errors, and missed-thrust events; under the strongest case investigated, CORTEX achieves terminal docking errors of $36.85 \pm 44.46$ mm in relative position and $1.25 \pm 2.26$ mm/s in relative velocity. On the planar air-bearing testbed, 18 cases are executed (10 nominal; 8 off-nominal requiring recomputation and/or abort due to simulated engine failure and sensor malfunctions), yielding terminal errors of $8.09 \pm 5.29$ mm in position and $2.23 \pm 1.72$ mm/s in velocity.

</details>


### [26] [Gradient-Enhanced Partitioned Gaussian Processes for Real-Time Quadrotor Dynamics Modeling](https://arxiv.org/abs/2602.12487)
*Xinhuan Sang,Adam Rozman,Sheryl Grace,Roberto Tron*

Main category: cs.RO

TL;DR: 本文提出一种结合梯度信息与状态空间划分的高斯过程（GP）模型，通过Schur补优化计算流程，实现了四旋翼气动效应的实时（>30 Hz）高精度推理。


<details>
  <summary>Details</summary>
Motivation: 传统的基于高斯过程（GP）的动力学模型虽然能提供不确定性量化，但计算成本极高，难以满足四旋翼实时仿真的需求。此外，传统模型难以精确捕获复杂的空气动力学效应（如旋翼间的相互作用）。

Method: 1. **梯度增强GP**：在GP模型中集成导数信息（通过有限差分法获取）以提高预测精度。
2. **状态空间划分与近似**：将状态空间划分为非重叠区域，每个区域关联一个局部GP。
3. **离线/在线计算分解**：利用Schur补（Schur complements）将训练数据分为“近场”和“远场”子集，将耗时的矩阵求逆运算移至离线阶段。
4. **中保真度数据驱动**：使用CHARM气动求解器模拟SUI Endurance四旋翼的旋翼间干扰及表观风向效应，生成包含力、转矩及噪声的训练集。

Result: 该模型在标准桌面硬件上实现了超过30 Hz的实时推理频率。实验表明，带梯度的划分GP在精度上显著优于不带梯度的标准划分GP，同时大幅缩短了在线计算时间。

Conclusion: 该框架为复杂且非稳态环境下的实时气动预测与控制算法提供了一个高效且精确的基础，能够有效处理四旋翼的高维动力学特性及复杂气动效应。

Abstract: We present a quadrotor dynamics Gaussian Process (GP) with gradient information that achieves real-time inference via state-space partitioning and approximation, and that includes aerodynamic effects using data from mid-fidelity potential flow simulations. While traditional GP-based approaches provide reliable Bayesian predictions with uncertainty quantification, they are computationally expensive and thus unsuitable for real-time simulations. To address this challenge, we integrate gradient information to improve accuracy and introduce a novel partitioning and approximation strategy to reduce online computational cost. In particular, for the latter, we associate a local GP with each non-overlapping region; by splitting the training data into local near and far subsets, and by using Schur complements, we show that a large part of the matrix inversions required for inference can be performed offline, enabling real-time inference at frequencies above 30 Hz on standard desktop hardware. To generate a training dataset that captures aerodynamic effects, such as rotor-rotor interactions and apparent wind direction, we use the CHARM code, which is a mid-fidelity aerodynamic solver. It is applied to the SUI Endurance quadrotor to predict force and torque, along with noise at three specified locations. The derivative information is obtained via finite differences. Experimental results demonstrate that the proposed partitioned GP with gradient conditioning achieves higher accuracy than standard partitioned GPs without gradient information, while greatly reducing computational time. This framework provides an efficient foundation for real-time aerodynamic prediction and control algorithms in complex and unsteady environments.

</details>


### [27] [Composable Model-Free RL for Navigation with Input-Affine Systems](https://arxiv.org/abs/2602.12492)
*Xinhuan Sang,Abdelrahman Abdelgawad,Roberto Tron*

Main category: cs.RO

TL;DR: 本文提出一种可组合的无模型强化学习方法，通过学习单个环境元素的价值函数并利用QCQP在线组合策略，实现了具有形式化安全保证的连续时间非线性系统避障与导航。


<details>
  <summary>Details</summary>
Motivation: 自动机器人在复杂动态环境中难以预见所有行为且难以精确建模非线性动力学，因此需要一种既能处理未知动力学，又能提供实时安全导航保证的可组合学习方法。

Method: 1. 针对输入仿射的连续时间非线性动力学，推导出HJB方程并证明优势函数关于动作和最优策略呈二次型。
2. 提出一种无模型Actor-Critic算法，利用梯度下降学习单个环境元素（目标或障碍物）的价值函数和策略。
3. 引入二次约束二次规划（QCQP）实现在线多目标/障碍物策略组合。
4. 利用价值函数水平集（level sets）提供避障的形式化保证。

Result: 仿真结果显示，该方法在处理连续时间动态任务时，性能明显优于基于离散时间近似的PPO（近端策略优化）基准算法。

Conclusion: 该研究为处理具有未知非线性动力学的机器人系统提供了一个可组合、可扩展且具备形式化安全保障的无模型控制框架，是替代传统CLF/CBF控制器的有效方案。

Abstract: As autonomous robots move into complex, dynamic real-world environments, they must learn to navigate safely in real time, yet anticipating all possible behaviors is infeasible. We propose a composable, model-free reinforcement learning method that learns a value function and an optimal policy for each individual environment element (e.g., goal or obstacle) and composes them online to achieve goal reaching and collision avoidance. Assuming unknown nonlinear dynamics that evolve in continuous time and are input-affine, we derive a continuous-time Hamilton-Jacobi-Bellman (HJB) equation for the value function and show that the corresponding advantage function is quadratic in the action and optimal policy. Based on this structure, we introduce a model-free actor-critic algorithm that learns policies and value functions for static or moving obstacles using gradient descent. We then compose multiple reach/avoid models via a quadratically constrained quadratic program (QCQP), yielding formal obstacle-avoidance guarantees in terms of value-function level sets, providing a model-free alternative to CLF/CBF-based controllers. Simulations demonstrate improved performance over a PPO baseline applied to a discrete-time approximation.

</details>


### [28] [Monocular Reconstruction of Neural Tactile Fields](https://arxiv.org/abs/2602.12508)
*Pavan Mantripragada,Siddhanth Deshmukh,Eadom Dessalene,Manas Desai,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: 本文引入“神经触觉场”，首创通过单目RGB图像预测环境的触觉反馈，使机器人能够区分障碍物的阻力大小，从而在复杂环境中规划出可穿过软性物体（如植被）的最优路径。


<details>
  <summary>Details</summary>
Motivation: 传统机器人规划依赖静态几何占用（Occupancy）表示，无法区分物体的物理特性（如变形、屈服）。这导致机器人将所有占据空间视为不可逾越的障碍，而无法识别并穿越低阻力区域（如落叶或灌木丛）。

Method: 提出一种名为“神经触觉场”（Neural Tactile Fields）的新型3D表示法，将空间位置映射为预期的触觉响应。该方法首次实现了从单张单目RGB图像预测3D触觉场，并能将其与现成的路径规划器结合，通过评估区域阻力进行路径生成。

Result: 实验显示，该模型在体积3D重建上比现有SOTA单目重建方法（LRM和Direct3D）提升了85.8%，表面重建精度提升了26.7%。在规划应用中，机器人成功实现了避开高阻力物体并主动选择低阻力区域穿行。

Conclusion: 神经触觉场（NTFs）不仅在几何重建精度上显著优于现有方法，更重要的是通过赋予空间物理属性感知能力，打破了机器人只能避开所有障碍物的局限，实现了更具交互性的路径规划。

Abstract: Robots operating in the real world must plan through environments that deform, yield, and reconfigure under contact, requiring interaction-aware 3D representations that extend beyond static geometric occupancy. To address this, we introduce neural tactile fields, a novel 3D representation that maps spatial locations to the expected tactile response upon contact. Our model predicts these neural tactile fields from a single monocular RGB image -- the first method to do so. When integrated with off-the-shelf path planners, neural tactile fields enable robots to generate paths that avoid high-resistance objects while deliberately routing through low-resistance regions (e.g. foliage), rather than treating all occupied space as equally impassable. Empirically, our learning framework improves volumetric 3D reconstruction by $85.8\%$ and surface reconstruction by $26.7\%$ compared to state-of-the-art monocular 3D reconstruction methods (LRM and Direct3D).

</details>


### [29] [CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning](https://arxiv.org/abs/2602.12532)
*Yike Zhang,Yaonan Wang,Xinxin Sun,Kaizhen Huang,Zhiyuan Xu,Junjie Ji,Zhengping Che,Jian Tang,Jingtao Sun*

Main category: cs.RO

TL;DR: 本文提出 CRAFT，一个通过变分信息瓶颈和课程微调增强力觉感知的 VLA 框架，旨在解决机器人复杂接触任务中力反馈被忽视的问题，显著提升了操作的鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作 (VLA) 模型在处理接触密集型任务（如精确对准、稳定接触和柔性体操作）时表现不佳。其核心挑战在于高熵的视觉/语言输入与低熵但至关重要的力信号之间存在不平衡，导致模型过度依赖感知而忽略了不稳定的控制需求。

Method: 提出 CRAFT 框架：1. **变分信息瓶颈 (VIB) 模块**：在训练早期调节并限制视觉和语言嵌入的权重，防止模型产生感知依赖；2. **力觉感知课程学习策略**：初期优先学习力信号，随后逐步恢复全多模态信息的接入；3. **同源主从遥操作系统**：用于采集同步的视觉、语言及力觉多模态数据。

Result: 真实世界实验表明，CRAFT 显著提升了接触密集型任务的成功率。该框架不仅能泛化到未见物体和新任务变体，还能有效适配不同的 VLA 架构。

Conclusion: CRAFT 有效解决了 VLA 模型中力感知信号容易被高熵感知输入掩盖的问题，为实现鲁棒且通用的机器人接触密集型操作提供了有效的框架支持。

Abstract: Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.

</details>


### [30] [Eva-Tracker: ESDF-update-free, Visibility-aware Planning with Target Reacquisition for Robust Aerial Tracking](https://arxiv.org/abs/2602.12549)
*Yue Lin,Yang Liu,Dong Wang,Huchuan Lu*

Main category: cs.RO

TL;DR: 本文提出 Eva-Tracker，一种通过预计算 FoV-ESDF 消除更新开销，并结合恢复路径生成算法实现高效、稳健的无人机可见性感知追踪框架。


<details>
  <summary>Details</summary>
Motivation: 传统的欧几里得符号距离场（ESDF）在可见性评估中更新频繁，导致计算负担过重。此外，现有方法在目标丢失后缺乏有效的快速重捕获机制。

Method: 1. 提出目标轨迹预测与具备重捕获能力的可见性感知初始路径生成算法；2. 构建 FoV-ESDF（视场 ESDF），通过预计算方式消除实时更新开销；3. 结合可微 FoV-ESDF 目标函数进行轨迹优化，确保全过程的持续可见性。

Result: 仿真和真实世界实验表明，与现有最先进（SOTA）方法相比，该方法在降低计算开销的同时，提供了更强的追踪稳健性。

Conclusion: Eva-Tracker 证明了通过预计算视场相关的距离场（FoV-ESDF）并结合主动恢复策略，可以有效解决空中追踪任务中遮挡避免与实时计算性能之间的矛盾。

Abstract: The Euclidean Signed Distance Field (ESDF) is widely used in visibility evaluation to prevent occlusions and collisions during tracking. However, frequent ESDF updates introduce considerable computational overhead. To address this issue, we propose Eva-Tracker, a visibility-aware trajectory planning framework for aerial tracking that eliminates ESDF updates and incorporates a recovery-capable path generation method for target reacquisition. First, we design a target trajectory prediction method and a visibility-aware initial path generation algorithm that maintain an appropriate observation distance, avoid occlusions, and enable rapid replanning to reacquire the target when it is lost. Then, we propose the Field of View ESDF (FoV-ESDF), a precomputed ESDF tailored to the tracker's field of view, enabling rapid visibility evaluation without requiring updates. Finally, we optimize the trajectory using differentiable FoV-ESDF-based objectives to ensure continuous visibility throughout the tracking process. Extensive simulations and real-world experiments demonstrate that our approach delivers more robust tracking results with lower computational effort than existing state-of-the-art methods. The source code is available at https://github.com/Yue-0/Eva-Tracker.

</details>


### [31] [Hemispherical Angular Power Mapping of Installed mmWave Radar Modules Under Realistic Deployment Constraints](https://arxiv.org/abs/2602.12584)
*Maaz Qureshi,Mohammad Omid Bagheri,William Melek,George Shaker*

Main category: cs.RO

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Characterizing the angular radiation behavior of installed millimeter-wave (mmWave) radar modules is increasingly important in practical sensing platforms, where packaging, mounting hardware, and nearby structures can significantly alter the effective emission profile. However, once a device is embedded in its host environment, conventional chamber- and turntable-based antenna measurements are often impractical. This paper presents a hemispherical angular received-power mapping methodology for in-situ EM validation of installed mmWave modules under realistic deployment constraints. The approach samples the accessible half-space around a stationary device-under-test by placing a calibrated receiving probe at prescribed (phi, theta, r) locations using geometry-consistent positioning and quasi-static acquisition. Amplitude-only received-power is recorded using standard RF instrumentation to generate hemispherical angular power maps that capture installation-dependent radiation characteristics. Proof-of-concept measurements on a 60-GHz radar module demonstrate repeatable hemi-spherical mapping with angular trends in good agreement with full-wave simulation, supporting practical on-site characterization of embedded mmWave transmitters.

</details>


### [32] [PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People](https://arxiv.org/abs/2602.12597)
*Mahdi Haghighat Joo,Maryam Karimi Jafari,Alireza Taheri*

Main category: cs.RO

TL;DR: PISHYAR 是一款集成了大模型交互与社会感知导航的智能手杖，通过视觉、语音和触觉多模态协作，在实现避障导航的同时，为视障人士提供智能化的社交支持与环境感知。


<details>
  <summary>Details</summary>
Motivation: 旨在解决视障人士在复杂社会环境下的物理移动挑战，通过整合社会感知能力与多模态人机交互（语音/视觉），提供超越传统导航的交互式辅助。

Method: 提出双组件系统：(1) 社会导航框架：基于树莓派5、OAK-D相机、YOLOv8识别、COMPOSER集体活动识别、D* Lite路径规划及触觉反馈；(2) 智能交互框架：集成LLM和VLM，支持语音识别、视觉场景描述及动态路由。

Result: 实验显示社会化导航准确率约为80%，集体活动识别表现稳健。初步用户研究（8名视障受试者）表明，系统在可用性、信任度和社交感知方面具有极高的用户接受度。

Conclusion: PISHYAR 证明了将社会感知导航与大语言模型交互相结合的潜力，不仅能作为可靠的辅助出行工具，还能通过多模态支持提升视障用户的社交互动与生活独立性。

Abstract: This paper presents PISHYAR, a socially intelligent smart cane designed by our group to combine socially aware navigation with multimodal human-AI interaction to support both physical mobility and interactive assistance. The system consists of two components: (1) a social navigation framework implemented on a Raspberry Pi 5 that integrates real-time RGB-D perception using an OAK-D Lite camera, YOLOv8-based object detection, COMPOSER-based collective activity recognition, D* Lite dynamic path planning, and haptic feedback via vibration motors for tasks such as locating a vacant seat; and (2) an agentic multimodal LLM-VLM interaction framework that integrates speech recognition, vision language models, large language models, and text-to-speech, with dynamic routing between voice-only and vision-only modes to enable natural voice-based communication, scene description, and object localization from visual input. The system is evaluated through a combination of simulation-based tests, real-world field experiments, and user-centered studies. Results from simulated and real indoor environments demonstrate reliable obstacle avoidance and socially compliant navigation, achieving an overall system accuracy of approximately 80% under different social conditions. Group activity recognition further shows robust performance across diverse crowd scenarios. In addition, a preliminary exploratory user study with eight visually impaired and low-vision participants evaluates the agentic interaction framework through structured tasks and a UTAUT-based questionnaire reveals high acceptance and positive perceptions of usability, trust, and perceived sociability during our experiments. The results highlight the potential of PISHYAR as a multimodal assistive mobility aid that extends beyond navigation to provide socially interactive support for such users.

</details>


### [33] [RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models](https://arxiv.org/abs/2602.12628)
*Liangzhi Shi,Shuaihang Chen,Feng Gao,Yinuo Chen,Kang Chen,Tonghe Zhang,Hongzhi Zhang,Weinan Zhang,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 本文提出了 RL-Co 框架，通过在仿真环境中结合强化学习和真实数据的辅助监督训练，解决了传统 SFT 忽略仿真交互性的问题，显著提升了 VLA 模型在真实机器人任务中的成功率和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型训练主要依赖仿真数据的监督微调（SFT），将仿真仅视为静态演示源，未能利用大规模闭环交互的潜力，导致模型在真实场景中的泛化能力和成功率受限。

Method: 提出 RL-Co 框架，采用两阶段设计：
1. **预热阶段（Warm-start）**：在真实和仿真演示数据的混合体上进行监督微调（SFT）。
2. **微调阶段（Fine-tuning）**：在仿真环境中使用强化学习（RL）优化策略，同时在真实世界数据上施加辅助监督损失（Auxiliary Supervised Loss），以防止模型对真实场景能力的灾难性遗忘。

Result: 在四项真实世界桌面操作任务中，使用 RL-Co 框架后，OpenVLA 和 $\pi_{0.5}$ 的成功率分别比基准提升了 24% 和 20%。此外，该方法显著增强了模型对未见任务变体的泛化能力，并提高了真实世界数据的利用效率。

Conclusion: RL-Co 证明了将仿真环境中的闭环强化学习与真实世界数据的监督学习相结合，是提升视觉-语言-动作（VLA）模型在真实机器人部署中表现的一种高效且可扩展的途径。

Abstract: Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \underline{\textit{RL}}-based sim-real \underline{\textit{Co}}-training \modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.

</details>


### [34] [Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning](https://arxiv.org/abs/2602.12633)
*Tianyi Xiang,Jiahang Cao,Sikai Guo,Guoyang Zhao,Andrew F. Luo,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出一种物理约束的 Real-to-Sim 流水线，利用可微仿真和接触图优化单视图 RGB-D 重建，确保 3D 场景的物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的单视图3D重建往往忽略物理约束，导致物体悬空或相互穿透等物理无效状态，使得下游的机器人仿真和控制变得不可靠。

Method: 提出一种可微优化流水线，通过“接触图”（Contact Graph）显式建模物体间的空间依赖关系，并利用“可微刚体仿真”（Differentiable Rigid-body Simulation）共同精炼物体的位姿和物理属性。

Result: 在仿真和真实世界评估中均表现出极高的物理保真度，能够准确复制现实世界的接触动力学，从而支持稳定且可靠的接触密集型机器人操纵。

Conclusion: 该研究通过将物理约束集成到重建流程中，有效弥补了视觉感知与物理仿真之间的鸿沟，为实现可靠的 Real-to-Sim 转换和复杂的机器人操纵任务提供了坚实基础。

Abstract: Reconstructing physically valid 3D scenes from single-view observations is a prerequisite for bridging the gap between visual perception and robotic control. However, in scenarios requiring precise contact reasoning, such as robotic manipulation in highly cluttered environments, geometric fidelity alone is insufficient. Standard perception pipelines often neglect physical constraints, resulting in invalid states, e.g., floating objects or severe inter-penetration, rendering downstream simulation unreliable. To address these limitations, we propose a novel physics-constrained Real-to-Sim pipeline that reconstructs physically consistent 3D scenes from single-view RGB-D data. Central to our approach is a differentiable optimization pipeline that explicitly models spatial dependencies via a contact graph, jointly refining object poses and physical properties through differentiable rigid-body simulation. Extensive evaluations in both simulation and real-world settings demonstrate that our reconstructed scenes achieve high physical fidelity and faithfully replicate real-world contact dynamics, enabling stable and reliable contact-rich manipulation.

</details>


### [35] [PMG: Parameterized Motion Generator for Human-like Locomotion Control](https://arxiv.org/abs/2602.12656)
*Chenxi Han,Yuheng Min,Zihao Huang,Ao Hong,Hang Liu,Yi Cheng,Houde Liu*

Main category: cs.RO

TL;DR: 本文提出 PMG 框架，通过紧凑的参数化运动合成与模仿学习，克服了人形机器人控制对大数据的依赖和环境适应性差的挑战，实现了高响应且自然的类人步态控制。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人全身参考引导控制方法难以适配高层指令接口，且存在对大规模数据集依赖性强、在不同速度/姿态下鲁棒性差以及对机器人标定敏感等局限性。

Method: 提出参数化运动生成器（PMG），通过分析人体运动结构，利用紧凑的参数化运动数据与高维控制命令实时合成参考轨迹。此外，结合了模仿学习流水线和基于优化的 Sim-to-Real 电机参数辨识模块。

Result: 在人形机器人原型 ZERITH Z1 上验证表明，PMG 能够生成自然的类人运动，精准响应高维控制输入（如 VR 遥操作），并实现了高效、可验证的从仿真到现实（Sim-to-Real）的迁移。

Conclusion: 该研究为自然且可部署的人形机器人控制提供了一条实用的、经过实验验证的技术路径，成功提升了机器人运动的自然度、响应精度和部署效率。

Abstract: Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with High-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control.

</details>


### [36] [Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution](https://arxiv.org/abs/2602.12684)
*Rui Cai,Jun Guo,Xinze He,Piaopiao Jin,Jie Li,Bingxuan Lin,Futeng Liu,Wei Liu,Fei Ma,Kun Ma,Feng Qiu,Heng Qu,Yifei Su,Qiao Sun,Dong Wang,Donghao Wang,Yunhong Wang,Rujie Wu,Diyun Xiang,Yu Yang,Hangjun Ye,Yuan Zhang,Quanyun Zhou*

Main category: cs.RO

TL;DR: 小米机器人推出 Xiaomi-Robotics-0，这是一款专为实时、平滑控制优化的先进视觉-语言-动作（VLA）模型，通过创新的训练与部署方案，在模拟和真机双臂操控任务中均实现了顶尖性能并已开源。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有视觉-语言-动作（VLA）模型在真实机器人部署中面临的推理延迟、执行不平滑以及视觉语义知识遗忘等问题，实现高性能且流畅的实时双臂精细操作。

Method: 1. **两阶段训练策略**：首先在跨具身机器人轨迹和视觉语言数据上进行大规模预训练，以获得通用的动作生成能力；2. **异步执行后训练**：针对推理延迟问题，提出专门的后训练技术以支持异步执行；3. **部署优化**：在部署阶段对连续预测的动作块（Action Chunks）进行时间步对齐，确保机器人运动的连续和平滑。

Result: 该模型在所有模拟基准测试中均达到了 SOTA（先进）性能。在真实机器人双臂任务中，仅利用消费级 GPU 即可实现快速平滑的运行，并表现出极高的成功率和吞吐量。

Conclusion: Xiaomi-Robotics-0 证明了通过结合跨具身预训练、异步执行训练以及动作块对齐部署策略，可以显著提升 VLA 模型在处理复杂、高精度双臂操作任务时的实时性与成功率，为具身智能的实际应用提供了高效的解决方案。

Abstract: In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io

</details>


### [37] [SignScene: Visual Sign Grounding for Mapless Navigation](https://arxiv.org/abs/2602.12686)
*Nicky Zimmerman,Joel Loo,Benjamin Koh,Zishuo Wang,David Hsu*

Main category: cs.RO

TL;DR: 本文提出 SignScene，通过将标识信息转化为 VLM 可理解的空间语义表示，解决了机器人无地图导航中的标识理解与动作落地问题，在实机测试中实现了高精度的自主导航。


<details>
  <summary>Details</summary>
Motivation: 人类可以仅凭标识在陌生环境中导航，而机器人通常依赖地图。由于现实世界中的标识形式多样且语义抽象，如何将标识上的语义指令准确地对应（Grounding）到具体的本地 3D 场景元素和导航动作中，是实现机器人无地图导航的核心挑战。

Method: 提出了 **SignScene**，一种以标识为中心的空间语义表示方法。该方法将导航相关的场景元素与标识信息进行结构化整合，并以易于 VLM 推理的形式呈现，从而利用 VLM 的常识和语义理解能力完成“标识落地”（Sign Grounding），即从抽象语义到 3D 场景元素及动作的映射。

Result: 在包含 9 种环境类型的 114 个查询数据集上，该方法达到了 88% 的落地准确率，显著优于基准模型。此外，该研究在 Boston Dynamics 的 Spot 机器人上成功演示了仅依赖标识的真实世界无地图导航。

Conclusion: 本研究证明了通过构建合理的空间语义表示，预训练的视觉语言模型（VLM）能够有效理解复杂的物理标识并指导机器人行动，为实现在未知环境中的类人化、无地图导航提供了重要技术路径。

Abstract: Navigational signs enable humans to navigate unfamiliar environments without maps. This work studies how robots can similarly exploit signs for mapless navigation in the open world. A central challenge lies in interpreting signs: real-world signs are diverse and complex, and their abstract semantic contents need to be grounded in the local 3D scene. We formalize this as sign grounding, the problem of mapping semantic instructions on signs to corresponding scene elements and navigational actions. Recent Vision-Language Models (VLMs) offer the semantic common-sense and reasoning capabilities required for this task, but are sensitive to how spatial information is represented. We propose SignScene, a sign-centric spatial-semantic representation that captures navigation-relevant scene elements and sign information, and presents them to VLMs in a form conducive to effective reasoning. We evaluate our grounding approach on a dataset of 114 queries collected across nine diverse environment types, achieving 88% grounding accuracy and significantly outperforming baselines. Finally, we demonstrate that it enables real-world mapless navigation on a Spot robot using only signs.

</details>


### [38] [ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training](https://arxiv.org/abs/2602.12691)
*Rushuai Yang,Hecheng Wang,Chiming Liu,Xiaohan Yan,Yunlong Wang,Xuan Du,Shuoyu Yue,Yongcheng Liu,Chuheng Zhang,Lizhe Qi,Yi Chen,Wei Shan,Maoqing Yao*

Main category: cs.RO

TL;DR: 本文提出 ALOE 框架，通过动作级离策评估和块状 TD 自举法，解决了 VLA 模型在真实世界在线强化学习中评估不准和学习效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 在真实世界中，视觉-语言-动作（VLA）系统通常从混合数据（历史策略、人工干预等）中学习。现有方法为保证稳定性多采用保守的在策（On-policy）估计，限制了对高容量策略的评估效率和学习效果。本文旨在通过有效的离策评估提升 VLA 系统的在线强化学习表现。

Method: 提出了 ALOE（Action-Level Off-policy Evaluation）框架。该方法采用基于块（chunking-based）的时序差分（TD）自举法，针对单个动作序列而非仅仅预测最终任务结果进行评估。这种设计增强了在稀疏奖励环境下的信用分配（Credit Assignment）能力，并支持稳定的策略改进。

Result: 在智能手机装箱（高精度）、衣物折叠（长程变形体）和双臂拾取（多对象感知）三个真实任务中，ALOE 均表现出比现有方法更高的学习效率，且能够处理稀疏奖励下的复杂任务。

Conclusion: 本文证明了离策强化学习（Off-policy RL）可以可靠地重新引入真实世界的 VLA 后训练中，在不牺牲执行速度的前提下，显著提升了策略的鲁棒性和任务成功率。

Abstract: We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.

</details>


### [39] [Constrained PSO Six-Parameter Fuzzy PID Tuning Method for Balanced Optimization of Depth Tracking Performance in Underwater Vehicles](https://arxiv.org/abs/2602.12700)
*Yanxi Ding,Tingyue Jia*

Main category: cs.RO

TL;DR: 本文提出一种受约束的粒子群优化（PSO）方法，用于水下航行器六参数模糊PID深度控制器的联合调优，在满足执行器能量约束的同时显著提升了跟踪速度并降低了超调。


<details>
  <summary>Details</summary>
Motivation: 水下航行器深度控制需同时满足快速跟踪、低超调及执行器约束。然而，传统模糊PID调参多依赖经验，难以在性能提升与控制成本（能量消耗/执行器饱和）之间实现稳定的平衡。

Method: 提出一种基于约束粒子群算法（PSO）的六参数模糊PID调优方法。该方法同步优化基准PID参数、输入量化因子和输出比例增益，并构建了包含ITAE、调节时间、相对超调、控制能量及饱和占有率的约束驱动综合评价体系，通过限制控制能量来抑制无效的性能提升。

Result: 仿真结果表明，在控制能量（7980至7935）和饱和占有率（0.004至0.003）保持稳定的前提下，ITAE从0.2631降至0.1473，调节时间由2.301s缩短至1.613s，相对超调量从0.1494大幅优化至0.01839。

Conclusion: 该约束六参数联合调优策略成功解决了水下航行器在深度控制中的多目标平衡问题，在满足执行器约束的前提下显著提升了动态响应性能，具有极高的工程应用价值和可重复性。

Abstract: Depth control of underwater vehicles in engineering applications must simultaneously satisfy requirements for rapid tracking, low overshoot, and actuator constraints. Traditional fuzzy PID tuning often relies on empirical methods, making it difficult to achieve a stable and reproducible equilibrium solution between performance enhancement and control cost. This paper proposes a constrained particle swarm optimization (PSO) method for tuning six-parameter fuzzy PID controllers. By adjusting the benchmark PID parameters alongside the fuzzy controller's input quantization factor and output proportional gain, it achieves synergistic optimization of the overall tuning strength and dynamic response characteristics of the fuzzy PID system. To ensure engineering feasibility of the optimization results, a time-weighted absolute error integral, adjustment time, relative overshoot control energy, and saturation occupancy rate are introduced. Control energy constraints are applied to construct a constraint-driven comprehensive evaluation system, suppressing pseudo-improvements achieved solely by increasing control inputs. Simulation results demonstrate that, while maintaining consistent control energy and saturation levels, the proposed method significantly enhances deep tracking performance: the time-weighted absolute error integral decreases from 0.2631 to 0.1473, the settling time shortens from 2.301 s to 1.613 s, and the relative overshoot reduces from 0.1494 to 0.01839. Control energy varied from 7980 to 7935, satisfying the energy constraint, while saturation occupancy decreased from 0.004 to 0.003. These results validate the effectiveness and engineering significance of the proposed constrained six-parameter joint tuning strategy for depth control in underwater vehicle navigation scenarios.

</details>


### [40] [TRANS: Terrain-aware Reinforcement Learning for Agile Navigation of Quadruped Robots under Social Interactions](https://arxiv.org/abs/2602.12724)
*Wei Zhu,Irfan Tito Kurniawan,Ye Zhao,Mistuhiro Hayashibe*

Main category: cs.RO

TL;DR: 本文提出TRANS框架，通过深度强化学习实现了四足机器人在崎岖地形下的社交意识导航，解决了传统方法在动态复杂环境中的感知与控制集成难题。


<details>
  <summary>Details</summary>
Motivation: 传统四足机器人导航存在规划与控制脱节、忽视地形感知、端到端方法计算开销大以及难以应对动态社交环境（如人群）等局限性。

Method: 提出两阶段训练框架及三条DRL流水线：1. TRANS-Loco：利用非对称Actor-Critic模型实现无需显式感知的非结构化地形步态控制；2. TRANS-Nav：利用对称AC框架将LiDAR数据映射为具有社交意识的导航动作；3. TRANS整合流水线：将运动控制与社交导航统一。

Result: 在运动性能和社交导航基准测试中表现优于现有方案，并通过硬件实验验证了从仿真到现实（Sim-to-Real）的有效迁移能力。

Conclusion: TRANS框架成功实现了四足机器人在非结构化、动态社交环境中的敏捷导航，证明了分阶段强化学习在处理复杂机器人任务中的有效性。

Abstract: This study introduces TRANS: Terrain-aware Reinforcement learning for Agile Navigation under Social interactions, a deep reinforcement learning (DRL) framework for quadrupedal social navigation over unstructured terrains. Conventional quadrupedal navigation typically separates motion planning from locomotion control, neglecting whole-body constraints and terrain awareness. On the other hand, end-to-end methods are more integrated but require high-frequency sensing, which is often noisy and computationally costly. In addition, most existing approaches assume static environments, limiting their use in human-populated settings. To address these limitations, we propose a two-stage training framework with three DRL pipelines. (1) TRANS-Loco employs an asymmetric actor-critic (AC) model for quadrupedal locomotion, enabling traversal of uneven terrains without explicit terrain or contact observations. (2) TRANS-Nav applies a symmetric AC framework for social navigation, directly mapping transformed LiDAR data to ego-agent actions under differential-drive kinematics. (3) A unified pipeline, TRANS, integrates TRANS-Loco and TRANS-Nav, supporting terrain-aware quadrupedal navigation in uneven and socially interactive environments. Comprehensive benchmarks against locomotion and social navigation baselines demonstrate the effectiveness of TRANS. Hardware experiments further confirm its potential for sim-to-real transfer.

</details>


### [41] [SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies](https://arxiv.org/abs/2602.12794)
*Thies Oelerich,Gerald Ebmer,Christian Hartl-Nesic,Andreas Kugi*

Main category: cs.RO

TL;DR: 本文提出 SafeFlowMPC，一种结合流匹配与在线优化的机器人控制框架，在保证实时安全性的同时，实现了强大的任务泛化能力，并在 KUKA 机械臂的动态人机协作任务中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 旨在解决机器人从工业环境走向日常生活面临的挑战：学习类方法虽具泛化性但属于“黑盒”且缺乏安全保证，而传统的优化方法虽具备安全性但灵活性和泛化能力不足。目标是开发一种兼具两者优点的实时反应式控制框架。

Method: 提出 SafeFlowMPC 方法，将流匹配（Flow Matching）模型与在线优化算法相结合。该方法通过流匹配学习演示轨迹的分布以获取泛化能力，并利用基于亚优化模型预测控制（Suboptimal MPC）的公式来确保实时性及严格的安全性约束。

Result: 在 KUKA 7自由度机械臂上完成了三项真实世界实验（两项抓取实验及一项动态人机物体移交实验），结果显示该方法在保证安全的前提下表现出强劲的性能。

Conclusion: SafeFlowMPC 成功证明了将流匹配的生成能力与在线优化的严谨性相结合是可行的，为机器人在复杂、动态的人机交互环境中实现安全、灵活的作业提供了有效方案。

Abstract: The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.

</details>


### [42] [SKYSURF: A Self-learning Framework for Persistent Surveillance using Cooperative Aerial Gliders](https://arxiv.org/abs/2602.12838)
*Houssem Eddine Mohamadi,Nadjia Kara*

Main category: cs.RO

TL;DR: 本文提出了一种针对自主滑翔无人机的局部-全局协作决策框架，利用上升气流能量实现长时监控，在显著降低能耗的同时将目标探测效率提升了两倍。


<details>
  <summary>Details</summary>
Motivation: 小型无人机（UAV）由于车载电池容量有限，导致监控任务的续航时间受限。为解决此问题，研究者寻求利用上升气流等可再生升力源来延长无人机的留空时间。

Method: 1. 将协作无人机建模为基于非确定性有限状态的理性智能体。2. 提出一种局部-全局行为管理与决策框架，包含任务分配和动态导航点生成的任务规划模块。3. 结合可见性（Visibility）与预测（Prediction）概念设计避障路径规划方案。4. 采用延迟学习与调优策略（Delayed Learning and Tuning Strategy）对路径跟踪控制器的增益进行优化。

Result: 通过与3个基准方案及15种进化算法的对比实验表明：1. 目标检测率比非协作或半协作方法提高了两倍。2. 显著提升了监控持久性，在6小时的任务中仅消耗约6%的电池电量。

Conclusion: 该研究证明了通过合理的局部-全局行为管理和协作机制，具备滑翔能力的无人机群可以在大幅降低能耗的同时，显著提升复杂监控任务的覆盖率和持续时间。

Abstract: The success of surveillance applications involving small unmanned aerial vehicles (UAVs) depends on how long the limited on-board power would persist. To cope with this challenge, alternative renewable sources of lift are sought. One promising solution is to extract energy from rising masses of buoyant air. This paper proposes a local-global behavioral management and decision-making approach for the autonomous deployment of soaring-capable UAVs. The cooperative UAVs are modeled as non-deterministic finite state-based rational agents. In addition to a mission planning module for assigning tasks and issuing dynamic navigation waypoints for a new path planning scheme, in which the concepts of visibility and prediction are applied to avoid the collisions. Moreover, a delayed learning and tuning strategy is employed optimize the gains of the path tracking controller. Rigorous comparative analyses carried out with three benchmarking baselines and 15 evolutionary algorithms highlight the adequacy of the proposed approach for maintaining the surveillance persistency (staying aloft for longer periods without landing) and maximizing the detection of targets (two times better than non-cooperative and semi-cooperative approaches) with less power consumption (almost 6% of battery consumed in six hours).

</details>


### [43] [Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips](https://arxiv.org/abs/2602.12918)
*Iris Andrussow,Jans Solano,Benjamin A. Richardson,Georg Martius,Katherine J. Kuchenbecker*

Main category: cs.RO

TL;DR: 本文提出一种结合视觉（空间形变）与声学（高频振动）的双模态机器人触觉系统，通过模仿人类揉搓动作，在20种织物分类任务中达到97%的准确率。


<details>
  <summary>Details</summary>
Motivation: 人类通过整合空间力模式和纹理诱发的振动来识别织物，但机器人触觉传感器通常难以平衡高空间分辨率与高采样频率，导致其对复杂动态纹理的感知能力不足。

Method: 开发了一种双模态触觉感知系统：中指配备光学传感器 Minsight（通过摄像头捕捉形变与压力，采样率为50Hz），拇指配备新型声学传感器 Minsound（利用MEMS麦克风捕捉50Hz-15kHz的振动）。机器人模仿人类动作对织物进行包裹与揉搓。采用 Transformer 模型进行特征融合与分类，并引入外部麦克风以抵消环境噪声。

Result: 在20种常见织物的数据集上，该方法达到了97%的分类准确率。实验表明音频传感器对分类性能贡献显著。此外，系统成功学习到了织物拉伸性、厚度和粗糙度的通用表征，展示了良好的跨样本泛化能力。

Conclusion: 通过结合低频视觉形变感知（Minsight）与高频声学振动感知（Minsound），该系统能够有效模拟人类触觉整合机制。研究证明了声学模态在织物识别中的关键作用，且系统在噪声环境下具有鲁棒性，能够提取跨类别、可泛化的物理属性表征。

Abstract: Distinguishing the feel of smooth silk from coarse cotton is a trivial everyday task for humans. When exploring such fabrics, fingertip skin senses both spatio-temporal force patterns and texture-induced vibrations that are integrated to form a haptic representation of the explored material. It is challenging to reproduce this rich, dynamic perceptual capability in robots because tactile sensors typically cannot achieve both high spatial resolution and high temporal sampling rate. In this work, we present a system that can sense both types of haptic information, and we investigate how each type influences robotic tactile perception of fabrics. Our robotic hand's middle finger and thumb each feature a soft tactile sensor: one is the open-source Minsight sensor that uses an internal camera to measure fingertip deformation and force at 50 Hz, and the other is our new sensor Minsound that captures vibrations through an internal MEMS microphone with a bandwidth from 50 Hz to 15 kHz. Inspired by the movements humans make to evaluate fabrics, our robot actively encloses and rubs folded fabric samples between its two sensitive fingers. Our results test the influence of each sensing modality on overall classification performance, showing high utility for the audio-based sensor. Our transformer-based method achieves a maximum fabric classification accuracy of 97 % on a dataset of 20 common fabrics. Incorporating an external microphone away from Minsound increases our method's robustness in loud ambient noise conditions. To show that this audio-visual tactile sensing approach generalizes beyond the training data, we learn general representations of fabric stretchiness, thickness, and roughness.

</details>


### [44] [INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval](https://arxiv.org/abs/2602.12971)
*YukTungSamuel Fang,Zhikang Shi,Jiabin Qiu,Zixuan Chen,Jieqi Shi,Hao Xu,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出了 INHerit-SG，一种专为机器人导航设计的 RAG 驱动型层级语义场景图系统。它通过显式的语言描述锚点、异步双进程架构及 LLM 辅助的检索策略，解决了传统场景图在人类意图对齐和实时性方面的不足，实现了高效、可解释的 3D 环境感知与检索。


<details>
  <summary>Details</summary>
Motivation: 现有的 3D 语义场景图（SSG）通常依赖离线批量处理或隐式特征嵌入，导致其难以支持 embodied 任务中复杂且具解释性的人类意图推理。此外，在复杂动态环境中保持地图的长期一致性与低计算开销也是一大挑战。

Method: 1. **地图表示**：引入自然语言描述作为显式语义锚点，构建 RAG 就绪的知识库。
2. **架构设计**：采用异步双进程架构和“楼层-房间-区域-物体”的层级结构，实现几何分割与语义推理的解耦。
3. **更新机制**：采用事件触发的地图更新机制，仅在发生有意义的语义事件时重组图谱，降低功耗并保持长期一致性。
4. **检索策略**：部署多角色大语言模型（LLMs）分解复杂查询，结合“硬到软”过滤策略处理逻辑否定和原子约束。

Result: 在新建数据集 HM3DSem-SQR 和真实场景评估中，INHerit-SG 在处理复杂查询方面达到了 SOTA 性能，展现了极高的检索成功率、可靠性以及对复杂机器人导航任务的适配能力。

Conclusion: INHerit-SG 通过将 3D 环境抽象为显式的、可解释的知识库，显著提升了机器人在复杂室内环境中理解人类意图和处理复杂检索任务的能力，并为下游导航任务提供了良好的扩展性。

Abstract: Driven by advancements in foundation models, semantic scene graphs have emerged as a prominent paradigm for high-level 3D environmental abstraction in robot navigation. However, existing approaches are fundamentally misaligned with the needs of embodied tasks. As they rely on either offline batch processing or implicit feature embeddings, the maps can hardly support interpretable human-intent reasoning in complex environments. To address these limitations, we present INHerit-SG. We redefine the map as a structured, RAG-ready knowledge base where natural-language descriptions are introduced as explicit semantic anchors to better align with human intent. An asynchronous dual-process architecture, together with a Floor-Room-Area-Object hierarchy, decouples geometric segmentation from time-consuming semantic reasoning. An event-triggered map update mechanism reorganizes the graph only when meaningful semantic events occur. This strategy enables our graph to maintain long-term consistency with relatively low computational overhead. For retrieval, we deploy multi-role Large Language Models (LLMs) to decompose queries into atomic constraints and handle logical negations, and employ a hard-to-soft filtering strategy to ensure robust reasoning. This explicit interpretability improves the success rate and reliability of complex retrievals, enabling the system to adapt to a broader spectrum of human interaction tasks. We evaluate INHerit-SG on a newly constructed dataset, HM3DSem-SQR, and in real-world environments. Experiments demonstrate that our system achieves state-of-the-art performance on complex queries, and reveal its scalability for downstream navigation tasks. Project Page: https://fangyuktung.github.io/INHeritSG.github.io/

</details>


### [45] [Learning Native Continuation for Action Chunking Flow Policies](https://arxiv.org/abs/2602.12978)
*Yufeng Liu,Hang Yu,Juntu Zhao,Bocheng Li,Di Zhang,Mingzhu Li,Wenxuan Wu,Yingdong Hu,Junyuan Xie,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: Legato 是一种针对流式 VLA 模型的训练时连续性方法，通过重塑流动力学和混合动作初始化，显著提升了机器人动作分块执行的平滑度与效率。


<details>
  <summary>Details</summary>
Motivation: 虽然动作分块（Action Chunking）能提升 VLA 模型的实时性，但简单的分块执行常导致边界处轨迹不连续。现有方案（如 RTC）多为外部干预，易导致伪多模态切换（Spurious Multimodal Switching）且轨迹本质上不够平滑。

Method: 提出 Legato 方法：1. **初始化去噪**：从已知动作和噪声的混合分布中初始化去噪过程，使模型感知部分动作信息；2. **动力学重塑**：重塑流动力学（Flow Dynamics），确保每步引导下的训练与推理保持一致；3. **随机调度条件**：训练时采用随机调度以应对推理延迟并实现平滑度可控。

Result: 实验表明，Legato 在 5 项现实操控任务中表现优于 RTC，轨迹平滑度和任务完成时间均提升约 10%，显著减少了执行中的犹豫感和伪多模态切换。

Conclusion: Legato 通过在训练阶段引入连续性建模，有效地解决了基于流的 VLA 模型在分块执行时的不连续性问题，为实现高效、流畅的机器人动作控制提供了新方案。

Abstract: Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.

</details>


### [46] [How Swarms Differ: Challenges in Collective Behaviour Comparison](https://arxiv.org/abs/2602.13016)
*André Fialho Jesus,Jonas Kuckling*

Main category: cs.RO

TL;DR: 本文研究了群体机器人行为特征集与相似性度量的鲁棒性，提出一种基于SOM的方法来识别特征空间中难以区分的行为区域，旨在为自动设计群体行为提供更有效的量化评估手段。


<details>
  <summary>Details</summary>
Motivation: 群体行为通常需要通过数值特征进行表征，但现有研究多采用针对特定场景的临时特征集，缺乏对跨场景稳健性的考虑。自动设计群体行为依赖于能够定量衡量行为相似性的能力，因此需要深入探讨特征集对行为区分的影响。

Method: 从现有的群体机器人研究中选取多种特征集和相似性度量，并对其稳健性进行评估；提出了一种基于自组织映射（Self-Organised Map, SOM）的方法，用于识别特征空间中行为难以区分的区域。

Result: 实验证明特征集与相似性度量的结合方式显著影响区分相似行为的效果。研究确定了某些特定组合在区分行为组时更具优势，并通过SOM方法成功定位了特征空间中无法有效辨别行为的薄弱区域。

Conclusion: 特征集与相似性度量的选择对群体行为的量化评估至关重要。研究强调了在自动设计群体机器人行为时，需要根据具体语境选择最优的特征组合，并利用所提工具识别特征空间的局限性，以提升系统的鲁棒性。

Abstract: Collective behaviours often need to be expressed through numerical features, e.g., for classification or imitation learning. This problem is often addressed by proposing an ad-hoc feature set for a particular swarm behaviour context, usually without further consideration of the solution's resilience outside of the conceived context. Yet, the development of automatic methods to design swarm behaviours is dependent on the ability to measure quantitatively the similarity of swarm behaviours. Hence, we investigate the impact of feature sets for collective behaviours. We select swarm feature sets and similarity measures from prior swarm robotics works, which mainly considered a narrow behavioural context and assess their robustness. We demonstrate that the interplay of feature set and similarity measure makes some combinations more suitable to distinguish groups of similar behaviours. We also propose a self-organised map-based approach to identify regions of the feature space where behaviours cannot be easily distinguished.

</details>


### [47] [SENSE-STEP: Learning Sim-to-Real Locomotion for a Sensory-Enabled Soft Quadruped Robot](https://arxiv.org/abs/2602.13078)
*Storm de Kam,Ebrahim Shahabi,Cosimo Della Santina*

Main category: cs.RO

TL;DR: 本文介绍了一种针对气动软体四足机器人的学习型闭环控制框架，通过融合触觉与本体感知反馈，显著提升了机器人在平地及斜坡上的运动速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 软体四足机器人由于具有高维动力学特性、执行器迟滞效应以及难以精确建模的接触相互作用，实现鲁棒的闭环运动控制十分困难。此外，传统的本体感知手段难以提供充足的地面接触信息，限制了机器人在复杂地形上的表现。

Method: 提出一种基于学习的控制框架，应用于配备触觉吸盘足的气动软体四足机器人。采用分阶段学习策略（Staged Learning）：首先基于参考步态进行训练，随后在随机环境条件下进行迭代精炼。控制策略将本体感受与触觉力反馈实时映射为气动执行器和吸盘的协调动作指令。

Result: 在物理硬件上的实验表明，该闭环策略显著优于开环基准方案：在平地上的前进速度提升了41%，在5度斜坡上提升了91%。消融实验显示，引入触觉力估计和惯性反馈后，运动性能较无传感反馈配置提升了高达56%。

Conclusion: 该研究证明了通过集成触觉反馈和分阶段强化学习，可以有效解决软体四足机器人因高维动力学和非线性接触导致的控制难题。研究强调了多模态感知（触觉与惯性反馈）在增强软体机器人运动鲁棒性和环境适应性方面的核心价值。

Abstract: Robust closed-loop locomotion remains challenging for soft quadruped robots due to high-dimensional dynamics, actuator hysteresis, and difficult-to-model contact interactions, while conventional proprioception provides limited information about ground contact. In this paper, we present a learning-based control framework for a pneumatically actuated soft quadruped equipped with tactile suction-cup feet, and we validate the approach experimentally on physical hardware. The control policy is trained in simulation through a staged learning process that starts from a reference gait and is progressively refined under randomized environmental conditions. The resulting controller maps proprioceptive and tactile feedback to coordinated pneumatic actuation and suction-cup commands, enabling closed-loop locomotion on flat and inclined surfaces. When deployed on the real robot, the closed-loop policy outperforms an open-loop baseline, increasing forward speed by 41% on a flat surface and by 91% on a 5-degree incline. Ablation studies further demonstrate the role of tactile force estimates and inertial feedback in stabilizing locomotion, with performance improvements of up to 56% compared to configurations without sensory feedback.

</details>


### [48] [Agentic AI for Robot Control: Flexible but still Fragile](https://arxiv.org/abs/2602.13081)
*Oscar Lima,Marc Vinci,Martin Günther,Marian Renz,Alexander Sung,Sebastian Stock,Johannes Brust,Lennart Niecksch,Zongyao Yi,Felix Igelbrink,Benjamin Kisliuk,Martin Atzmueller,Joachim Hertzberg*

Main category: cs.RO

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work leverages the capabilities and commonsense priors of generative models for robot control. In this paper, we present an agentic control system in which a reasoning-capable language model plans and executes tasks by selecting and invoking robot skills within an iterative planner and executor loop. We deploy the system on two physical robot platforms in two settings: (i) tabletop grasping, placement, and box insertion in indoor mobile manipulation (Mobipick) and (ii) autonomous agricultural navigation and sensing (Valdemar). Both settings involve uncertainty, partial observability, sensor noise, and ambiguous natural-language commands. The system exposes structured introspection of its planning and decision process, reacts to exogenous events via explicit event checks, and supports operator interventions that modify or redirect ongoing execution. Across both platforms, our proof-of-concept experiments reveal substantial fragility, including non-deterministic suboptimal behavior, instruction-following errors, and high sensitivity to prompt specification. At the same time, the architecture is flexible: transfer to a different robot and task domain largely required updating the system prompt (domain model, affordances, and action catalogue) and re-binding the same tool interface to the platform-specific skill API.

</details>


### [49] [UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph](https://arxiv.org/abs/2602.13086)
*Haichao Liu,Yuanjiang Xue,Yuheng Zhou,Haoyuan Deng,Yinan Liang,Lihua Xie,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出 UniManip 框架，通过双层智能体操作图（AOG）统一语义推理与物理约束，实现了高精度、具备故障恢复能力的通用机器人零样本操作。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端视觉-语言-动作（VLA）模型在长程任务中精度不足，而传统的层级规划器在应对开放世界的语义变化时过于僵化，导致在非结构化环境下的零样本（zero-shot）泛化表现欠佳。

Method: 提出 UniManip 框架，其核心是双层智能体操作图（Bi-level Agentic Operational Graph, AOG）：
1. **智能体层（Agentic Layer）**：负责高层任务编排与语义推理。
2. **场景层（Scene Layer）**：从非结构化感知中实时构建以物体为中心的场景图，提供动态状态表示。
3. **闭环执行机制**：通过安全感知局部规划器将抽象规划转化为无碰撞轨迹，并利用结构化存储（Structured Memory）实现自主故障诊断与恢复。

Result: 实验表明，UniManip 在处理未知物体和任务时的成功率分别比最先进的 VLA 模型和层级基准模型高出 22.5% 和 25.0%。此外，该系统实现了从固定基座到移动操作任务的直接零样本迁移，无需任何微调或重新配置。

Conclusion: UniManip 通过解耦并实时对齐高层语义推理与底层几何约束，显著增强了机器人在非结构化环境中的零样本泛化能力、长程任务稳定性和跨平台迁移能力，为实现通用机器人操作提供了一种高效的架构方案。

Abstract: Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system's robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip.

</details>


### [50] [Temporally-Sampled Efficiently Adaptive State Lattices for Autonomous Ground Robot Navigation in Partially Observed Environments](https://arxiv.org/abs/2602.13159)
*Ashwin Satish Menon,Eric R. Damm,Eli S. Lancaster,Felix A. Sanchez,Jason M. Gregory,Thomas M. Howard*

Main category: cs.RO

TL;DR: 针对越野机器人因地图动态更新导致路径规划不稳定的问题，本文提出 TSEASL 架构，通过仲裁历史优化轨迹与当前轨迹，显著提升了导航的安全性和规划平滑度，减少了人工干预。


<details>
  <summary>Details</summary>
Motivation: 在越野环境下，由于传感器的局限性，环境地图会随机器人移动不断更新，导致区域规划器输出的参考轨迹在相邻周期内发生剧烈跳变。这种不稳定的引导信号容易导致机器人产生危险的导航行为，通常需要人工干预才能确保安全。

Method: 提出了一种名为 TSEASL（Temporally-Sampled Efficiently Adaptive State Lattices）的区域规划仲裁架构。该方法不再仅仅依赖当前周期的规划结果，而是将之前生成的轨迹进行更新和优化，并将其与当前新生成的轨迹进行对比和仲裁，从而选择最优且最平滑的路径。

Result: 在 Clearpath Robotics Warthog 无人地面车辆（UGV）上的实测及真实地图数据验证表明，相比于基准规划器，使用 TSEASL 的机器人在相同位置不再需要人工干预，且记录到了更高水平的规划稳定性。

Conclusion: TSEASL 能够显著提升越野机器人在复杂、部分可观测环境下的导航安全性和规划稳定性。未来研究将侧重于进一步优化该算法，以提高其在各种越野自主场景中的泛化能力。

Abstract: Due to sensor limitations, environments that off-road mobile robots operate in are often only partially observable. As the robots move throughout the environment and towards their goal, the optimal route is continuously revised as the sensors perceive new information. In traditional autonomous navigation architectures, a regional motion planner will consume the environment map and output a trajectory for the local motion planner to use as a reference. Due to the continuous revision of the regional plan guidance as a result of changing map information, the reference trajectories which are passed down to the local planner can differ significantly across sequential planning cycles. This rapidly changing guidance can result in unsafe navigation behavior, often requiring manual safety interventions during autonomous traversals in off-road environments. To remedy this problem, we propose Temporally-Sampled Efficiently Adaptive State Lattices (TSEASL), which is a regional planner arbitration architecture that considers updated and optimized versions of previously generated trajectories against the currently generated trajectory. When tested on a Clearpath Robotics Warthog Unmanned Ground Vehicle as well as real map data collected from the Warthog, results indicate that when running TSEASL, the robot did not require manual interventions in the same locations where the robot was running the baseline planner. Additionally, higher levels of planner stability were recorded with TSEASL over the baseline. The paper concludes with a discussion of further improvements to TSEASL in order to make it more generalizable to various off-road autonomy scenarios.

</details>


### [51] [Human Emotion-Mediated Soft Robotic Arts: Exploring the Intersection of Human Emotions, Soft Robotics and Arts](https://arxiv.org/abs/2602.13163)
*Saitarun Nadipineni,Chenhao Hong,Tanishtha Ramlall,Chapa Sirithunge,Kaspar Althoefer,Fumiya Iida,Thilina Dulantha Lalitharatne*

Main category: cs.RO

TL;DR: 本研究开发了一种由人类情绪（Alpha脑电波）驱动的软体机器人艺术系统，通过软体角色和花朵的动态变形来具身化表达人类的情感状态。


<details>
  <summary>Details</summary>
Motivation: 旨在探索人类情感、软体机器人与艺术的交叉点，利用软体机器人天然的柔韧性和生物模拟特性，创造一种能实时响应人类生理情绪信号的新型艺术表现形式。

Method: 设计了两种软体装置（软体角色与软体花朵），通过脑电图（EEG）采集人类Alpha波以衡量情绪水平，并将该信号映射为软体装置的动态变形动作，最后通过实验验证其交互效果。

Result: 成功开发并演示了能够根据脑波信号实时调整形态的软体艺术装置，实现了人类情绪状态向物理形态变化的动态转化。

Conclusion: 软体机器人可作为体现人类情绪状态的新媒介，为艺术表达和交互提供了新途径，证明了艺术展示实现具身化（Embodied）的可行性。

Abstract: Soft robotics has emerged as a versatile field with applications across various domains, from healthcare to industrial automation, and more recently, art and interactive installations. The inherent flexibility, adaptability, and safety of soft robots make them ideal for applications that require delicate, organic, and lifelike movement, allowing for immersive and responsive interactions. This study explores the intersection of human emotions, soft robotics, and art to establish and create new forms of human emotion-mediated soft robotic art. In this paper, we introduce two soft embodiments: a soft character and a soft flower as an art display that dynamically responds to brain signals based on alpha waves, reflecting different emotion levels. We present how human emotions can be measured as alpha waves based on brain/EEG signals, how we map the alpha waves to the dynamic movements of the two soft embodiments, and demonstrate our proposed concept using experiments. The findings of this study highlight how soft robotics can embody human emotional states, offering a new medium for insightful artistic expression and interaction, and demonstrating how art displays can be embodied.

</details>


### [52] [Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control](https://arxiv.org/abs/2602.13193)
*William Chen,Jagdeep Singh Bhatia,Catherine Glossop,Nikhil Mathihalli,Ria Doshi,Andy Tang,Danny Driess,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出 Steerable Policies，通过在多层级抽象指令（运动、坐标等）上训练 VLA 模型，增强了对机器人底层行为的可控性，从而能更好地利用预训练 VLM 的推理能力来完成复杂的具身任务。


<details>
  <summary>Details</summary>
Motivation: 现有的具身智能架构通常采用层次化方法（VLM 提供高层推理，VLA 执行底层动作），但两者之间的接口主要依赖自然语言指令。这种单一且抽象的接口限制了 VLM 推理结果对机器人底层行为的精细引导，导致模型在复杂任务和泛化场景中表现受限。

Method: 1. **Steerable Policies 训练**：在涵盖不同抽象层级（如具体子任务、运动指令、像素坐标等）的丰富合成指令集上训练 VLA 模型。
2. **多层级抽象接口**：超越传统的单一自然语言指令，提供更精细的控制接口。
3. **双重控制机制**：分别利用“学习型高层具身推理器”和“现成 VLM（通过上下文学习 Prompt）”来生成抽象指令，从而引导底层 Steerable Policies 执行任务。

Result: 在广泛的真实世界机器人操控实验中，Steerable Policies 在挑战性的泛化任务和长程（long-horizon）任务上表现出色，其性能显著优于现有的具身推理 VLA 模型以及基于 VLM 的层次化基准方法。

Conclusion: 通过提升底层策略（VLA）的可控性和指令粒度，可以更有效地释放预训练 VLM 的具身推理潜力。这种增强的接口不仅提高了任务泛化能力，还为处理复杂、长程的机器人操控任务提供了更有效的途径。

Abstract: Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks.
  Website: steerable-policies.github.io

</details>


### [53] [Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos](https://arxiv.org/abs/2602.13197)
*Albert J. Zhai,Kuo-Hao Zeng,Jiasen Lu,Ali Farhadi,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.RO

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [54] [A Lightweight LLM Framework for Disaster Humanitarian Information Classification](https://arxiv.org/abs/2602.12284)
*Han Jinzhen,Kim Jisung,Yang Jong Soo,Yun Hong Sik*

Main category: cs.CL

TL;DR: 本文提出了一种基于Llama 3.1 8B的轻量化灾难推文分类框架。通过LoRA和QLoRA微调，在极低资源消耗下显著提升了分类精度，同时发现RAG在微调模型上因噪声问题表现不佳。


<details>
  <summary>Details</summary>
Motivation: 在灾难响应中，及时对社交媒体的人道主义信息进行分类至关重要，但在资源受限的紧急环境下，部署大型语言模型（LLMs）面临计算资源不足和成本过高的挑战。

Method: 基于Llama 3.1 8B模型，通过整合HumAID数据集（包含19个灾难事件的7.6万余条推文）构建了统一的双任务基准。研究系统地评估并对比了提示策略（Prompting）、LoRA参数高效微调、QLoRA量化微调以及检索增强生成（RAG）的表现。

Result: (1) LoRA仅训练约2%的参数即可达到79.62%的准确率（比零样本提升37.79%）；(2) QLoRA在节省50%显存的情况下保留了LoRA 99.4%的性能；(3) RAG策略因检索到的样本存在标签噪声，反而会降低微调模型的性能。

Conclusion: 本研究建立了一套实用且可重复的流水线，证明了在有限计算资源下，通过轻量化微调技术构建可靠的危机情报系统是可行的。

Abstract: Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for disaster tweet classification using parameter-efficient fine-tuning. We construct a unified experimental corpus by integrating and normalizing the HumAID dataset (76,484 tweets across 19 disaster events) into a dual-task benchmark: humanitarian information categorization and event type identification. Through systematic evaluation of prompting strategies, LoRA fine-tuning, and retrieval-augmented generation (RAG) on Llama 3.1 8B, we demonstrate that: (1) LoRA achieves 79.62% humanitarian classification accuracy (+37.79% over zero-shot) while training only ~2% of parameters; (2) QLoRA enables efficient deployment with 99.4% of LoRA performance at 50% memory cost; (3) contrary to common assumptions, RAG strategies degrade fine-tuned model performance due to label noise from retrieved examples. These findings establish a practical, reproducible pipeline for building reliable crisis intelligence systems with limited computational resources.

</details>


### [55] [From Biased Chatbots to Biased Agents: Examining Role Assignment Effects on LLM Agent Robustness](https://arxiv.org/abs/2602.12285)
*Linbo Cao,Lihao Sun,Yang Yue*

Main category: cs.CL

TL;DR: 本研究首次系统探讨了人格设定对LLM智能体性能的影响，发现基于人口统计的无关人格提示会导致智能体在推理与规划任务中出现高达26.2%的性能退化，揭示了智能体系统决策可靠性中的重大安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在文本生成中的偏见已得到广泛研究，但其作为自主智能体在执行具有现实影响的任务时，人格诱导偏见如何影响其操作性能及决策可靠性仍处于探索空白，这直接关系到智能体的操作风险。

Method: 通过在涵盖策略推理、规划和技术操作的智能体基准测试中，为广泛使用的LLM模型分配基于人口统计学（Demographic-based）的人格特征，系统性地评估模型在不同任务场景下的行为一致性与执行效率。

Result: 研究发现人口统计人格分配会显著改变智能体行为并降低性能，在不同领域中最高导致26.2%的性能下降。这些性能波动是由任务无关的人格线索驱动的，且在多种模型架构和任务类型中表现出一致性。

Conclusion: 人格分配会向LLM智能体引入隐性偏见并增加行为波动性。这种现象在不同模型架构中普遍存在，揭示了当前智能体系统在安全性和鲁棒性部署方面存在被忽视的脆弱性。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents capable of actions with real-world impacts beyond text generation. While persona-induced biases in text generation are well documented, their effects on agent task performance remain largely unexplored, even though such effects pose more direct operational risks. In this work, we present the first systematic case study showing that demographic-based persona assignments can alter LLM agents' behavior and degrade performance across diverse domains. Evaluating widely deployed models on agentic benchmarks spanning strategic reasoning, planning, and technical operations, we uncover substantial performance variations - up to 26.2% degradation, driven by task-irrelevant persona cues. These shifts appear across task types and model architectures, indicating that persona conditioning and simple prompt injections can distort an agent's decision-making reliability. Our findings reveal an overlooked vulnerability in current LLM agentic systems: persona assignments can introduce implicit biases and increase behavioral volatility, raising concerns for the safe and robust deployment of LLM agents.

</details>


### [56] [Retrieval-Augmented Self-Taught Reasoning Model with Adaptive Chain-of-Thought for ASR Named Entity Correction](https://arxiv.org/abs/2602.12287)
*Junjie An,Jingguang Tian,Tianyi Wang,Yu Gao,Xiaofeng Mou,Yi Xu*

Main category: cs.CL

TL;DR: 本研究提出了一种基于检索增强和自适应思维链推理（A-STAR）的框架，旨在利用LLM的推理能力纠正自动语音识别中的命名实体错误。


<details>
  <summary>Details</summary>
Motivation: 端到端ASR系统在识别命名实体等特定领域短语时经常出错，且现有的LLM纠错方法未能充分利用其复杂的推理能力。

Method: 提出一种新型检索增强生成（RAG）框架，包含：(1) 改写语言模型（RLM），用于命名实体识别并结合音素级编辑距离进行候选词检索；(2) 自教推理模型 A-STAR（Adaptive Chain-of-Thought），可根据任务难度动态调整推理深度。

Result: 在 AISHELL-1 和 Homophone 数据集上，该方法使命名实体字错误率（CER）分别相对降低了 17.96% 和 34.42%。

Conclusion: 该研究证明了结合检索增强生成与自适应推理能力可以显著提升ASR系统对命名实体的纠错性能，为解决语音识别中的特定领域术语误识别问题提供了有效方案。

Abstract: End-to-end automatic speech recognition (ASR) systems frequently misrecognize domain-specific phrases like named entities, which can cause catastrophic failures in downstream tasks. A new family of named entity correction methods based on large language models (LLMs) has recently emerged. However, these approaches have yet to fully exploit the sophisticated reasoning capabilities inherent to LLMs. To bridge this gap, we propose a novel retrieval-augmented generation framework for correcting named entity errors in ASR. Our approach consists of two key components: (1) a rephrasing language model (RLM) for named entity recognition, followed by candidate retrieval using a phonetic-level edit distance; and (2) a novel self-taught reasoning model with adaptive chain-of-thought (A-STAR) that dynamically adjusts the depth of its reasoning based on task difficulty. Experiments on the AISHELL-1 and Homophone datasets demonstrate the effectiveness of our method, which achieves relative reductions in the named entity character error rate of 17.96\% and 34.42\%, respectively, compared to a strong baseline.

</details>


### [57] [Grandes Modelos de Linguagem Multimodais (MLLMs): Da Teoria à Prática](https://arxiv.org/abs/2602.12302)
*Neemias da Silva,Júlio C. W. Scholz,John Harrison,Marina Borges,Paulo Ávila,Frances A Santos,Myriam Delgado,Rodrigo Minetto,Thiago H Silva*

Main category: cs.CL

TL;DR: 本文是一篇关于多模态大语言模型（MLLMs）的综合综述与实践指南，涵盖了基础理论、代表性模型、基于 LangChain 和 LangGraph 的开发实战以及未来趋势展望。


<details>
  <summary>Details</summary>
Motivation: 随着 MLLMs 将 LLM 的自然语言处理能力与图像、音频等感知能力相结合，成为当前人工智能领域的关键进展，开发者和研究者需要一套结合理论基础与实战指南的参考资料。

Method: 本文系统地介绍了 MLLMs 的基本原理和典型模型，深入探讨了预处理、提示工程等实用技术，并详细说明了如何利用 LangChain 和 LangGraph 构建多模态工作流（Pipelines）。

Result: 提供了一个涵盖理论与实践的综合框架，包括公开的配套代码库（GitHub），旨在帮助读者掌握 MLLMs 的开发流程与应用构建。

Conclusion: 文章最后总结了 MLLMs 面临的主要挑战，并指出了该领域值得关注的未来发展趋势。

Abstract: Multimodal Large Language Models (MLLMs) combine the natural language understanding and generation capabilities of LLMs with perception skills in modalities such as image and audio, representing a key advancement in contemporary AI. This chapter presents the main fundamentals of MLLMs and emblematic models. Practical techniques for preprocessing, prompt engineering, and building multimodal pipelines with LangChain and LangGraph are also explored. For further practical study, supplementary material is publicly available online: https://github.com/neemiasbsilva/MLLMs-Teoria-e-Pratica. Finally, the chapter discusses the challenges and highlights promising trends.

</details>


### [58] [RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty](https://arxiv.org/abs/2602.12424)
*Ziqian Zhang,Xingjian Hu,Yue Huang,Kai Zhang,Ruoxi Chen,Yixin Liu,Qingsong Wen,Kaidi Xu,Xiangliang Zhang,Neil Zhenqiang Gong,Lichao Sun*

Main category: cs.CL

TL;DR: RankLLM 是一个通过双向分数传播机制同时评估题目难度与模型能力的框架，相比传统方法能更精准地实现细粒度 LLM 能力对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 评估基准忽视了题目难度的差异，导致无法有效区分不同模型之间的能力梯度。需要一种能够同时衡量题目挑战性和模型水平的评估框架。

Method: 提出 RankLLM 框架，通过双向分数传播（Bidirectional Score Propagation）机制同时量化模型胜任力与题目难度：模型答对加分，题目“难倒”模型则增加难度分。该机制旨在实现模型与题目能力的细粒度对齐。

Result: 在涉及 30 个模型和 35,550 道题目的跨领域测试中，RankLLM 与人类判断的一致性达 90%，性能超越了项目反应理论（IRT）等基准，并表现出良好的稳定性、收敛速度和计算效率。

Conclusion: RankLLM 为大规模 LLM 评测提供了一种感知难度（difficulty-aware）且兼顾计算效率的实用解决方案，解决了传统基准测试在模型区分度上的局限性。

Abstract: Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability to effectively distinguish models' capabilities. To address this limitation, we propose RankLLM, a novel framework designed to quantify both question difficulty and model competency. RankLLM introduces difficulty as the primary criterion for differentiation, enabling a more fine-grained evaluation of LLM capabilities. RankLLM's core mechanism facilitates bidirectional score propagation between models and questions. The core intuition of RankLLM is that a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Using this framework, we evaluate 30 models on 35,550 questions across multiple domains. RankLLM achieves 90% agreement with human judgments and consistently outperforms strong baselines such as IRT. It also exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.

</details>


### [59] [RBCorr: Response Bias Correction in Language Models](https://arxiv.org/abs/2602.12445)
*Om Bhatt,Anna A. Ivanova*

Main category: cs.CL

TL;DR: 本文提出 RBCorr 方法，通过纠正语言模型在固定选项任务中的响应偏差，以极低成本提升模型性能并实现更准确的能力评估。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理固定响应问题时容易出现选项偏好等响应偏差，这不仅影响了模型的任务表现，还导致无法对其真实能力进行准确评估。因此，亟需开发低成本且有效的纠正方法。

Method: 提出了一种名为 RBCorr 的响应偏差纠正策略。该方法主要基于 LogProbs（对数概率）进行处理，并在 12 个开源权重语言模型上进行了测试，涵盖了“是/否”问答、文本蕴含和多项选择题等多种任务格式。

Result: 实验证明响应偏差在纠正前的模型中普遍存在。应用 RBCorr 后，偏差被有效消除，模型性能得到提升。研究还发现，基于 LogProbs 的纠正效果高度依赖于特定的模型、数据集和提示（Prompt）格式。

Conclusion: RBCorr 是一种易于使用的工具，能够显著提升小型语言模型的表现，并确保闭卷响应基准测试的结果能更真实地反映模型的实际能力。

Abstract: Language models (LMs) are known to be prone to response biases, which present as option preference biases in fixed-response questions. It is therefore imperative to develop low-cost and effective response bias correction methods to improve LM performance and enable more accurate evaluations of model abilities. Here, we propose a simple response bias correction strategy ($\texttt{RBCorr}$) and test it on 12 open-weight language models using yes-no, entailment, and multiple choice questions. We show that response bias is prevalent in LMs pre-correction and that $\texttt{RBCorr}$ effectively eliminates bias and boosts model performance. We also explore the generalizability of bias behavior across models, datasets, and prompt formats, showing that LogProbs-based correction is highly dependent on all three of these aspects. Overall, $\texttt{RBCorr}$ is an easy-to-use method that can boost the performance of smaller LMs and ensure that LM performance on closed-response benchmarks aligns more closely with their true capabilities.

</details>


### [60] [Discovering Semantic Latent Structures in Psychological Scales: A Response-Free Pathway to Efficient Simplification](https://arxiv.org/abs/2602.12575)
*Bo Wang,Yuxuan Zhang,Yueqin Hu,Hanchao Hou,Kaiping Peng,Shiguang Ni*

Main category: cs.CL

TL;DR: 本文提出一种利用自然语言处理（语义嵌入与聚类）进行心理量表缩减的框架，无需收集被试响应数据即可实现平均60.5%的题量精简，且保持了良好的心理测量学效能。


<details>
  <summary>Details</summary>
Motivation: 传统的心理量表优化方法（如因素分析、IRT）依赖大规模样本响应数据，存在数据获取难和跨文化比较受限的问题。本文旨在探索是否可以利用NLP技术，通过分析问卷题目的语义结构来实现无需响应数据的量表简化。

Method: 提出一种基于主题建模的量表缩减框架：1. 使用上下文句子嵌入（contextual sentence embeddings）对题目进行编码；2. 采用基于密度的聚类发现潜在语义因素；3. 利用基于类别的词项加权生成可解释的主题表示；4. 合并语义邻近的簇并根据成员标准筛选代表性题目。

Result: 在DASS、IPIP和EPOCH量表上的测试表明，该框架平均可将量表长度缩减60.5%，且在结构恢复、内部一致性、因素一致性和相关性保留方面表现优异，缩减后的量表与原始因素结构保持高度一致。

Conclusion: 语义潜结构（semantic latent structure）可以作为测量结构的有效响应无关近似（response-free approximation）。该研究不仅形式化了这一框架，还提供了一个可视化工具，证明了在无需大规模样本数据的情况下，利用语义分析进行心理量表构建和缩减的可行性与效率。

Abstract: Psychological scale refinement traditionally relies on response-based methods such as factor analysis, item response theory, and network psychometrics to optimize item composition. Although rigorous, these approaches require large samples and may be constrained by data availability and cross-cultural comparability. Recent advances in natural language processing suggest that the semantic structure of questionnaire items may encode latent construct organization, offering a complementary response-free perspective. We introduce a topic-modeling framework that operationalizes semantic latent structure for scale simplification. Items are encoded using contextual sentence embeddings and grouped via density-based clustering to discover latent semantic factors without predefining their number. Class-based term weighting derives interpretable topic representations that approximate constructs and enable merging of semantically adjacent clusters. Representative items are selected using membership criteria within an integrated reduction pipeline. We benchmarked the framework across DASS, IPIP, and EPOCH, evaluating structural recovery, internal consistency, factor congruence, correlation preservation, and reduction efficiency. The proposed method recovered coherent factor-like groupings aligned with established constructs. Selected items reduced scale length by 60.5% on average while maintaining psychometric adequacy. Simplified scales showed high concordance with original factor structures and preserved inter-factor correlations, indicating that semantic latent organization provides a response-free approximation of measurement structure. Our framework formalizes semantic structure as an inspectable front-end for scale construction and reduction. To facilitate adoption, we provide a visualization-supported tool enabling one-click semantic analysis and structured simplification.

</details>


### [61] [Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats](https://arxiv.org/abs/2602.12635)
*Pengxiang Zhao,Hui-Ling Zhen,Xing Li,Han Bao,Weizhe Lin,Zhiyuan Yang,Ziwei Yu,Xin Wang,Mingxuan Yuan,Xianzhi Yu,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本文评估了专为昇腾 NPU 设计的 HiFloat (HiF8/HiF4) 低比特浮点格式，证明其在处理高方差数据和 4-bit 量化任务时优于传统定点格式，是实现高效 LLM 推理的有效方案。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM 规模的增长，低比特浮点格式（如 MXFP, NVFP4）成为平衡精度与效率的关键，但仍需针对昇腾 NPU 架构开发优化的低比特格式以提升推理性能。

Method: 提出了专为昇腾（Ascend）NPU 设计的 HiFloat 格式（包含 HiF8 和 HiF4），并在权重-激活（Weight-Activation）和 KV-Cache 任务中进行了严谨的对比实验，同时将其集成至主流的后训练量化（PTQ）框架中进行验证。

Result: 1. INT8 适用于窄范围数据，而浮点格式在处理高方差数据时更具优势；2. 在 4-bit 量化场景下，HiF4 的层次化缩放机制有效防止了定点格式常见的精度崩溃现象；3. HiFloat 展现了与现有 SOTA PTQ 框架的完全兼容性。

Conclusion: HiFloat 系列格式为昇腾 NPU 提供了一种高效的量化方案，能够在保证推理效率的同时，显著提升大语言模型在低比特环境下的数值精度。

Abstract: As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.

</details>


### [62] [CLASE: A Hybrid Method for Chinese Legalese Stylistic Evaluation](https://arxiv.org/abs/2602.12639)
*Yiran Rex Ma,Yuxiao Ye,Huiyuan Xie*

Main category: cs.CL

TL;DR: 本文介绍了 CLASE，一种针对中文法律写作风格的混合评估方法，通过结合语言特征分析与经验增强的 LLM 评分，解决了法律文本专业性难以自动量化的难题。


<details>
  <summary>Details</summary>
Motivation: LLM 生成的法律文本虽事实准确，但往往不符合法律写作的专业风格规范。现有的手动评估成本极高且难以量化，而传统自动指标（如基于参考的指标）易混淆语义与风格，纯 LLM 评分则存在透明度低、一致性差的问题。

Method: 提出 CLASE（中文法律风格评估）混合评估框架。该方法结合了：1) 基于语言特征的统计评分；2) 经验引导的 LLM-as-a-judge 评分。其核心创新在于通过真实法律文档与 LLM 还原文档的对比对（Contrastive Pairs）来学习特征权重和评分经验，实现无参考（Reference-free）评估。

Result: 在 200 份中文法律文档上的实验结果显示，CLASE 与人类判断的一致性显著优于传统评估指标和纯 LLM 评分方法，并能提供具体的得分细目和改进建议。

Conclusion: CLASE 证明了结合统计特征与模型经验的混合方案在高度专业化文本评估中的优越性，为法律文本生成的风格优化提供了可扩展且具解释性的工具。

Abstract: Legal text generated by large language models (LLMs) can usually achieve reasonable factual accuracy, but it frequently fails to adhere to the specialised stylistic norms and linguistic conventions of legal writing. In order to improve stylistic quality, a crucial first step is to establish a reliable evaluation method. However, having legal experts manually develop such a metric is impractical, as the implicit stylistic requirements in legal writing practice are difficult to formalise into explicit rubrics. Meanwhile, existing automatic evaluation methods also fall short: reference-based metrics conflate semantic accuracy with stylistic fidelity, and LLM-as-a-judge evaluations suffer from opacity and inconsistency. To address these challenges, we introduce CLASE (Chinese LegAlese Stylistic Evaluation), a hybrid evaluation method that focuses on the stylistic performance of legal text. The method incorporates a hybrid scoring mechanism that combines 1) linguistic feature-based scores and 2) experience-guided LLM-as-a-judge scores. Both the feature coefficients and the LLM scoring experiences are learned from contrastive pairs of authentic legal documents and their LLM-restored counterparts. This hybrid design captures both surface-level features and implicit stylistic norms in a transparent, reference-free manner. Experiments on 200 Chinese legal documents show that CLASE achieves substantially higher alignment with human judgments than traditional metrics and pure LLM-as-a-judge methods. Beyond improved alignment, CLASE provides interpretable score breakdowns and suggestions for improvements, offering a scalable and practical solution for professional stylistic evaluation in legal text generation (Code and data for CLASE is available at: https://github.com/rexera/CLASE).

</details>


### [63] [Beyond Normalization: Rethinking the Partition Function as a Difficulty Scheduler for RLVR](https://arxiv.org/abs/2602.12642)
*Dohyung Kim,Minbeom Kim,Jeonghye Kim,Sangmook Lee,Sojeong Rhee,Kyomin Jung*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reward-maximizing RL methods enhance the reasoning performance of LLMs, but often reduce the diversity among outputs. Recent works address this issue by adopting GFlowNets, training LLMs to match a target distribution while jointly learning its partition function. In contrast to prior works that treat this partition function solely as a normalizer, we reinterpret it as a per-prompt expected-reward (i.e., online accuracy) signal, leveraging this unused information to improve sample efficiency. Specifically, we first establish a theoretical relationship between the partition function and per-prompt accuracy estimates. Building on this key insight, we propose Partition Function-Guided RL (PACED-RL), a post-training framework that leverages accuracy estimates to prioritize informative question prompts during training, and further improves sample efficiency through an accuracy estimate error-prioritized replay. Crucially, both components reuse information already produced during GFlowNet training, effectively amortizing the compute overhead into the existing optimization process. Extensive experiments across diverse benchmarks demonstrate strong performance improvements over GRPO and prior GFlowNet approaches, highlighting PACED-RL as a promising direction for a more sample efficient distribution-matching training for LLMs.

</details>


### [64] [Learning Ordinal Probabilistic Reward from Preferences](https://arxiv.org/abs/2602.12660)
*Longze Chen,Lu Wang,Renke Shan,Ze Gong,Run Luo,Jiaming Li,Jing Luo,Qiyao Wang,Min Yang*

Main category: cs.CL

TL;DR: 本文提出了一种概率奖励模型 (PRM) 及其离散实现 OPRM，通过区域泛洪微调 (RgFT) 策略，实现了对文本相对排名和绝对质量的有效建模，显著提升了奖励模型的准确率和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型（RM）主要分为生成式（GRM）和判别式（DRM）。GRM 依赖昂贵的逐点监督，而 DRM 产生的相对得分缺乏概率解释且未经校准。本文旨在克服这些局限，构建一种既能反映相对排序又能体现绝对质量的奖励模型。

Method: 1. **概率奖励模型 (PRM)**：将奖励建模为随机变量而非确定性标量，学习响应质量的完整概率分布。
2. **序数概率奖励模型 (OPRM)**：PRM 的离散化实现，将质量分数划分为有限的序数评级集合。
3. **区域泛洪微调 (RgFT)**：一种高效的训练策略，利用质量级标注引导模型将概率质量集中在对应的评分亚区域，从而更好地反映绝对文本质量。

Result: 在多个奖励模型基准测试中，该方法相比之前的模型准确率提升了 **2.9%~7.4%**。分析表明，该模型在保持强排序能力的同时，其得分分布能有效体现文本的绝对质量。

Conclusion: PRM 不仅能够捕捉文本之间的相对排名，还能有效反映文本的绝对质量，为大语言模型的对齐提供了一种兼具高性能、数据效率和概率解释性的新范式。

Abstract: Reward models are crucial for aligning large language models (LLMs) with human values and intentions. Existing approaches follow either Generative (GRMs) or Discriminative (DRMs) paradigms, yet both suffer from limitations: GRMs typically demand costly point-wise supervision, while DRMs produce uncalibrated relative scores that lack probabilistic interpretation. To address these challenges, we introduce a novel reward modeling paradigm: Probabilistic Reward Model (PRM). Instead of modeling reward as a deterministic scalar, our approach treats it as a random variable, learning a full probability distribution for the quality of each response. To make this paradigm practical, we present its closed-form, discrete realization: the Ordinal Probabilistic Reward Model (OPRM), which discretizes the quality score into a finite set of ordinal ratings. Building on OPRM, we propose a data-efficient training strategy called Region Flooding Tuning (RgFT). It enables rewards to better reflect absolute text quality by incorporating quality-level annotations, which guide the model to concentrate the probability mass within corresponding rating sub-regions. Experiments on various reward model benchmarks show that our method improves accuracy by $\textbf{2.9%}\sim\textbf{7.4%}$ compared to prior reward models, demonstrating strong performance and data efficiency. Analysis of the score distribution provides evidence that our method captures not only relative rankings but also absolute quality.

</details>


### [65] [$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2602.12674)
*Yuang Cai,Yuyu Yuan*

Main category: cs.CL

TL;DR: 本文提出了 $\mathcal{X}$-KD，通过引入反向强化学习，让学生模型在教师的原始学习环境中进行蒸馏，从而提升 LLM 知识迁移的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 知识蒸馏方法主要侧重于模仿教师模型的输出行为，却忽视了塑造教师知识的原始学习环境，导致学生模型难以完全捕获教师的决策逻辑。

Method: 提出体验式知识蒸馏（$\mathcal{X}$-KD）框架，采用近似变分奖励模仿学习（AVRIL）联合建模教师的原始奖励函数并进行策略蒸馏。该方法将过程简化为监督学习框架，且兼容序列级和基于散度的蒸馏方法。

Result: 在摘要生成、机器翻译和算术推理任务中，$\mathcal{X}$-KD 均优于广义 KD 和 MiniLLM 等基准。此外，该方法在性能与生成多样性的权衡以及数据利用效率方面表现更为出色。

Conclusion: $\mathcal{X}$-KD 是一种简单、灵活且通用的蒸馏框架，通过引入教师原始学习环境的上下文，显著提升了小型语言模型的性能和学习效率。

Abstract: Knowledge Distillation (KD) for Large Language Models (LLMs) has become increasingly important as models grow in size and complexity. While existing distillation approaches focus on imitating teacher behavior, they often overlook the original learning environment that shaped the teacher's knowledge. Inspired by the experiential learning theory and inverse reinforcement learning, we propose Experiential Knowledge Distillation ($\mathcal{X}$-KD), a novel and general framework that enables student models to learn in the teacher's original learning environment. $\mathcal{X}$-KD adopts the Approximated Variational Reward Imitation Learning (AVRIL) framework to jointly model the teacher's original reward function and perform policy distillation, encouraging consistency between the student policy and the original reward function. Our derivation demonstrates that $\mathcal{X}$-KD follows the supervised learning framework and applies to both sequence-level and divergence-based distillation methods, underlining the simplicity and flexibility of our approach. Empirical results show that $\mathcal{X}$-KD outperforms the generalized KD and MiniLLM baselines on abstractive summarization, machine translation, and arithmetic reasoning tasks. Additionally, $\mathcal{X}$-KD achieves better performance-diversity trade-off and data efficiency than baseline KD approaches.

</details>


### [66] [MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs](https://arxiv.org/abs/2602.12705)
*Baorong Shi,Bo Cui,Boyuan Jiang,Deli Yu,Fang Qian,Haihua Yang,Huichao Wang,Jiale Chen,Jianfei Pan,Jieqiong Cao,Jinghao Lin,Kai Wu,Lin Yang,Shengsheng Yao,Tao Chen,Xiaojun Xiao,Xiaozhong Ji,Xu Wang,Yijun He,Zhixiong Yang*

Main category: cs.CL

TL;DR: MedXIAOHE 是一款医疗视觉-语言基础模型，通过实体感知预训练和强化学习技术，在多项医疗基准测试中超越了闭源模型，显著提升了临床推理的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有模型在真实临床应用中通用理解与推理能力的不足，特别是针对知识覆盖广度有限（如罕见病）及长尾知识鸿沟等挑战，提升医疗多模态系统的可靠性。

Method: 1. 采用**实体感知持续预训练框架**（Entity-aware continual pretraining），通过组织异构医疗语料扩大知识覆盖面。2. 利用**强化学习**与**工具增强的智能体训练**（Tool-augmented agentic training），实现具备可验证决策轨迹的多步诊断推理。3. 集成用户偏好准则、证据锚定推理及低幻觉长报告生成技术，优化指令遵循能力。

Result: 该模型在多项医疗基准测试中取得了当前最优（SOTA）性能，并在多项关键能力指标上超越了领先的闭源多模态系统。

Conclusion: MedXIAOHE 证明了通过结合领域特定的持续预训练、强化学习和工具增强技术，可以显著提升多模态模型在临床场景下的可靠性、指令遵循能力和逻辑推理透明度，为医疗 AI 的实际部署提供了重要参考。

Abstract: We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.

</details>


### [67] [ReFilter: Improving Robustness of Retrieval-Augmented Generation via Gated Filter](https://arxiv.org/abs/2602.12709)
*Yixin Chen,Ying Xiong,Shangyu Wu,Xiangrui Ke,Nan Guan,Chun Jason Xue*

Main category: cs.CL

TL;DR: 本文提出了ReFilter框架，通过Token级别的门控过滤和潜在空间融合，解决了RAG在增加检索候选项时面临的噪声干扰和推理效率问题，显著提升了通用及专业领域（如医疗）的问答性能。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）在处理大规模检索结果（即$k$值增加）时面临挑战。虽然增加检索数量能提高证据覆盖率，但也会引入大量无关或冗余内容，导致推理成本增加且现有融合方法（如查询融合、参数融合、潜在融合）难以有效扩展。

Method: 提出ReFilter框架，一种新型的潜在空间（latent-based）融合方案，包含三个核心组件：
1. **上下文编码器（Context Encoder）**：用于提取检索上下文的特征。
2. **门控过滤器（Gated Filter）**：在Token级别进行权重分配，以识别并削弱冗余或无关信息。
3. **Token融合模块（Token Fusion Module）**：将加权后的Token特征集成到大语言模型（LLM）的隐藏状态中。

Result: 1. **通用领域**：在4个通用QA基准测试中，ReFilter在域内适配和跨域转移测试中均取得最佳平均性能。
2. **零样本泛化**：在无需领域微调的情况下，直接迁移至5个生物医学QA基准，配合Qwen2.5-14B-Instruct达到了70.01%的平均准确率。

Conclusion: ReFilter通过在潜在空间进行细粒度的Token级过滤，有效克服了传统RAG融合方法在处理大规模检索候选项时的扩展性瓶颈，显著提升了模型在多领域任务中的准确性与泛化能力。

Abstract: Retrieval-augmented generation (RAG) has become a dominant paradigm for grounding large language models (LLMs) with external evidence in knowledge-intensive question answering. A core design choice is how to fuse retrieved samples into the LLMs, where existing internal fusion approaches broadly fall into query-based fusion, parametric fusion, and latent-based fusion. Despite their effectiveness at modest retrieval scales, these methods often fail to scale gracefully as the number of retrieved candidates k increases: Larger k improves evidence coverage, yet realistic top-k retrieval inevitably contains irrelevant or redundant content and increases the inference cost.
  To address these limitations, we propose ReFilter, a novel latent-based fusion framework that performs token-level filtering and fusion. ReFilter consists of three key components: a context encoder for encoding context features, a gated filter for weighting each token, and a token fusion module for integrating the weighted token feature into the LLM's hidden states. Our experiments across four general-domain QA benchmarks show that ReFilter consistently achieves the best average performance under both in-domain adaptation and out-of-domain transfer. ReFilter further generalizes to five biomedical QA benchmarks in zero-shot transfer without domain fine-tuning, reaching 70.01% average accuracy with Qwen2.5-14B-Instruct.

</details>


### [68] [Towards a Diagnostic and Predictive Evaluation Methodology for Sequence Labeling Tasks](https://arxiv.org/abs/2602.12759)
*Elena Alvarez-Mellado,Julio Gonzalo*

Main category: cs.CL

TL;DR: 本文提出一种针对序列标注任务的新型评估方法，通过手工构建涵盖多种语言学属性的小型测试集，提供具有诊断性、行动性和预测性的模型评估，能精准预测模型在分布外数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的NLP评估方法（如仅对比平均分）存在两个主要缺陷：1. 无法为模型改进提供具体指导信息；2. 模型在分布内数据表现良好但在外部（分布外）数据上表现不可预测。研究旨在建立一种更具诊断性和预测性的评估方案。

Method: 提出一种基于错误分析的评估框架。核心不再依赖大规模真实世界的分布内抓取数据，而是通过手工构建覆盖各种跨度属性（如形状、长度、大小写、句子位置等）的语言学动机小型示例集，穷举模型在实际场景中可能遇到的各种情况。

Result: 在西班牙语外来语（anglicism）识别基准上的实验表明，该方法能够有效识别模型的系统性弱点。此外，该方法具有极强的预测性，其评估结果与模型在外部数据集上的实际表现高度相关（中值相关系数达0.85）。

Conclusion: 该研究为序列标注任务提供了一种比传统基准测试更可靠且具指导意义的评估框架。它证明了通过精心设计的少量语言学样本，可以实现比大规模随机采样数据更深层的模型洞察。

Abstract: Standard evaluation in NLP typically indicates that system A is better on average than system B, but it provides little info on how to improve performance and, what is worse, it should not come as a surprise if B ends up being better than A on outside data. We propose an evaluation methodology for sequence labeling tasks grounded on error analysis that provides both quantitative and qualitative information on where systems must be improved and predicts how models will perform on a different distribution. The key is to create test sets that, contrary to common practice, do not rely on gathering large amounts of real-world in-distribution scraped data, but consists in handcrafting a small set of linguistically motivated examples that exhaustively cover the range of span attributes (such as shape, length, casing, sentence position, etc.) a system may encounter in the wild. We demonstrate this methodology on a benchmark for anglicism identification in Spanish. Our methodology provides results that are diagnostic (because they help identify systematic weaknesses in performance), actionable (because they can inform which model is better suited for a given scenario) and predictive: our method predicts model performance on external datasets with a median correlation of 0.85.

</details>


### [69] [Aspect-Based Sentiment Analysis for Future Tourism Experiences: A BERT-MoE Framework for Persian User Reviews](https://arxiv.org/abs/2602.12778)
*Hamidreza Kazemi Taskooh,Taha Zare Harofte*

Main category: cs.CL

TL;DR: 本研究开发了一个针对波斯语旅游评论的高效混合BERT模型，通过Top-K路由机制提升了方面级情感分析性能并显著降低了功耗，同时发布了首个波斯语旅游ABSA标注数据集。


<details>
  <summary>Details</summary>
Motivation: 针对波斯语等低资源语言在旅游领域缺乏方面级情感分析（ABSA）研究的问题，同时旨在解决深度学习模型在部署时的计算效率与能耗挑战，以符合可持续发展目标。

Method: 提出一种基于BERT的混合模型，核心改进包括：1. 引入Top-K路由（Routing）机制和辅助损失函数以防止路由坍塌；2. 构建三阶段处理管线（整体情感分类、六类旅游特定方面的多标签抽取、集成ABSA的动态路由）；3. 基于Jabama平台的58,473条波斯语评论构建并手动标注数据集。

Result: 该模型在ABSA任务上的加权F1分数达到90.6%，优于基准BERT（89.25%）和标准混合方法（85.7%）。此外，与密集型BERT相比，该模型将GPU功耗降低了39%，显著提升了推理效率。

Conclusion: 本研究证明了结合Top-K路由机制的BERT混合模型在处理低资源语言（波斯语）ABSA任务中的有效性。该研究不仅为波斯语旅游领域提供了高性能的情感分析工具，还通过降低计算能耗响应了可持续发展目标，填补了该领域的数据和研究空白。

Abstract: This study advances aspect-based sentiment analysis (ABSA) for Persian-language user reviews in the tourism domain, addressing challenges of low-resource languages. We propose a hybrid BERT-based model with Top-K routing and auxiliary losses to mitigate routing collapse and improve efficiency. The pipeline includes: (1) overall sentiment classification using BERT on 9,558 labeled reviews, (2) multi-label aspect extraction for six tourism-related aspects (host, price, location, amenities, cleanliness, connectivity), and (3) integrated ABSA with dynamic routing. The dataset consists of 58,473 preprocessed reviews from the Iranian accommodation platform Jabama, manually annotated for aspects and sentiments. The proposed model achieves a weighted F1-score of 90.6% for ABSA, outperforming baseline BERT (89.25%) and a standard hybrid approach (85.7%). Key efficiency gains include a 39% reduction in GPU power consumption compared to dense BERT, supporting sustainable AI deployment in alignment with UN SDGs 9 and 12. Analysis reveals high mention rates for cleanliness and amenities as critical aspects. This is the first ABSA study focused on Persian tourism reviews, and we release the annotated dataset to facilitate future multilingual NLP research in tourism.

</details>


### [70] [RAT-Bench: A Comprehensive Benchmark for Text Anonymization](https://arxiv.org/abs/2602.12806)
*Nataša Krčo,Zexi Yao,Matthieu Meeus,Yves-Alexandre de Montjoye*

Main category: cs.CL

TL;DR: 本文引入了RAT-Bench，这是一种基于重识别风险的文本匿名化工具评估基准，发现LLM匿名化工具虽表现优于传统NER方法，但在处理间接标识符方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的文本匿名化工具（如Presidio）通常仅评估其移除特定个人标识符（PII）的能力，而其防止“重识别（Re-identification）”的实际效果尚不明确，尤其是在利用LLM处理包含个人信息的数据时。

Method: 引入RAT-Bench基准。基于美国人口统计数据生成包含多种标识符（跨领域、语言和难度）的合成文本。使用LLM模拟攻击者试图从匿名文本中推断属性，并据此量化美国人口中的重识别风险，同时评估了NER和LLM匿名化工具。

Result: 即使是表现最好的工具也远非完美，特别是在处理非标准格式的直接标识符和导致重识别的间接标识符时。LLM匿名化工具（包括迭代式工具）在隐私与效用权衡以及跨语言任务中表现更优，但计算成本更高。

Conclusion: 提出未来匿名化工具的发展建议，并发布RAT-Bench基准，鼓励社区将其扩展至全球其他地理区域。

Abstract: Data containing personal information is increasingly used to train, fine-tune, or query Large Language Models (LLMs). Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft's Presidio or Anthropic's PII purifier. These tools have traditionally been evaluated on their ability to remove specific identifiers (e.g., names), yet their effectiveness at preventing re-identification remains unclear. We introduce RAT-Bench, a comprehensive benchmark for text anonymization tools based on re-identification risk. Using U.S. demographic statistics, we generate synthetic text containing various direct and indirect identifiers across domains, languages, and difficulty levels. We evaluate a range of NER- and LLM-based text anonymization tools and, based on the attributes an LLM-based attacker is able to correctly infer from the anonymized text, we report the risk of re-identification in the U.S. population, while properly accounting for the disparate impact of identifiers. We find that, while capabilities vary widely, even the best tools are far from perfect in particular when direct identifiers are not written in standard ways and when indirect identifiers enable re-identification. Overall we find LLM-based anonymizers, including new iterative anonymizers, to provide a better privacy-utility trade-off albeit at a higher computational cost. Importantly, we also find them to work well across languages. We conclude with recommendations for future anonymization tools and will release the benchmark and encourage community efforts to expand it, in particular to other geographies.

</details>


### [71] [Left-right asymmetry in predicting brain activity from LLMs' representations emerges with their formal linguistic competence](https://arxiv.org/abs/2602.12811)
*Laurent Bonnasse-Gahot,Christophe Pallier*

Main category: cs.CL

TL;DR: 研究发现 LLM 在训练过程中预测大脑活动时表现出的左右脑不对称性，主要源于模型对**形式语言能力**（语法和结构）的习得，而非逻辑推理或世界知识。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明，随着 LLM 训练的推进，其预测人类大脑活动的能力在左半球的提升显著优于右半球。本研究旨在探究究竟是 LLM 的哪种特定能力（是语言形式能力，还是逻辑推理/百科知识）驱动了这种预测能力的左右脑不对称性。

Method: 研究采用了 OLMo-2 7B 和 Pythia 系列模型在不同训练阶段的检查点。通过对比英语和法语受试者的 fMRI 数据，计算 LLM 内部激活对大脑活动的预测得分（Brain scores），并分析其左右脑预测差异与模型在各类基准任务（语法判断、算术、Dyck 逻辑、世界知识、推理）中表现的相关性。

Result: 1. 左右脑预测的不对称性与 LLM 的**形式语言能力**（如区分语法正确性、生成规范文本）呈同步演化趋势。
2. 该不对称性与算术、Dyck 语言逻辑任务、世界知识以及基于文本的推理任务的表现**无相关性**。
3. 上述结果在不同模型家族（Pythia）和不同语言（法语）中均具有一致性。

Conclusion: LLM 预测大脑活动时的左右脑不对称性本质上反映了模型对语言模式（Linguistic patterns）的形式化掌握。这种不对称性捕捉的是语言的结构化特征，而非通用的推理或百科知识。

Abstract: When humans and large language models (LLMs) process the same text, activations in the LLMs correlate with brain activity measured, e.g., with functional magnetic resonance imaging (fMRI). Moreover, it has been shown that, as the training of an LLM progresses, the performance in predicting brain activity from its internal activations improves more in the left hemisphere than in the right one. The aim of the present work is to understand which kind of competence acquired by the LLMs underlies the emergence of this left-right asymmetry. Using the OLMo-2 7B language model at various training checkpoints and fMRI data from English participants, we compare the evolution of the left-right asymmetry in brain scores alongside performance on several benchmarks. We observe that the asymmetry co-emerges with the formal linguistic abilities of the LLM. These abilities are demonstrated in two ways: by the model's capacity to assign a higher probability to an acceptable sentence than to a grammatically unacceptable one within a minimal contrasting pair, or its ability to produce well-formed text. On the opposite, the left-right asymmetry does not correlate with the performance on arithmetic or Dyck language tasks; nor with text-based tasks involving world knowledge and reasoning. We generalize these results to another family of LLMs (Pythia) and another language, namely French. Our observations indicate that the left-right asymmetry in brain predictivity matches the progress in formal linguistic competence (knowledge of linguistic patterns).

</details>


### [72] [BaziQA-Benchmark: Evaluating Symbolic and Temporally Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.12889)
*Jiangxi Chen,Qian Liu*

Main category: cs.CL

TL;DR: 本文提出 BaziQA-Benchmark，这是一个利用专业命理竞赛题目构建的标准化基准，旨在评估大语言模型在处理符号图表和复杂时间条件下的结构化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 评估往往依赖轶事证据或提示词驱动，缺乏针对符号逻辑和时间组合推理的客观、标准化基准。本文旨在通过结构化推理任务，客观评估模型在固定符号图表与动态时间条件下的推断能力。

Method: 1. 基于 200 道专业筛选的“全球命理师大赛”多选题构建标准化基准。2. 采用多轮对话设置评估模型在时间难度、推理领域及推理协议下的性能。3. 引入轻量级“结构化推理协议（SRP）”，在不增加领域知识的前提下约束推理顺序以探究模型行为。

Result: 模型表现虽优于随机预测，但远未达到饱和。模型对时间组合复杂度和推理顺序表现出极高的敏感性，并在精确的时间定位和多条件符号判断任务中存在系统性失效。

Conclusion: BaziQA-Benchmark 揭示了当前大语言模型在符号和时间推理方面的显著局限性，尤其是在处理高精度结构化推断和多条件符号判断时，现有模型仍存在巨大提升空间。

Abstract: We present BaziQA-Benchmark, a standardized benchmark for evaluating symbolic and temporally compositional reasoning in large language models. The benchmark is derived from 200 professionally curated, multiple-choice problems from the Global Fortune-teller Competition (2021--2025), where each instance requires structured inference over a fixed symbolic chart and interacting temporal conditions. Unlike anecdotal or prompt-driven evaluations, BaziQA-Benchmark enables objective scoring and controlled comparison across years, domains, and model families. We evaluate contemporary language models under a multi-turn setting and analyze performance variation across temporal difficulty, reasoning domains, and inference protocols.To further probe reasoning behavior, we introduce a lightweight Structured Reasoning Protocol that constrains inference order without adding domain knowledge. Results show that models consistently outperform chance but remain far from saturation, exhibiting pronounced sensitivity to temporal composition and reasoning order, as well as systematic failures on precise temporal localization and multi-condition symbolic judgments.

</details>


### [73] [ViMedCSS: A Vietnamese Medical Code-Switching Speech Dataset & Benchmark](https://arxiv.org/abs/2602.12911)
*Tung X. Nguyen,Nhu Vo,Giang-Son Nguyen,Duy Mai Hoang,Chien Dinh Huynh,Inigo Jauregi Unanue,Massimo Piccardi,Wray Buntine,Dung D. Le*

Main category: cs.CL

TL;DR: 本文发布了首个越南语医疗中英混杂语音数据集 ViMedCSS，并探讨了通过结合多语言预训练与语言特定优化来提升ASR系统对医疗术语识别的效果。


<details>
  <summary>Details</summary>
Motivation: 在越南语医疗交流中，混合使用英语术语（如药品名或手术名）的现象十分普遍，但现有的自动语音识别（ASR）系统在处理这种低资源语言的语码转换时准确率较低，且缺乏相关的基准测试数据集。

Method: 1. **构建数据集 (ViMedCSS)**：包含34小时、16,576条语音，涵盖5个医疗主题，每条语音均包含至少一个英语医疗术语。
2. **构建双语词典**：建立专门的医疗术语库。
3. **模型评估与微调**：对比了多种SOTA ASR模型，并针对医疗术语识别测试了不同的微调策略。

Result: 1. 越南语优化模型在普通文本段表现更优。
2. 多语言预训练模型在捕捉英语插入词方面更具优势。
3. 结合两者的微调策略在整体准确率与语码转换识别精度之间达到了最佳平衡。

Conclusion: 本研究填补了越南语医疗领域中英混杂语音识别的空白，为低资源语言在特定专业领域的语码转换（Code-switching）识别提供了有效的数据支撑和技术路径。

Abstract: Code-switching (CS), which is when Vietnamese speech uses English words like drug names or procedures, is a common phenomenon in Vietnamese medical communication. This creates challenges for Automatic Speech Recognition (ASR) systems, especially in low-resource languages like Vietnamese. Current most ASR systems struggle to recognize correctly English medical terms within Vietnamese sentences, and no benchmark addresses this challenge. In this paper, we construct a 34-hour \textbf{Vi}etnamese \textbf{Med}ical \textbf{C}ode-\textbf{S}witching \textbf{S}peech dataset (ViMedCSS) containing 16,576 utterances. Each utterance includes at least one English medical term drawn from a curated bilingual lexicon covering five medical topics. Using this dataset, we evaluate several state-of-the-art ASR models and examine different specific fine-tuning strategies for improving medical term recognition to investigate the best approach to solve in the dataset. Experimental results show that Vietnamese-optimized models perform better on general segments, while multilingual pretraining helps capture English insertions. The combination of both approaches yields the best balance between overall and code-switched accuracy. This work provides the first benchmark for Vietnamese medical code-switching and offers insights into effective domain adaptation for low-resource, multilingual ASR systems.

</details>


### [74] [When Words Don't Mean What They Say: Figurative Understanding in Bengali Idioms](https://arxiv.org/abs/2602.12921)
*Adib Sakhawat,Shamim Ara Parveen,Md Ruhul Amin,Shamim Al Mahmud,Md Saiful Islam,Tahera Khatun*

Main category: cs.CL

TL;DR: 本文介绍了一个包含10,361个词条的孟加拉语成语数据集，采用19字段专家标注架构，并建立基准测试证明了当前主流大模型在修辞语义理解上远落后于人类表现。


<details>
  <summary>Details</summary>
Motivation: 旨在解决大语言模型（LLMs）在理解修辞语言（尤其是针对低资源语言）时面临的挑战，并填补孟加拉语在具备文化底蕴且结构化的成语语料库方面的空白。

Method: 1. 构建了一个包含10,361个孟加拉语成语的大规模语料库；2. 通过专家共识建立了包含19个字段的深度标注架构（涵盖语义、句法、文化及宗教维度）；3. 在修辞含义推断任务上，对30种最先进的多语言及指令微调LLMs进行了基准测试评估。

Result: 实验显示所有受测LLMs的准确率均未超过50%，与人类表现（83.4%）相比存在巨大差距，表明现有模型在处理特定文化语境下的修辞语言时表现乏力。

Conclusion: 当前LLMs在低资源语言的跨语言和文化推理方面存在显著局限。该研究通过提供高质量的数据集和基准，为推进孟加拉语及类似语言的修辞语言理解和文化融合奠定了基础设施。

Abstract: Figurative language understanding remains a significant challenge for Large Language Models (LLMs), especially for low-resource languages. To address this, we introduce a new idiom dataset, a large-scale, culturally-grounded corpus of 10,361 Bengali idioms. Each idiom is annotated under a comprehensive 19-field schema, established and refined through a deliberative expert consensus process, that captures its semantic, syntactic, cultural, and religious dimensions, providing a rich, structured resource for computational linguistics. To establish a robust benchmark for Bangla figurative language understanding, we evaluate 30 state-of-the-art multilingual and instruction-tuned LLMs on the task of inferring figurative meaning. Our results reveal a critical performance gap, with no model surpassing 50% accuracy, a stark contrast to significantly higher human performance (83.4%). This underscores the limitations of existing models in cross-linguistic and cultural reasoning. By releasing the new idiom dataset and benchmark, we provide foundational infrastructure for advancing figurative language understanding and cultural grounding in LLMs for Bengali and other low-resource languages.

</details>


### [75] [Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models](https://arxiv.org/abs/2602.12937)
*Ali Mekky,Mohamed El Zeftawy,Lara Hassan,Amr Keleg,Preslav Nakov*

Main category: cs.CL

TL;DR: 本文提出 LAHJATBERT，通过 GPT-4o 自动化构建多标签数据集并结合课程学习，解决了阿拉伯语方言识别中缺乏多标签数据及方言重叠的挑战，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 虽然阿拉伯语方言识别（ADI）正从单标签任务转向多标签任务，但目前缺乏大规模多标签训练数据。传统的单标签数据集在转换为多标签任务时，其负样本选择存在问题（许多负样本在多个方言中其实是可接受的），导致模型难以有效区分方言间的重叠。

Method: 1. **数据构建**：利用 GPT-4o 和二元方言可接受性分类器自动生成多标签标注；
2. **数据聚合**：引入“阿拉伯语方言化水平 (ALDi)”指导标签整合；
3. **模型训练**：开发基于 BERT 的 LAHJATBERT 模型，并采用基于方言复杂度和标签基数（Label Cardinality）的课程学习（Curriculum Learning）策略。

Result: LAHJATBERT 模型在 MLADI 排行榜上取得了 0.69 的 Macro F1 分数，相比之前最优系统（0.55）提升了约 25%。

Conclusion: 本文证明了通过自动化工具构建多标签数据集，并结合课程学习策略，可以显著提升多标签阿拉伯语方言识别的准确性。这为解决方言间重叠导致的分类难题提供了新的路径。

Abstract: Being modeled as a single-label classification task for a long time, recent work has argued that Arabic Dialect Identification (ADI) should be framed as a multi-label classification task. However, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training. By analyzing models trained on single-label ADI data, we show that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, as many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi). Afterward, we train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system. Code and data are available at https://mohamedalaa9.github.io/lahjatbert/.

</details>


### [76] [ProbeLLM: Automating Principled Diagnosis of LLM Failures](https://arxiv.org/abs/2602.12966)
*Yue Huang,Zhengzhe Jiang,Yuchen Ma,Yu Jiang,Xiangqi Wang,Yujun Zhou,Yuexing Hao,Kehan Guo,Pin-Yu Chen,Stefan Feuerriegel,Xiangliang Zhang*

Main category: cs.CL

TL;DR: ProbeLLM 是一个自动化探测框架，通过分层蒙特卡洛树搜索系统性地挖掘 LLM 的弱点，并将孤立的失败案例归纳为可解释的结构化失败模式，提升了模型评估的深度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 评估手段存在局限性：静态基准测试更新缓慢，而现有的自动探测方法往往只能发现零散、孤立的失败案例，缺乏系统性的探索策略，且难以揭示模型弱点的深层结构。

Method: 提出 ProbeLLM 自动化探测框架，核心技术包括：
1. **分层蒙特卡洛树搜索 (Hierarchical MCTS)**：在全局探索新失败领域与局部精炼已有错误模式之间优化分配探测预算。
2. **工具增强验证**：利用外部工具进行用例生成与结果校验，确保失败案例的可靠性。
3. **结构化整合**：结合失败感知嵌入 (Failure-aware Embeddings) 和边界感知归纳 (Boundary-aware Induction) 将孤立失败案例聚类为可解释的失败模式。

Result: 在多个基准测试和不同 LLM 上的实验表明，相比于静态基准和先前的自动方法，ProbeLLM 能够发现更广泛、更准确且粒度更细的“失败景观” (Failure Landscapes)。

Conclusion: ProbeLLM 成功将 LLM 的评估范式从单一的失败案例发现转向了系统化、结构化的弱点模式挖掘，为模型安全性评估和能力边界探测提供了更具洞察力的工具。

Abstract: Understanding how and why large language models (LLMs) fail is becoming a central challenge as models rapidly evolve and static evaluations fall behind. While automated probing has been enabled by dynamic test generation, existing approaches often discover isolated failure cases, lack principled control over exploration, and provide limited insight into the underlying structure of model weaknesses. We propose ProbeLLM, a benchmark-agnostic automated probing framework that elevates weakness discovery from individual failures to structured failure modes. ProbeLLM formulates probing as a hierarchical Monte Carlo Tree Search, explicitly allocating limited probing budgets between global exploration of new failure regions and local refinement of recurring error patterns. By restricting probing to verifiable test cases and leveraging tool-augmented generation and verification, ProbeLLM grounds failure discovery in reliable evidence. Discovered failures are further consolidated into interpretable failure modes via failure-aware embeddings and boundary-aware induction. Across diverse benchmarks and LLMs, ProbeLLM reveals substantially broader, cleaner, and more fine-grained failure landscapes than static benchmarks and prior automated methods, supporting a shift from case-centric evaluation toward principled weakness discovery.

</details>


### [77] [SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents](https://arxiv.org/abs/2602.12984)
*Yujiong Shen,Yajie Yang,Zhiheng Xi,Binze Hu,Huayu Sha,Jiazheng Zhang,Qiyuan Peng,Junlin Shang,Jixuan Huang,Yutao Fan,Jingqi Tong,Shihan Dou,Ming Zhang,Lei Bai,Zhenfei Yin,Tao Gui,Xingjun Ma,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.CL

TL;DR: 本文推出了 SciAgentGym 环境与 SciAgentBench 基准测试以评估智能体的科学工具调用能力，并提出 SciForge 数据合成技术，通过微调使 8B 规模的模型在复杂科学工作流执行上超越了 235B 规模的领先模型。


<details>
  <summary>Details</summary>
Motivation: 尽管科学推理需要复杂的工具整合，但现有基准测试缺乏对智能体在严谨科学工作流中编排工具能力的评估，导致难以衡量智能体在处理复杂科学任务时的真实表现。

Method: 1. 构建 SciAgentGym：一个包含四门自然科学学科、1,780个专业工具的可扩展交互式环境。 2. 开发 SciAgentBench：一套用于测试从基础操作到长程工作流能力的分层评估套件。 3. 提出 SciForge：一种基于依赖图（Dependency Graph）对工具动作空间建模的数据合成方法，用于生成逻辑感知的训练轨迹，并据此微调得到 SciAgent-8B。

Result: 1. 现有模型（包括 GPT-5）在长程任务中表现不佳，成功率随交互步数增加从 60.6% 锐减至 30.9%。 2. 经过微调的 SciAgent-8B 性能优于参数量大得多的 Qwen3-VL-235B-Instruct。 3. 模型展现出积极的科学工具调用跨领域迁移能力。

Conclusion: 本研究揭示了当前大模型在执行多步科学工作流方面的瓶颈，并证明了通过逻辑感知的数据合成（SciForge）可以显著增强小型智能体的科学工具调用能力及跨领域迁移能力，为下一代自主科学智能体的发展提供了可行路径。

Abstract: Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.

</details>


### [78] [Evaluating the Homogeneity of Keyphrase Prediction Models](https://arxiv.org/abs/2602.12989)
*Maël Houbre,Florian Boudin,Beatrice Daille*

Main category: cs.CL

TL;DR: 本文研究了关键词预测模型的“同质性”，发现提取式模型表现优异，而生成式模型产生“缺席关键词”的能力反而可能降低预测的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的关键词预测基准缺乏对“同质性”（即对主题相同的文档预测相同关键词的能力）的评估。直觉上，生成式模型因能预测文中未出现的“缺席关键词”，在同质性上应优于提取式模型，但这一假设尚未得到验证。

Method: 提出了一种评估关键词预测模型同质性的新方法，并对比研究了生成模型与提取模型在同质性上的表现，特别是探讨了生成“缺席关键词”的能力对同质性的具体影响。

Result: 实验表明，关键词提取方法在同质性上与生成模型相比具有竞争力；更重要的是，生成“缺席关键词”的能力实际上可能对模型的同质性产生负面影响。

Conclusion: 关键词生成模型预测“缺席关键词”的能力并不一定会提升索引的一致性，提取式方法在保持文档间预测同质性方面表现依然稳健。

Abstract: Keyphrases which are useful in several NLP and IR applications are either extracted from text or predicted by generative models. Contrarily to keyphrase extraction approaches, keyphrase generation models can predict keyphrases that do not appear in a document's text called `absent keyphrases`. This ability means that keyphrase generation models can associate a document to a notion that is not explicitly mentioned in its text. Intuitively, this suggests that for two documents treating the same subjects, a keyphrase generation model is more likely to be homogeneous in their indexing i.e. predict the same keyphrase for both documents, regardless of those keyphrases appearing in their respective text or not; something a keyphrase extraction model would fail to do. Yet, homogeneity of keyphrase prediction models is not covered by current benchmarks. In this work, we introduce a method to evaluate the homogeneity of keyphrase prediction models and study if absent keyphrase generation capabilities actually help the model to be more homogeneous. To our surprise, we show that keyphrase extraction methods are competitive with generative models, and that the ability to generate absent keyphrases can actually have a negative impact on homogeneity. Our data, code and prompts are available on huggingface and github.

</details>


### [79] [Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models](https://arxiv.org/abs/2602.12996)
*Hao Chen,Ye He,Yuchun Fan,Yukun Yan,Zhenghao Liu,Qingfu Zhu,Maosong Sun,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出一种元认知框架，通过内部信号将知识空间划分为不同区域并实施差异化干预，同时引入一致性机制对齐模型的信心与准确度，从而解决LLM的过度自信或不确定性问题，实现更可靠的知识增强。


<details>
  <summary>Details</summary>
Motivation: 现有的知识增强方法通常简单地将模型性能等同于其内部知识，忽略了“知识-信心”差距（Knowledge-Confidence Gaps）。这导致模型容易出现“对错误答案过度自信”或“对正确事实犹豫不决”的问题，限制了模型在知识密集型任务中的可靠性。

Method: 提出一种元认知驱动的可靠知识增强框架，包含两个核心组件：
1. **差异化干预（Differentiated Intervention）**：利用模型内部认知信号，将知识空间划分为“已掌握”、“混淆”和“缺失”三个区域，进行针对性的知识扩展。
2. **认知一致性机制（Cognitive Consistency Mechanism）**：通过同步模型的主观确定性（Certainty）与客观准确性（Accuracy），实现知识边界的精确对齐与校准。

Result: 广泛的实验评估表明，该框架在性能上一致优于现有的强基准方法。它不仅增强了模型处理复杂知识的能力，还显著改善了模型的校准度，使其能够更好地识别自身认知的局限性。

Conclusion: 该框架不仅提升了LLM的知识能力，更重要的是培养了其区分“已知”与“未知”的元认知行为。通过区分干预和信心对齐，实现了更可靠、校准性更强的知识增强，为构建具有自我察觉能力的智能系统提供了新思路。

Abstract: Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that lead to overconfident errors or uncertain truths. To bridge this gap, we propose a novel meta-cognitive framework for reliable knowledge augmentation via differentiated intervention and alignment. Our approach leverages internal cognitive signals to partition the knowledge space into mastered, confused, and missing regions, guiding targeted knowledge expansion. Furthermore, we introduce a cognitive consistency mechanism to synchronize subjective certainty with objective accuracy, ensuring calibrated knowledge boundaries. Extensive experiments demonstrate the our framework consistently outperforms strong baselines, validating its rationality in not only enhancing knowledge capabilities but also fostering cognitive behaviors that better distinguish knowns from unknowns.

</details>


### [80] [Can we trust AI to detect healthy multilingual English speakers among the cognitively impaired cohort in the UK? An investigation using real-world conversational speech](https://arxiv.org/abs/2602.13047)
*Madhurananda Pahar,Caitlin Illingworth,Dorota Braun,Bahman Mirheidari,Lise Sproson,Daniel Blackburn,Heidi Christensen*

Main category: cs.CL

TL;DR: 本研究揭示了当前利用语音检测认知障碍的 AI 模型在处理英国少数族裔多语言者时存在显著偏见，容易产生误诊，强调了现有工具在临床推广中的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着英国少数族裔社区痴呆症患病率的快速增长，评估 AI 模型在检测认知下降（如痴呆和 MCI）时是否存在针对多语言者的偏见至关重要，以确保这些工具在临床应用中对不同族裔和语言背景的人群均具有可靠性和公平性。

Method: 研究招募了英国全国范围内的单语参与者，以及来自谢菲尔德和布拉德福德社区中心的多语言参与者（涉及索马里语、中文或南亚语言，并细分为西/南约克郡口音）。研究通过声学和语言特征构建分类与回归模型，并在公开数据集（如 DementiaBank）及本地数据上进行训练与测试，以评估 ASR 系统及检测模型的公平性。

Result: 虽然 ASR 系统未显示出显著偏差，但分类和回归模型在记忆、流畅度和阅读任务中对多语言者表现出显著偏见；当模型在 DementiaBank 数据集上训练时，这种偏见更加严重。多语言者更易被误分类为认知受损，且特定口音（南约克郡）的说话者易被误判为更严重的认知下降。

Conclusion: 本初步研究结论指出，现有的 AI 认知障碍检测工具在少数族裔多语言人群中尚未达到临床可靠水平。未来的研究方向应致力于开发更具泛化性且能有效减轻偏见的模型，以确保临床诊断的公平性。

Abstract: Conversational speech often reveals early signs of cognitive decline, such as dementia and MCI. In the UK, one in four people belongs to an ethnic minority, and dementia prevalence is expected to rise most rapidly among Black and Asian communities. This study examines the trustworthiness of AI models, specifically the presence of bias, in detecting healthy multilingual English speakers among the cognitively impaired cohort, to make these tools clinically beneficial. For experiments, monolingual participants were recruited nationally (UK), and multilingual speakers were enrolled from four community centres in Sheffield and Bradford. In addition to a non-native English accent, multilinguals spoke Somali, Chinese, or South Asian languages, who were further divided into two Yorkshire accents (West and South) to challenge the efficiency of the AI tools thoroughly. Although ASR systems showed no significant bias across groups, classification and regression models using acoustic and linguistic features exhibited bias against multilingual speakers, particularly in memory, fluency, and reading tasks. This bias was more pronounced when models were trained on the publicly available DementiaBank dataset. Moreover, multilinguals were more likely to be misclassified as having cognitive decline. This study is the first of its kind to discover that, despite their strong overall performance, current AI models show bias against multilingual individuals from ethnic minority backgrounds in the UK, and they are also more likely to misclassify speakers with a certain accent (South Yorkshire) as living with a more severe cognitive decline. In this pilot study, we conclude that the existing AI tools are therefore not yet reliable for diagnostic use in these populations, and we aim to address this in future work by developing more generalisable, bias-mitigated models.

</details>


### [81] [TraceBack: Multi-Agent Decomposition for Fine-Grained Table Attribution](https://arxiv.org/abs/2602.13059)
*Tejas Anvekar,Junha Park,Rajat Jha,Devanshu Gupta,Poojah Ganesan,Puneeth Mathur,Vivek Gupta*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Question answering (QA) over structured tables requires not only accurate answers but also transparency about which cells support them. Existing table QA systems rarely provide fine-grained attribution, so even correct answers often lack verifiable grounding, limiting trust in high-stakes settings. We address this with TraceBack, a modular multi-agent framework for scalable, cell-level attribution in single-table QA. TraceBack prunes tables to relevant rows and columns, decomposes questions into semantically coherent sub-questions, and aligns each answer span with its supporting cells, capturing both explicit and implicit evidence used in intermediate reasoning steps. To enable systematic evaluation, we release CITEBench, a benchmark with phrase-to-cell annotations drawn from ToTTo, FetaQA, and AITQA. We further propose FairScore, a reference-less metric that compares atomic facts derived from predicted cells and answers to estimate attribution precision and recall without human cell labels. Experiments show that TraceBack substantially outperforms strong baselines across datasets and granularities, while FairScore closely tracks human judgments and preserves relative method rankings, supporting interpretable and scalable evaluation of table-based QA.

</details>


### [82] [Exploring a New Competency Modeling Process with Large Language Models](https://arxiv.org/abs/2602.13084)
*Silin Du,Manqing Xin,Raymond Jia Wang*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型（LLM）的胜任力建模新框架，通过结构化计算组件和自适应特征融合，实现了从定性专家分析向数据驱动、透明且可评估的标准化流程转变。


<details>
  <summary>Details</summary>
Motivation: 传统胜任力建模依赖专家手工分析海量面试访谈记录，存在成本高、主观随意性强、透明度低以及难以复现等局限性，亟需一种更高效、客观的自动化处理方案。

Method: 1. **工作流重构**：将专家实践分解为结构化计算组件。
2. **特征提取**：利用LLM从面试文本中提取行为与心理特征描述。
3. **映射机制**：通过嵌入（Embedding）相似度将特征映射至胜任力库。
4. **自适应融合**：引入可学习参数，动态调节行为与心理信号的权重。
5. **离线评估**：建立了一套无需额外大规模数据收集的模型选择与验证程序。

Result: 在某软件外包公司的实证结果显示，该模型具有显著的预测效度，并在跨胜任力库的一致性和结构稳健性方面表现优异。

Conclusion: 该研究成功将胜任力建模从高度依赖专家经验的定性分析转型为透明、可量化且可评估的数据驱动流程，显著提升了人力资源评估的科学性和效率。

Abstract: Competency modeling is widely used in human resource management to select, develop, and evaluate talent. However, traditional expert-driven approaches rely heavily on manual analysis of large volumes of interview transcripts, making them costly and prone to randomness, ambiguity, and limited reproducibility. This study proposes a new competency modeling process built on large language models (LLMs). Instead of merely automating isolated steps, we reconstruct the workflow by decomposing expert practices into structured computational components. Specifically, we leverage LLMs to extract behavioral and psychological descriptions from raw textual data and map them to predefined competency libraries through embedding-based similarity. We further introduce a learnable parameter that adaptively integrates different information sources, enabling the model to determine the relative importance of behavioral and psychological signals. To address the long-standing challenge of validation, we develop an offline evaluation procedure that allows systematic model selection without requiring additional large-scale data collection. Empirical results from a real-world implementation in a software outsourcing company demonstrate strong predictive validity, cross-library consistency, and structural robustness. Overall, our framework transforms competency modeling from a largely qualitative and expert-dependent practice into a transparent, data-driven, and evaluable analytical process.

</details>


### [83] [Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts](https://arxiv.org/abs/2602.13102)
*Kais Allkivi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9. Additional evaluation on an earlier exam sample revealed that the writings have become more complex over a 7-10-year period, while accuracy still reached 0.8 with some feature sets. The results have been implemented in the writing evaluation module of an Estonian open-source language learning environment.

</details>


### [84] [SCOPE: Selective Conformal Optimized Pairwise LLM Judging](https://arxiv.org/abs/2602.13110)
*Sher Badshah,Ali Emami,Hassan Sajjad*

Main category: cs.CL

TL;DR: 本文提出了 SCOPE 框架和双向偏好熵（BPE），通过共形预测技术为 LLM 裁判提供统计学上的错误率保证，在消除位置偏见的同时，实现了高覆盖率且可靠的自动化对战评估。


<details>
  <summary>Details</summary>
Motivation: LLM 裁判虽然被广泛用于替代昂贵的人工偏好标注，但普遍存在校准不良和系统性偏见（如位置偏见），导致评估结果的不可靠。现有的方法难以在保证评估准确性的同时提供统计学上的风险保证。

Method: 1. **SCOPE 框架**：基于共形预测（Conformal Prediction）理论，在交换性假设下校准接受阈值，确保非弃权判定的错误率控制在用户指定的 $\alpha$ 水平以下。
2. **双向偏好熵 (BPE)**：通过交换两个待评测回答的顺序进行双向查询，聚合概率以消除位置偏见（Position Bias），并将其转化为基于熵的不确定性得分，作为选择性判断的依据。

Result: 在 MT-Bench、RewardBench 和 Chatbot Arena 等基准上，SCOPE 始终能满足预设风险水平（如 $\alpha=0.10$ 时，经验风险约为 0.097-0.099）。在相同风险约束下，SCOPE 在 MT-Bench 上的接受判断数最高可达基准方法的 2.4 倍，且在 RewardBench 上实现了高达 0.89 至 0.98 的覆盖率。

Conclusion: BPE 提供了一种偏好中性且高质量的不确定性信号。结合 SCOPE 框架，可以在保证统计可靠性（控制错误率）的同时，最大限度地保留 LLM 裁判的自动化评估比例，有效解决了 LLM 作为裁判时的偏见和校准问题。

Abstract: Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $α$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $α= 0.10$, \textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to naïve baselines, \textsc{Scope} accepts up to $2.4\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.

</details>


### [85] [From sunblock to softblock: Analyzing the correlates of neology in published writing and on social media](https://arxiv.org/abs/2602.13123)
*Maria Ryskina,Matthew R. Gormley,Kyle Mahowald,David R. Mortensen,Taylor Berg-Kirkpatrick,Vivek Kulkarni*

Main category: cs.CL

TL;DR: 本研究利用上下文嵌入分析了 Twitter 上的新词产生机制，发现虽然新词产生的核心驱动力在不同领域具有一致性，但由于形成机制的不同，话题流行度对社交媒体新词的影响力弱于传统出版物。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨语言演化压力在不同社交和对话语境下的差异。此前关于英语新词（neology）产生的分布语义学研究主要集中在历史出版文本上，本研究试图验证这些发现是否同样适用于社交媒体（如 Twitter）等非正式语境。

Method: 扩展了 Ryskina 等人（2020）的方法论，不仅使用静态词嵌入，还引入了上下文嵌入（Contextual Embeddings）。研究将该方法应用于大规模 Twitter 语料库，并将其与历史出版文本的分析结果进行对比。

Result: 研究证实，先前在出版文本中发现的新词产生因子在 Twitter 领域同样成立。然而，“话题流行度增长（topic popularity growth）”因子对 Twitter 上新词产生的贡献显著低于其在正式出版物中的贡献。

Conclusion: 不同领域对新词形成机制（neologism formation mechanisms）的偏好差异，导致了演化压力在不同语境下的影响权重不同。虽然核心影响因子具有跨领域的鲁棒性，但领域特性（如社交媒体与正式出版物）决定了具体因子的贡献度。

Abstract: Living languages are shaped by a host of conflicting internal and external evolutionary pressures. While some of these pressures are universal across languages and cultures, others differ depending on the social and conversational context: language use in newspapers is subject to very different constraints than language use on social media. Prior distributional semantic work on English word emergence (neology) identified two factors correlated with creation of new words by analyzing a corpus consisting primarily of historical published texts (Ryskina et al., 2020, arXiv:2001.07740). Extending this methodology to contextual embeddings in addition to static ones and applying it to a new corpus of Twitter posts, we show that the same findings hold for both domains, though the topic popularity growth factor may contribute less to neology on Twitter than in published writing. We hypothesize that this difference can be explained by the two domains favouring different neologism formation mechanisms.

</details>


### [86] [Semantic Chunking and the Entropy of Natural Language](https://arxiv.org/abs/2602.13194)
*Weishun Zhong,Doron Sivan,Tankut Can,Mikhail Katkov,Misha Tsodyks*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [87] [Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues](https://arxiv.org/abs/2602.12381)
*Marco Willi,Melanie Mathys,Michael Graber*

Main category: cs.CV

TL;DR: 本文通过构建 SynthCLIC 数据集并结合可解释性分析，揭示了基于 CLIP 的合成图像检测器主要利用高层摄影特征而非伪影进行判断，并指出其在不同生成架构间存在泛化瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管基于 CLIP 的合成图像检测（SID）表现出较强的准确性和泛化性，但其决策依据尚不明确（是依赖视觉伪影还是语义偏见）。此外，现有检测器在面对新型高质量生成模型（如扩散模型）时，往往难以保持良好的泛化性能。

Method: 1. 构建了 **SynthCLIC**：一个配对数据集，包含真实照片及其对应的高质量扩散模型生成图像，旨在消除语义偏见；2. 采用可解释的线性分类头，结合去相关激活（de-correlated activations）和基于文本的概念模型（concept-model）来剖析 CLIP 特征。

Result: 1. 基于 CLIP 的检测器在 GAN 基准上达到 0.96 mAP，在 SynthCLIC 扩散数据集上为 0.92 mAP，但跨生成器家族的泛化能力最低降至 0.37 mAP；2. 分析发现，检测器主要依赖**高层摄影属性**（如极简风格、镜头光晕、深度层级）进行识别，而非特定的生成器伪影。

Conclusion: CLIP 是构建通用、鲁棒的 SID 系统的强有力基础，但面对多样化的生成架构时表现不一。研究强调了为了应对不断演进的生成技术，必须进行持续的模型更新和更广泛的训练数据覆盖。

Abstract: Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.

</details>


### [88] [Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models](https://arxiv.org/abs/2602.12393)
*Ali Subhan,Ashir Raza*

Main category: cs.CV

TL;DR: 本文通过复现实验验证了 DragDiffusion 的有效性，证实其核心结论可复现，但同时指出该方法对时间步和特征层级等特定超参数具有较强的依赖性。


<details>
  <summary>Details</summary>
Motivation: 验证 DragDiffusion 这一交互式点选图像编辑方法的可复现性。旨在通过独立实验确认原论文提出的定性与定量趋势，并探索模型性能对关键超参数设置的敏感度。

Method: 基于官方开源代码和 DragBench 基准测试集，对 DragDiffusion 进行可复现性研究。复现了包括扩散时间步选择、LoRA 微调、掩码正则化强度和 UNet 特征监督在内的核心消融实验，并额外评估了一种多时间步潜空间优化变体。

Result: 复现结果与原论文报告的趋势一致。实验发现模型性能对“优化时间步”和“运动监督特征层”高度敏感，而其他组件的参数容错范围较宽。此外，多时间步优化变体虽增加了计算成本，但未能提升空间控制的准确性。

Conclusion: DragDiffusion 的核心主张在很大程度上是可靠且可复现的。然而，该方法的成功高度依赖于特定超参数的精确选择（如时间步和特征层），而非所有组件都具有同样的鲁棒性。

Abstract: DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors' released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.

</details>


### [89] [What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis](https://arxiv.org/abs/2602.12395)
*Xirui Li,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 本研究分析了强化学习（RL）对视觉语言模型推理能力的具体影响，发现 RL 的增益主要源于模型中后层对“视觉-推理对齐”的优化，而非视觉感知能力的提升。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习（RL）已成为提升视觉语言模型（VLM）推理的标准手段，但由于基准测试得分受多种因素影响，难以厘清 RL 相比监督微调（SFT/冷启动）究竟改进了哪些具体能力（如感知还是推理）。

Method: 提出一种名为“Frankenstein-style”的分析框架，包含三部分：(i) 通过**因果探测（Causal Probing）**进行功能定位；(ii) 通过**参数比较**分析更新特征；(iii) 通过**模型合并（Model Merging）**进行迁移性测试。

Result: RL 引起的改进并非均匀分布，而是集中在模型的中后层；这些层级的细化对推理性能的提升既是充分的（可迁移）也是必要的（冻结后增益消失）。实验证明 RL 并没有显著增强基础视觉感知能力，而是优化了推理过程。

Conclusion: RL 对视觉推理的贡献并非全方位的增强，而是通过对 Transformer 中后层计算的系统性优化，改善了“视觉到推理”的对齐能力。研究强调，仅依赖基准测试得分不足以揭示多模态推理改进的深层机理。

Abstract: Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.

</details>


### [90] [ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning](https://arxiv.org/abs/2602.12401)
*Zihan Ye,Shreyank N Gowda,Kaile Du,Weijian Luo,Ling Shao*

Main category: cs.CV

TL;DR: 本文针对 ZSL 中的视觉-语义伪相关和数据脱节问题，提出了基于扩散模型的生成框架 ZeroDiff++，通过测试时自适应（DiffTTA）和特征生成（DiffGen）显著提升了模型在稀缺数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式零样本学习（ZSL）存在视觉-语义伪相关问题，且受训练数据稀缺影响严重。此外，传统全噪声生成器产生的特征与真实测试样本脱节，导致模型无法有效建立视觉与语义的关联。

Method: 提出 ZeroDiff++ 扩散生成框架，核心包括：(1) 训练阶段：利用扩散增强（Diffusion Augmentation）生成多样化样本，采用监督对比（SC）表征实例级语义，并结合 Wasserstein 互学习的多视图判别器；(2) 生成阶段：引入测试时自适应（DiffTTA）通过伪标签重建优化生成器，以及测试时生成（DiffGen）通过跟踪去噪路径产生连接真实与合成数据的特征。

Result: 在三个 ZSL 基准数据集上的实验表明，ZeroDiff++ 不仅在性能上显著超越了现有方法，且在训练数据极度稀缺的情况下依然展现出极强的鲁棒性。

Conclusion: ZeroDiff++ 通过改进扩散生成机制并引入测试时自适应与生成技术，有效解决了 ZSL 中的视觉-语义伪相关和生成样本脱节问题，显著增强了模型在数据稀缺环境下的泛化能力和稳健性。

Abstract: Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.

</details>


### [91] [Prototype-driven fusion of pathology and spatial transcriptomics for interpretable survival prediction](https://arxiv.org/abs/2602.12441)
*Lihe Liu,Xiaoxi Pan,Yinyin Yuan,Lulu Shang*

Main category: cs.CV

TL;DR: PathoSpatial是一个结合WSI和空间转录组数据的端到端可解释框架，通过原型学习和多级专家架构提升了癌症预后预测的准确性与生物学可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管配对的病理全扫描切片（WSI）与空间转录组（ST）数据日益增多，但目前缺乏有效的跨模态融合策略来整合形态学与空间分子上下文信息，以实现精准的预后建模。

Method: 提出PathoSpatial框架，采用任务引导的原型学习（Task-guided prototype learning）和多级专家架构（Multi-level experts architecture），通过自适应地协调无监督模态内发现与有监督跨模态聚合，学习空间感知的预后表示。

Result: 在三阴性乳腺癌（TNBC）队列的五个生存终点评估中，该方法表现优于或等同于当前最先进的单模态和多模态模型，并能实现事后原型解释和分子风险分解。

Conclusion: PathoSpatial为空间组学与病理学的深度融合提供了一个可扩展且高度可解释的多模态学习范式，能够有效识别具有临床意义的候选预后因子。

Abstract: Whole slide images (WSIs) enable weakly supervised prognostic modeling via multiple instance learning (MIL). Spatial transcriptomics (ST) preserves in situ gene expression, providing a spatial molecular context that complements morphology. As paired WSI-ST cohorts scale to population level, leveraging their complementary spatial signals for prognosis becomes crucial; however, principled cross-modal fusion strategies remain limited for this paradigm. To this end, we introduce PathoSpatial, an interpretable end-to-end framework integrating co-registered WSIs and ST to learn spatially informed prognostic representations. PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation. By design, PathoSpatial substantially strengthens interpretability while maintaining discriminative ability. We evaluate PathoSpatial on a triple-negative breast cancer cohort with paired ST and WSIs. PathoSpatial delivers strong and consistent performance across five survival endpoints, achieving superior or comparable performance to leading unimodal and multimodal methods. PathoSpatial inherently enables post-hoc prototype interpretation and molecular risk decomposition, providing quantitative, biologically grounded explanations, highlighting candidate prognostic factors. We present PathoSpatial as a proof-of-concept for scalable and interpretable multimodal learning for spatial omics-pathology fusion.

</details>


### [92] [Semantic-aware Adversarial Fine-tuning for CLIP](https://arxiv.org/abs/2602.12461)
*Jiacheng Zhang,Jinhao Li,Hanxun Huang,Sarah M. Erfani,Benjamin I. P. Rubinstein,Feng Liu*

Main category: cs.CV

TL;DR: 本文提出 SAFT，通过集成基础模型生成的精炼语义描述来产生更强的对抗样本，从而显著提升 CLIP 模型在零样本分类任务中的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的 CLIP 对抗微调方法依赖单一的手工模板（如 "A photo of a {label}"）生成对抗样本，这种方式无法充分衡量图像与文本间的相似性，导致生成的对抗样本在面对更丰富的语义度量时失效，从而限制了模型微调后的鲁棒性。

Method: 提出了语义感知对抗微调（SAFT）：1. 语义集成攻击：利用基础模型生成多样的文本描述，并进行精炼以减少幻觉，通过最小化图像与这些描述集成的平均相似度来生成语义感知对抗样本（AEs）；2. 使用这些 AEs 对 CLIP 的图像编码器进行对抗微调。

Result: 在 16 个数据集上的实验结果显示，SAFT 在零样本对抗鲁棒性方面显著优于现有方法，实现了大幅度的性能提升。

Conclusion: 通过引入语义丰富的文本描述集成来生成对抗样本，可以显著增强 CLIP 图像编码器在多样化语义度量下的零样本对抗鲁棒性。

Abstract: Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.

</details>


### [93] [A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification](https://arxiv.org/abs/2602.12484)
*Md. Ehsanul Haque,Md. Saymon Hosen Polash,Rakib Hasan Ovi,Aminul Kader Bulbul,Md Kamrul Siam,Tamim Hasan Saykat*

Main category: cs.CV

TL;DR: 本研究开发了一种基于优化 DenseNet-121 的葡萄叶片病害分类框架，通过结合 Grad-CAM 视觉解释技术和迁移学习，实现了 99.27% 的高精度检测，同时显著降低了计算开销，为实时农业病害监测提供了透明且高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 葡萄病害（如细菌性腐烂病、霜霉病和白粉病）严重影响全球葡萄的产量和质量。现有的自动化检测方法（特别是基于 YOLO 框架的方法）往往计算成本高且缺乏可解释性，难以在实际农业环境中应用。因此，需要一种既能保证早期精确识别，又具备低计算开销和高透明度的检测框架。

Method: 提出了一种基于优化 DenseNet-121 的分类模型。该方法结合了领域特定的图像预处理、迁移学习以及模型优化技术，并引入 Grad-CAM（梯度加权类激活映射）技术来可视化病害相关区域，以增强模型的可解释性和透明度。此外，研究还通过交叉验证和与 ResNet18、VGG16、AlexNet 及 SqueezeNet 等模型的对比来验证性能。

Result: 该模型在各项指标上均表现优异，准确率达到 99.27%，F1 分数为 99.28%，特异性为 99.71%，Kappa 系数为 98.86%，推理时间仅为 9 秒。交叉验证显示平均准确率为 99.12%，证明了模型在不同类别间具有极强的鲁棒性和泛化能力。

Conclusion: 该框架通过高效的架构、领域特定的预处理和可解释的输出，为葡萄叶片病害检测提供了一种可扩展、高精度且计算成本低的方案，适用于实际农业场景中的实时部署。

Abstract: Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.

</details>


### [94] [Human-Like Coarse Object Representations in Vision Models](https://arxiv.org/abs/2602.12486)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: 研究发现分割模型在中间性能阶段（而非最高精度阶段）最符合人类的物理直觉表征，证明了人类化的粗略物体表征源于资源约束，并提供了获取物理高效模型的新途径。


<details>
  <summary>Details</summary>
Motivation: 人类在进行物理直觉推断时倾向于使用平滑凹面的“粗略体积体”表征，而目前的分割模型追求像素级的精确掩码。本研究旨在探究这些模型是否以及在何时能习得类似人类的物体表征。

Method: 引入了基于碰撞时间（TTC）预测的行为范式、比较流水线及对齐度指标。通过改变模型训练时间、模型大小以及通过剪枝改变有效容量，系统研究了模型表征与人类行为的一致性。

Result: 模型与人类行为的对齐度随模型复杂度和训练程度呈现“倒U型曲线”：规模过小/训练不足的模型倾向于欠分割（呈团块状）；规模过大/充分训练的模型则过度分割（边缘过于细碎）；唯有处于中间阶段的“理想物体粒度”与人类表现最匹配。

Conclusion: 人类在物理直觉中对物体表征的“粗略性”并非源于特定的设计偏差，而是资源优化（Resource-rationality）的结果。研究指出，通过简单的手段（如使用早期检查点、适度的模型架构或轻量化剪枝）即可诱导模型生成更符合物理效率的表征。

Abstract: Humans appear to represent objects for intuitive physics with coarse, volumetric bodies'' that smooth concavities - trading fine visual details for efficient physical predictions - yet their internal structure is largely unknown. Segmentation models, in contrast, optimize pixel-accurate masks that may misalign with such bodies. We ask whether and when these models nonetheless acquire human-like bodies. Using a time-to-collision (TTC) behavioral paradigm, we introduce a comparison pipeline and alignment metric, then vary model training time, size, and effective capacity via pruning. Across all manipulations, alignment with human behavior follows an inverse U-shaped curve: small/briefly trained/pruned models under-segment into blobs; large/fully trained models over-segment with boundary wiggles; and an intermediate ideal body granularity'' best matches humans. This suggests human-like coarse bodies emerge from resource constraints rather than bespoke biases, and points to simple knobs - early checkpoints, modest architectures, light pruning - for eliciting physics-efficient representations. We situate these results within resource-rational accounts balancing recognition detail against physical affordances.

</details>


### [95] [Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models](https://arxiv.org/abs/2602.12498)
*Ali Abbasi,Mehdi Taghipour,Rahmatollah Beheshti*

Main category: cs.CV

TL;DR: 本文提出了NAST，一种利用因果追踪效应指导梯度更新的医学多模态微调方法，旨在通过可解释性信号增强模型对临床报告中否定表述（如“无肺炎”）的识别与理解能力。


<details>
  <summary>Details</summary>
Motivation: 否定操作在临床报告中至关重要，但现有的医学视觉语言模型（VLM）在区分“肯定”与“否定”临床发现方面表现较差（极性敏感度不足）。现有的训练方法往往无法处理复杂的上下文否定（如涉及特定位置或严重程度的否定），这在医疗决策中存在显著的安全风险。

Method: 1. **基准测试与数据构建**：开发了放射学极性诊断基准，并构建了包含位置、严重程度等属性级否定的结构化临床数据集。
2. **因果追踪（CTE）**：利用因果追踪效应识别模型中对“否定词处理”起关键作用的特定层。
3. **NAST算法**：提出“否定感知选择性训练”（NAST），根据各层的因果贡献度动态调整微调时的梯度更新权重（非均匀学习率），而非统一更新所有层。

Result: 实验结果表明，NAST显著提升了模型对肯定与否定临床陈述的辨别能力。同时，该方法在增强否定语义理解的过程中，并未退化模型原有的通用视觉-语言对齐性能。

Conclusion: 本研究证明，将因果可解释性信号（如CTE）转化为优化规则，可以精准修复模型特定的逻辑缺陷（如否定处理）。这种针对性适配方法对于安全敏感的医疗AI应用具有重要意义，能在不牺牲通用性能的情况下增强临床逻辑的严谨性。

Abstract: Negation is a fundamental linguistic operation in clinical reporting, yet vision-language models (VLMs) frequently fail to distinguish affirmative from negated medical statements. To systematically characterize this limitation, we introduce a radiology-specific diagnostic benchmark that evaluates polarity sensitivity under controlled clinical conditions, revealing that common medical VLMs consistently confuse negated and non-negated findings. To enable learning beyond simple condition absence, we further construct a contextual clinical negation dataset that encodes structured claims and supports attribute-level negations involving location and severity. Building on these resources, we propose Negation-Aware Selective Training (NAST), an interpretability-guided adaptation method that uses causal tracing effects (CTEs) to modulate layer-wise gradient updates during fine-tuning. Rather than applying uniform learning rates, NAST scales each layer's update according to its causal contribution to negation processing, transforming mechanistic interpretability signals into a principled optimization rule. Experiments demonstrate improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, highlighting the value of causal interpretability for targeted model adaptation in safety-critical medical settings. Code and resources are available at https://github.com/healthylaife/NAST.

</details>


### [96] [Matching of SAR and optical images based on transformation to shared modality](https://arxiv.org/abs/2602.12515)
*Alexey Borisov,Evgeny Myasnikov,Vladislav Myasnikov*

Main category: cs.CV

TL;DR: 本文提出通过将光学和SAR图像映射到一种共同的中间模态，从而能够直接利用现成的通用图像匹配模型（如RoMa）实现高精度的跨模态遥感图像配准。


<details>
  <summary>Details</summary>
Motivation: 光学与合成孔径雷达（SAR）图像因成像物理原理迥异，存在显著的几何和辐射差异，导致跨模态图像的精确配准（Co-registration）极具挑战性。

Method: 提出一种将光学和SAR图像转换为“共享模态”的框架。该模态需满足通道数一致、配准图像相似度最大化以及保留显著特征（非退化）三个条件。转换后，直接利用现成的、在常规数字图像上预训练的RoMa模型进行特征匹配，无需重新训练。

Result: 在公开的MultiSenGE数据集上的实验结果表明，该方法在匹配质量上优于基于图像翻译（Image Translation）和各种传统特征匹配算法。其优势在于能够直接适配预训练的通用视觉模型，并保持高精度的配准效果。

Conclusion: 该方法证明了通过构建共享模态，可以有效利用在常规图像上预训练的深度学习模型（如RoMa和DeDoDe）来解决遥感图像配准问题，不仅提高了匹配精度，还具有很强的通用性和灵活性。

Abstract: Significant differences in optical images and Synthetic Aperture Radar (SAR) images are caused by fundamental differences in the physical principles underlying their acquisition by Earth remote sensing platforms. These differences make precise image matching (co-registration) of these two types of images difficult. In this paper, we propose a new approach to image matching of optical and SAR images, which is based on transforming the images to a new modality. The new image modality is common to both optical and SAR images and satisfies the following conditions. First, the transformed images must have an equal pre-defined number of channels. Second, the transformed and co-registered images must be as similar as possible. Third, the transformed images must be non-degenerate, meaning they must preserve the significant features of the original images. To further match images transformed to this shared modality, we train the RoMa image matching model, which is one of the leading solutions for matching of regular digital photographs. We evaluated the proposed approach on the publicly available MultiSenGE dataset containing both optical and SAR images. We demonstrated its superiority over alternative approaches based on image translation between original modalities and various feature matching algorithms. The proposed solution not only provides better quality of matching, but is also more versatile. It enables the use of ready-made RoMa and DeDoDe models, pre-trained for regular images, without retraining for a new modality, while maintaining high-quality matching of optical and SAR images.

</details>


### [97] [LiDAR-Anchored Collaborative Distillation for Robust 2D Representations](https://arxiv.org/abs/2602.12524)
*Wonjun Jo,Hyunwoo Ha,Kim Ji-Yeon,Hawook Jeong,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 本文提出“协同蒸馏”自监督方法，通过利用3D LiDAR数据引导2D图像编码器，显著提升了模型在恶劣天气下的鲁棒性和3D感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督预训练2D图像编码器在面对噪声和恶劣天气条件（如非晴天场景）时表现不佳，缺乏稳健的视觉感知能力，难以满足复杂现实场景的需求。

Method: 提出一种名为“协同蒸馏”（Collaborative Distillation）的新型自监督学习方法，利用3D LiDAR数据作为自监督信号，对2D图像编码器进行引导训练。

Result: 该方法在多种下游任务和多样化环境条件下均优于竞争方法，展现出极强的泛化能力。此外，由于引入了LiDAR特征，模型在3D感知能力上也得到了显著增强。

Conclusion: 该方法通过跨模态知识迁移，在不牺牲原有性能的前提下，有效解决了2D视觉模型在恶劣环境下的可靠性问题，证明了其在现实世界视觉系统中的高度实用性和适应性。

Abstract: As deep learning continues to advance, self-supervised learning has made considerable strides. It allows 2D image encoders to extract useful features for various downstream tasks, including those related to vision-based systems. Nevertheless, pre-trained 2D image encoders fall short in conducting the task under noisy and adverse weather conditions beyond clear daytime scenes, which require for robust visual perception. To address these issues, we propose a novel self-supervised approach, \textbf{Collaborative Distillation}, which leverages 3D LiDAR as self-supervision to improve robustness to noisy and adverse weather conditions in 2D image encoders while retaining their original capabilities. Our method outperforms competing methods in various downstream tasks across diverse conditions and exhibits strong generalization ability. In addition, our method also improves 3D awareness stemming from LiDAR's characteristics. This advancement highlights our method's practicality and adaptability in real-world scenarios.

</details>


### [98] [Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting](https://arxiv.org/abs/2602.12540)
*Haoran Zhu,Anna Choromanska*

Main category: cs.CV

TL;DR: 本文提出了 AD-LiST-JEPA，这是一种基于 JEPA 架构的自监督 LiDAR 世界模型，通过预测未来时空演变学习环境表示，并在占用补全与预测任务中取得了更优的表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要具备捕捉环境时空演变的“世界模型”以支持长期规划。为了实现可扩展性，模型需要能够以自监督的方式从海量无标注数据中学习，而传统的监督学习标注成本过高。

Method: 提出 AD-LiST-JEPA 框架。该模型基于联合嵌入预测架构（JEPA），利用大量的无标注 LiDAR 数据，通过在嵌入空间中预测未来的时空演化（Spatiotemporal Evolution）来进行自监督训练，避免了对高昂人工标注的依赖。

Result: 通过下游任务“LiDAR 占用补全与预测（OCF）”进行验证，实验结果显示，使用经过 JEPA 预训练的编码器比非预训练模型在感知和预测的联合任务中表现出更好的性能。

Conclusion: AD-LiST-JEPA 证明了在嵌入空间中预测 LiDAR 数据的时空演变是一种有效的自监督学习方式。该模型能够学习到高质量的环境表示，从而增强自动驾驶系统在复杂动态环境中的感知与长时预测能力。

Abstract: Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.

</details>


### [99] [PLLM: Pseudo-Labeling Large Language Models for CAD Program Synthesis](https://arxiv.org/abs/2602.12561)
*Yuanbo Li,Dule Shu,Yanying Chen,Matt Klenk,Daniel Ritchie*

Main category: cs.CV

TL;DR: PLLM 是一个针对无标签 3D 形状的自训练框架，通过迭代采样、筛选和增强合成程序-形状对，实现了无需成对数据的 CAD 程序高效合成。


<details>
  <summary>Details</summary>
Motivation: 从 3D 几何恢复 CAD 程序（CAD 逆向工程）具有挑战性，且现有方法高度依赖成对的“形状-程序”监督数据，而此类标注数据在现实中往往难以获取。

Method: 提出了 PLLM 框架。该框架利用预训练的 CAD LLM，在无标签形状数据集上进行迭代：首先采样候选程序，接着筛选出具有高几何保真度的执行结果，并对程序进行增强，从而构建伪标签（程序-形状对）用于模型微调。

Result: 在将 CAD-Recode 模型（基于 DeepCAD 数据集）迁移至无标签 ABC 数据集的实验中，PLLM 显著提升了重构结果的几何保真度和生成的程序多样性。

Conclusion: PLLM 证明了在缺乏成对标注数据的情况下，通过自训练和合成数据增强可以显著提升 LLM 在 3D CAD 建模任务中的跨域泛化能力和生成质量。

Abstract: Recovering Computer-Aided Design (CAD) programs from 3D geometries is a widely studied problem. Recent advances in large language models (LLMs) have enabled progress in CAD program synthesis, but existing methods rely on supervised training with paired shape-program data, which is often unavailable. We introduce PLLM, a self-training framework for CAD program synthesis from unlabeled 3D shapes. Given a pre-trained CAD-capable LLM and a shape dataset, PLLM iteratively samples candidate programs, selects high-fidelity executions, and augments programs to construct synthetic program-shape pairs for fine-tuning. We experiment on adapting CAD-Recode from DeepCAD to the unlabeled ABC dataset show consistent improvements in geometric fidelity and program diversity.

</details>


### [100] [The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving](https://arxiv.org/abs/2602.12563)
*Jiabao Wang,Hongyu Zhou,Yuanbo Yang,Jiahao Shao,Yiyi Liao*

Main category: cs.CV

TL;DR: 本文指出了自动驾驶中外观与结构偏移未解耦的问题，推出了navdream鲁棒性基准，并提出利用DINOv3提取外观无关特征作为感知接口，显著提升了规划算法在OOD条件下的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶算法在分布外（OOD）条件下表现脆弱。当前研究未能有效区分“外观偏移”（如天气、光照）与“结构变化”（如道路几何），导致难以判断规划器的失效是由复杂的路况还是仅仅由视觉干扰引起的。

Method: 1. **构建navdream基准**：利用生成式像素对齐风格迁移技术，在保持道路几何结构不变的前提下，仅改变环境外观（天气、光照等），以隔离外观对规划性能的影响。
2. **提出通用感知接口**：采用冻结权重的视觉基座模型（DINOv3）提取具有外观不变性（appearance-invariant）的特征，并将其作为回归、扩散和评分等不同类型规划器的统一输入接口。

Result: 实验表明，现有规划算法在仅改变外观的OOD条件下性能显著下降。而所提出的基于DINOv3的即插即用方案，在回归、扩散和评分等多种规划范式下均实现了卓越的零样本泛化性能，无需额外微调即可在极端外观变化下保持规划一致性。

Conclusion: 通过引入外观无关的视觉特征作为感知接口，可以有效解决自动驾驶规划器对环境风格变化的敏感性，显著提升系统在极端分布外（OOD）场景下的泛化能力与稳健性。

Abstract: Despite rapid progress, autonomous driving algorithms remain notoriously fragile under Out-of-Distribution (OOD) conditions. We identify a critical decoupling failure in current research: the lack of distinction between appearance-based shifts, such as weather and lighting, and structural scene changes. This leaves a fundamental question unanswered: Is the planner failing because of complex road geometry, or simply because it is raining? To resolve this, we establish navdream, a high-fidelity robustness benchmark leveraging generative pixel-aligned style transfer. By creating a visual stress test with negligible geometric deviation, we isolate the impact of appearance on driving performance. Our evaluation reveals that existing planning algorithms often show significant degradation under OOD appearance conditions, even when the underlying scene structure remains consistent. To bridge this gap, we propose a universal perception interface leveraging a frozen visual foundation model (DINOv3). By extracting appearance-invariant features as a stable interface for the planner, we achieve exceptional zero-shot generalization across diverse planning paradigms, including regression-based, diffusion-based, and scoring-based models. Our plug-and-play solution maintains consistent performance across extreme appearance shifts without requiring further fine-tuning. The benchmark and code will be made available.

</details>


### [101] [Unbiased Gradient Estimation for Event Binning via Functional Backpropagation](https://arxiv.org/abs/2602.12590)
*Jinze Chen,Wei Zhai,Han Han,Tiankai Ma,Yang Cao,Bin Li,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了 EventFBP，一个通过分部积分实现事件分箱函数无偏梯度估计的框架，显著提升了事件相机在光流、SLAM 等视觉任务中的优化效率与精度。


<details>
  <summary>Details</summary>
Motivation: 事件相机产生的异步脉冲通常需通过分箱（binning）操作转化为帧以便处理，但分箱函数的不连续性会导致反向传播中梯度截断或估计偏差，限制了模型直接从原始事件中学习的效率。

Method: 提出一种基于分部积分（Integration by Parts, IBP）的无偏梯度估计方法。该方法在反向传播过程中将目标函数提升为泛函，通过采样余切向量重建余切函数，从而合成弱导数（weak derivatives）。该导数在数学上可证明匹配平滑和非平滑目标的长期有限差分，且不改变前向传播输出。

Result: 实验表明，该方法在自我运动估计中误差降低 3.2% 且收敛速度提升 1.57 倍；在自监督光流任务中端点误差（EPE）降低 9.4%；在 SLAM 任务中均方根误差（RMS）降低 5.1%。

Conclusion: EventFBP 框架有效地解决了事件视觉中分箱操作带来的梯度不连续挑战，为事件驱动的感知任务提供了一种通用的、高性能的梯度优化方案。

Abstract: Event-based vision encodes dynamic scenes as asynchronous spatio-temporal spikes called events. To leverage conventional image processing pipelines, events are typically binned into frames. However, binning functions are discontinuous, which truncates gradients at the frame level and forces most event-based algorithms to rely solely on frame-based features. Attempts to directly learn from raw events avoid this restriction but instead suffer from biased gradient estimation due to the discontinuities of the binning operation, ultimately limiting their learning efficiency. To address this challenge, we propose a novel framework for unbiased gradient estimation of arbitrary binning functions by synthesizing weak derivatives during backpropagation while keeping the forward output unchanged. The key idea is to exploit integration by parts: lifting the target functions to functionals yields an integral form of the derivative of the binning function during backpropagation, where the cotangent function naturally arises. By reconstructing this cotangent function from the sampled cotangent vector, we compute weak derivatives that provably match long-range finite differences of both smooth and non-smooth targets. Experimentally, our method improves simple optimization-based egomotion estimation with 3.2\% lower RMS error and 1.57$\times$ faster convergence. On complex downstream tasks, we achieve 9.4\% lower EPE in self-supervised optical flow, and 5.1\% lower RMS error in SLAM, demonstrating broad benefits for event-based visual perception. Source code can be found at https://github.com/chjz1024/EventFBP.

</details>


### [102] [QuEPT: Quantized Elastic Precision Transformers with One-Shot Calibration for Multi-Bit Switching](https://arxiv.org/abs/2602.12609)
*Ke Xu,Yixin Wang,Zhongcheng Li,Hao Cui,Jinshui Hu,Xingyi Zhang*

Main category: cs.CV

TL;DR: 本文提出 QuEPT，一种针对 LLM 的高效训练后弹性精度量化方案。它通过级联低秩适配器（MB-CLoRA）和 Token 合并（MB-ToMe）技术，实现了在不同位宽及混合精度间的实时高效切换，并在保持高精度的同时显著降低了优化成本。


<details>
  <summary>Details</summary>
Motivation: 弹性精度量化虽能实现单次优化、多位宽部署，但 Transformer 架构（尤其是大语言模型）面临极高的存储和优化成本，导致现有弹性量化研究在 LLM 领域应用受限。

Method: 1. **QuEPT 架构**：采用训练后量化（PTQ）方案，通过少量数据的一次性校准重建块级多比特误差。
2. **MB-CLoRA（多比特级联低秩适配器）**：通过级联低秩适配器动态适应预定义位宽，增强不同位宽组间的相关性。
3. **MB-ToMe（多比特 Token 合并）**：动态融合不同位宽下的 Token 特征，提升位宽切换时的精度稳定性。
4. **实时切换**：支持在统一量化与混合精度量化之间实时切换，无需重复优化。

Result: 广泛的实验表明，QuEPT 在性能上达到甚至超过了现有最先进的（SOTA）训练后量化（PTQ）方法。

Conclusion: QuEPT 是一种针对大语言模型的高效、鲁棒且灵活的弹性精度量化框架，为多位宽部署提供了低成本的优化路径。

Abstract: Elastic precision quantization enables multi-bit deployment via a single optimization pass, fitting diverse quantization scenarios.Yet, the high storage and optimization costs associated with the Transformer architecture, research on elastic quantization remains limited, particularly for large language models.This paper proposes QuEPT, an efficient post-training scheme that reconstructs block-wise multi-bit errors with one-shot calibration on a small data slice. It can dynamically adapt to various predefined bit-widths by cascading different low-rank adapters, and supports real-time switching between uniform quantization and mixed precision quantization without repeated optimization. To enhance accuracy and robustness, we introduce Multi-Bit Token Merging (MB-ToMe) to dynamically fuse token features across different bit-widths, improving robustness during bit-width switching. Additionally, we propose Multi-Bit Cascaded Low-Rank adapters (MB-CLoRA) to strengthen correlations between bit-width groups, further improve the overall performance of QuEPT. Extensive experiments demonstrate that QuEPT achieves comparable or better performance to existing state-of-the-art post-training quantization methods.Our code is available at https://github.com/xuke225/QuEPT

</details>


### [103] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: 本文提出 LGIP 基准，通过同义改写和语义翻转测试 VLMs 的语言鲁棒性。实验发现 EVA02-CLIP 表现稳健，而 SigLIP 系列在语义微调上存在严重缺陷，且这些问题无法通过传统指标发现。


<details>
  <summary>Details</summary>
Motivation: 尽管 CLIP、SigLIP 等视觉语言模型（VLMs）在零样本任务中表现强劲，但它们在面对受控的语言扰动（如同义改写或语义翻转）时的鲁棒性和反应一致性尚不明确。本研究旨在填补这一评估空白。

Method: 引入了 **Language-Guided Invariance Probing (LGIP)** 基准测试。该方法利用 40k 张 MS COCO 图像及其人工说明，自动生成**意义保持的改写（Paraphrases）**和**改变意义的语义翻转（Flips，涉及对象类别、颜色或数量）**。评估指标包括：不变性误差（Invariance Error）、语义敏感度间隙（Semantic Sensitivity Gap）和正向率统计（Positive-rate Statistic）。

Result: 1. **模型表现分化**：EVA02-CLIP 和大型 OpenCLIP 变体在不变性与敏感度之间达到了较好的平衡。2. **SigLIP 缺陷**：SigLIP 和 SigLIP2 显示出较大的不变性误差，且经常错误地偏好语义翻转后的标题而非原始说明，尤其在物体和颜色修改方面。3. **度量标准差异**：这些鲁棒性失败在标准的检索指标（Retrieval Metrics）中几乎是不可见的。

Conclusion: LGIP 提供了一种与模型无关的诊断工具，证明了传统的检索指标无法全面反映 VLMs 的语言鲁棒性。该基准能够识别出常规准确率分数之外的模型缺陷，为评估和改进视觉语言模型的语言理解能力提供了新维度。

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [104] [ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models](https://arxiv.org/abs/2602.12640)
*Peijie Qiu,Hariharan Ramshankar,Arnau Ramisa,René Vidal,Amit Kumar K C,Vamsi Salaka,Rahul Bhagat*

Main category: cs.CV

TL;DR: ImageRAGTurbo 通过检索增强技术（RAG）和 $\mathcal{H}$ 空间适配器，在保持少步采样（1-4步）低延迟的同时，显著提升了扩散模型生成图像的质量与提示词对齐度。


<details>
  <summary>Details</summary>
Motivation: 尽管少步扩散模型（1-4步）显著降低了文本生成图像的延迟，但通常会牺牲图像质量和提示词对齐度，且现有的少步模型微调方法计算成本高昂。作者旨在通过检索增强技术，在不增加显著计算开销的前提下，提升少步生成模型的保真度。

Method: 提出 **ImageRAGTurbo** 架构。该方法根据输入提示词从数据库检索相关的“文本-图像对”，并将其作为上下文信息。具体操作上，在 UNet 去噪器的潜在空间（$\mathcal{H}$-space）中引入一个可训练的适配器，通过交叉注意力（Cross-attention）机制将检索到的内容与目标提示词进行融合，从而引导生成过程。

Result: 实验表明，即使不进行微调，仅在 $\mathcal{H}$ 空间编辑检索内容也能提升提示词忠实度。在结合可训练适配器后，ImageRAGTurbo 在保持极低推理延迟的同时，生成的图像保真度和提示词对齐度显著优于现有的少步扩散模型。

Conclusion: 通过在少步扩散模型中引入检索增强（RAG）和 $\mathcal{H}$ 空间适配器，证明了检索到的上下文信息能有效弥补步数减少带来的质量损失，为实现高效且高质量的文本生成图像提供了一种低成本且高性能的新途径。

Abstract: Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.

</details>


### [105] [Multi-Task Learning with Additive U-Net for Image Denoising and Classification](https://arxiv.org/abs/2602.12649)
*Vikram Lakkavalli,Neelam Sinha*

Main category: cs.CV

TL;DR: AddUNet 通过将拼接跳跃连接改为带门控的加性融合，实现了高效的架构正则化，在提升图像去噪稳定性的同时，优化了多任务学习中的特征解耦与分配。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统 U-Net 拼接式跳跃连接可能导致的捷径容量过剩及在多任务学习（MTL）中优化不稳定的问题。研究者希望通过改进跳跃连接方式，提升图像去噪及其关联多任务场景下的训练稳定性和特征分配效率。

Method: 提出 Additive U-Net (AddUNet)，将 U-Net 中传统的拼接式跳跃连接（Concatenative Skips）替换为门控加性融合（Gated Additive Fusion）。该设计限制了捷径通道的容量，并使模型在不同深度下保持固定的特征维度，从而实现对信息流的结构化正则。

Result: 1. **性能与稳定性**：AddUNet 在去噪和分类任务中表现出极佳的重建性能和更高的训练稳定性。2. **任务感知分布**：在 MTL 模式下，权重自动分配显示浅层跳跃侧重重建，深层特征侧重分类。3. **任务解耦**：即使在分类能力受限的情况下，重建任务仍能保持鲁棒性，证明了加性融合带来的隐式任务解耦效应。

Conclusion: 对跳跃连接施加简单的结构约束（如加性融合）是一种有效的架构正则化手段。它能在不增加模型复杂度的前提下，显著提升多任务学习的稳定性和可扩展性，并实现任务间的有效解耦。

Abstract: We investigate additive skip fusion in U-Net architectures for image denoising and denoising-centric multi-task learning (MTL). By replacing concatenative skips with gated additive fusion, the proposed Additive U-Net (AddUNet) constrains shortcut capacity while preserving fixed feature dimensionality across depth. This structural regularization induces controlled encoder-decoder information flow and stabilizes joint optimization. Across single-task denoising and joint denoising-classification settings, AddUNet achieves competitive reconstruction performance with improved training stability. In MTL, learned skip weights exhibit systematic task-aware redistribution: shallow skips favor reconstruction, while deeper features support discrimination. Notably, reconstruction remains robust even under limited classification capacity, indicating implicit task decoupling through additive fusion. These findings show that simple constraints on skip connections act as an effective architectural regularizer for stable and scalable multi-task learning without increasing model complexity.

</details>


### [106] [CBEN -- A Multimodal Machine Learning Dataset for Cloud Robust Remote Sensing Image Understanding](https://arxiv.org/abs/2602.12652)
*Marco Stricker,Masakazu Iwamura,Koichi Kise*

Main category: cs.CV

TL;DR: 本文通过构建CBEN数据集，研究并证明了将含云光学图像与雷达数据结合进行训练，可以显著提升遥感模型在多云天气下的鲁棒性和实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 光学卫星图像常受云层遮挡，而现有研究多将含云图像排除在训练集外，导致模型在应对自然灾害等时效性强的多云场景时表现不佳。虽然多模态（光学+雷达）融合是潜在方案，但缺乏针对含云数据的专门训练与评估。

Method: 1. 构建了CloudyBigEarthNet (CBEN) 数据集，包含配对的含云光学和雷达（SAR）图像。2. 定量评估了现有SOTA多模态方法在云遮挡下的性能下降情况。3. 将这些方法在含云光学数据上进行重新训练/微调，以增强其对云层的鲁棒性。

Result: 1. SOTA模型在晴空数据训练但在含云数据评估时，平均精度(AP)下降了23-33个百分点。2. 通过在含云光学数据上进行训练，模型在含云测试集上的AP相对提升了17.2-28.7个百分点。

Conclusion: 在训练多模态遥感模型时，包含含云光学数据是提高模型在现实、具有挑战性天气条件下鲁棒性的关键，这比单纯依赖晴空数据训练更为有效。

Abstract: Clouds are a common phenomenon that distorts optical satellite imagery, which poses a challenge for remote sensing. However, in the literature cloudless analysis is often performed where cloudy images are excluded from machine learning datasets and methods. Such an approach cannot be applied to time sensitive applications, e.g., during natural disasters. A possible solution is to apply cloud removal as a preprocessing step to ensure that cloudfree solutions are not failing under such conditions. But cloud removal methods are still actively researched and suffer from drawbacks, such as generated visual artifacts. Therefore, it is desirable to develop cloud robust methods that are less affected by cloudy weather. Cloud robust methods can be achieved by combining optical data with radar, a modality unaffected by clouds. While many datasets for machine learning combine optical and radar data, most researchers exclude cloudy images. We identify this exclusion from machine learning training and evaluation as a limitation that reduces applicability to cloudy scenarios. To investigate this, we assembled a dataset, named CloudyBigEarthNet (CBEN), of paired optical and radar images with cloud occlusion for training and evaluation. Using average precision (AP) as the evaluation metric, we show that state-of-the-art methods trained on combined clear-sky optical and radar imagery suffer performance drops of 23-33 percentage points when evaluated on cloudy images. We then adapt these methods to cloudy optical data during training, achieving relative improvement of 17.2-28.7 percentage points on cloudy test cases compared with the original approaches. Code and dataset are publicly available at: https://github.com/mstricker13/CBEN

</details>


### [107] [IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models](https://arxiv.org/abs/2602.12659)
*Aarish Shah Mohsin,Mohammed Tayyab Ilyas Khan,Mohammad Nadeem,Shahab Saquib Sohail,Erik Cambria,Jiechao Gao*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-Language Models (VLMs) are known to inherit and amplify societal biases from their web-scale training data with Indian being particularly misrepresented. Existing fairness-aware datasets have significantly improved demographic balance across global race and gender groups, yet they continue to treat Indian as a single monolithic category. The oversimplification ignores the vast intra-national diversity across 28 states and 8 Union Territories of India and leads to representational and geographical bias. To address the limitation, we present IndicFairFace, a novel and balanced face dataset comprising 14,400 images representing geographical diversity of India. Images were sourced ethically from Wikimedia Commons and open-license web repositories and uniformly balanced across states and gender. Using IndicFairFace, we quantify intra-national geographical bias in prominent CLIP-based VLMs and reduce it using post-hoc Iterative Nullspace Projection debiasing approach. We also show that the adopted debiasing approach does not adversely impact the existing embedding space as the average drop in retrieval accuracy on benchmark datasets is less than 1.5 percent. Our work establishes IndicFairFace as the first benchmark to study geographical bias in VLMs for the Indian context.

</details>


### [108] [Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening](https://arxiv.org/abs/2602.12679)
*Wooseok Jeon,Seunghyun Shin,Dongmin Shin,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent progress in image-to-video (I2V) diffusion models has significantly advanced the field of generative inbetweening, which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in parallel or alternating them sequentially, often suffers from temporal discontinuities and undesirable visual artifacts due to the misalignment between the two generated paths. This is because each path follows the motion prior induced by its own conditioning frame. In this work, we propose Motion Prior Distillation (MPD), a simple yet effective inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path. Our method can deliberately avoid denoising the end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. We not only perform quantitative evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios.

</details>


### [109] [Channel-Aware Probing for Multi-Channel Imaging](https://arxiv.org/abs/2602.12696)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: 本文提出了 Channel-Aware Probing (CAP)，通过独立编码各通道并采用解耦池化策略，显著提升了冻结预训练编码器在多通道成像任务中的下游迁移性能。


<details>
  <summary>Details</summary>
Motivation: 多通道成像（MCI）数据存在通道配置多样性的挑战，限制了预训练编码器的复用。现有研究多关注全微调或表征改进，而针对如何利用“冻结”表征处理下游任务的研究不足。此外，直接应用其他领域的探测策略于 MCI 效果较差，甚至不如从头训练。

Method: 提出了 Channel-Aware Probing (CAP) 框架，包含两个核心组件：
1. **独立特征编码 (Independent Feature Encoding, IFE)**：对每个通道进行单独编码，以捕捉 MCI 数据固有的通道间多样性。
2. **解耦池化 (Decoupled Pooling, DCP)**：在跨通道聚合之前先进行通道内池化，实现对特征流的精细化控制。

Result: 在三个 MCI 基准数据集上的实验表明，CAP 显著优于默认探测协议，其表现可媲美从头训练的模型，并大幅缩小了冻结编码器探测与全微调（full fine-tuning）之间的性能差距。

Conclusion: CAP 为多通道成像（MCI）任务提供了一种高效的评估与适配协议，通过优化固定特征的提取与聚合，有效克服了跨数据集通道配置不一致的问题，增强了预训练编码器的实用价值。

Abstract: Training and evaluating vision encoders on Multi-Channel Imaging (MCI) data remains challenging as channel configurations vary across datasets, preventing fixed-channel training and limiting reuse of pre-trained encoders on new channel settings. Prior work trains MCI encoders but typically evaluates them via full fine-tuning, leaving probing with frozen pre-trained encoders comparatively underexplored. Existing studies that perform probing largely focus on improving representations, rather than how to best leverage fixed representations for downstream tasks. Although the latter problem has been studied in other domains, directly transferring those strategies to MCI yields weak results, even worse than training from scratch. We therefore propose Channel-Aware Probing (CAP), which exploits the intrinsic inter-channel diversity in MCI datasets by controlling feature flow at both the encoder and probe levels. CAP uses Independent Feature Encoding (IFE) to encode each channel separately, and Decoupled Pooling (DCP) to pool within channels before aggregating across channels. Across three MCI benchmarks, CAP consistently improves probing performance over the default probing protocol, matches fine-tuning from scratch, and largely reduces the gap to full fine-tuning from the same MCI pre-trained checkpoints. Code can be found in https://github.com/umarikkar/CAP.

</details>


### [110] [SPRig: Self-Supervised Pose-Invariant Rigging from Mesh Sequences](https://arxiv.org/abs/2602.12740)
*Ruipeng Wang,Langkun Zhong,Miaowei Wang*

Main category: cs.CV

TL;DR: 针对缺乏 T-pose 的序列数据，本文提出了 SPRig 微调框架，通过跨帧一致性损失实现了姿态无关且时序稳定的骨架绑定，有效消除了帧间拓扑不一致产生的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有的骨架绑定（Rigging）方法通常依赖于标准静态姿势（如 T-pose），但这在动物运动捕捉或由视频/AIGC 生成的网格序列中往往不存在。若逐帧应用现有方法，会导致姿态敏感性及帧间拓扑不一致，产生严重的伪影。

Method: 提出 SPRig，这是一个通用的微调框架，旨在现有模型基础上通过引入“跨帧一致性损失（cross-frame consistency losses）”来学习姿态无关的绑定权重。此外，研究者还提出了一种全新的“置换不变稳定性评估协议（permutation-invariant stability protocol）”用于验证绑定效果。

Result: 实验分析表明，SPRig 在时序稳定性方面达到了 SOTA 水平。在处理极具挑战性的序列数据时，该方法能够生成连贯的绑定结果，并大幅减少了基准方法中常见的视觉瑕疵（artifacts）。

Conclusion: SPRig 成功解决了序列化网格数据在缺乏标准静态姿势时的绑定难题，通过引入时序一致性约束，显著提升了现有骨架绑定模型在处理复杂动画序列时的鲁棒性和拓扑稳定性。

Abstract: State-of-the-art rigging methods assume a canonical rest pose--an assumption that fails for sequential data (e.g., animal motion capture or AIGC/video-derived mesh sequences) that lack the T-pose. Applied frame-by-frame, these methods are not pose-invariant and produce topological inconsistencies across frames. Thus We propose SPRig, a general fine-tuning framework that enforces cross-frame consistency losses to learn pose-invariant rigs on top of existing models. We validate our approach on rigging using a new permutation-invariant stability protocol. Experiments demonstrate SOTA temporal stability: our method produces coherent rigs from challenging sequences and dramatically reduces the artifacts that plague baseline methods. The code will be released publicly upon acceptance.

</details>


### [111] [Synthetic Craquelure Generation for Unsupervised Painting Restoration](https://arxiv.org/abs/2602.12742)
*Jana Cuch-Guillén,Antonio Agudo,Raül Pérez-Gonzalo*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cultural heritage preservation increasingly demands non-invasive digital methods for painting restoration, yet identifying and restoring fine craquelure patterns from complex brushstrokes remains challenging due to scarce pixel-level annotations. We propose a fully annotation-free framework driven by a domain-specific synthetic craquelure generator, which simulates realistic branching and tapered fissure geometry using Bézier trajectories. Our approach couples a classical morphological detector with a learning-based refinement module: a SegFormer backbone adapted via Low-Rank Adaptation (LoRA). Uniquely, we employ a detector-guided strategy, injecting the morphological map as an input spatial prior, while a masked hybrid loss and logit adjustment constrain the training to focus specifically on refining candidate crack regions. The refined masks subsequently guide an Anisotropic Diffusion inpainting stage to reconstruct missing content. Experimental results demonstrate that our pipeline significantly outperforms state-of-the-art photographic restoration models in zero-shot settings, while faithfully preserving the original paint brushwork.

</details>


### [112] [Towards reconstructing experimental sparse-view X-ray CT data with diffusion models](https://arxiv.org/abs/2602.12755)
*Nelas J. Thomsen,Xinyuan Wang,Felix Lucka,Ezgi Demircan-Tureyen*

Main category: cs.CV

TL;DR: 本文研究了扩散先验在稀疏视图CT实验数据中的表现，揭示了域偏移和前向模型不匹配对成像质量的影响，并提出通过退火似然调度来优化实验数据的重建效果。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的生成模型在解决稀疏视图CT等逆问题中具有潜力，但现有研究多限于合成数据。本研究旨在探究训练数据不匹配（域偏移）和前向模型不匹配如何影响扩散先验在实际实验数据中的应用效果。

Method: 通过物理幻像采集实验CT数据，并在具有不同域偏移（domain shift）程度的合成图像集上训练扩散先验。采用分解扩散采样（Decomposed Diffusion Sampling, DDS）方案，在从合成到实验数据的多个难度级别上进行评估，并引入退火似然调度（annealed likelihood schedules）来优化推理过程。

Result: 严重的域偏移会导致模型崩溃和幻觉，但具有多样性的先验优于匹配良好但范围狭窄的先验。前向模型不匹配会使样本偏离先验流形导致伪影，而退火似然调度能有效缓解此问题并提升计算效率。

Conclusion: 合成数据上的性能增益并不能直接转化为实验数据的实际表现。未来的研究和算法开发必须通过真实世界的基准测试（real-world benchmarks）进行验证，以确保其在实际应用中的稳健性。

Abstract: Diffusion-based image generators are promising priors for ill-posed inverse problems like sparse-view X-ray Computed Tomography (CT). As most studies consider synthetic data, it is not clear whether training data mismatch (``domain shift'') or forward model mismatch complicate their successful application to experimental data. We measured CT data from a physical phantom resembling the synthetic Shepp-Logan phantom and trained diffusion priors on synthetic image data sets with different degrees of domain shift towards it. Then, we employed the priors in a Decomposed Diffusion Sampling scheme on sparse-view CT data sets with increasing difficulty leading to the experimental data. Our results reveal that domain shift plays a nuanced role: while severe mismatch causes model collapse and hallucinations, diverse priors outperform well-matched but narrow priors. Forward model mismatch pulls the image samples away from the prior manifold, which causes artifacts but can be mitigated with annealed likelihood schedules that also increase computational efficiency. Overall, we demonstrate that performance gains do not immediately translate from synthetic to experimental data, and future development must validate against real-world benchmarks.

</details>


### [113] [Towards complete digital twins in cultural heritage with ART3mis 3D artifacts annotator](https://arxiv.org/abs/2602.12761)
*Dimitrios Karamatskos,Vasileios Arampatzakis,Vasileios Sevetlidis,Stavros Nousias,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,George Pavlidis*

Main category: cs.CV

TL;DR: ART3mis 是一个基于 Web 的 3D 文物标注工具，符合 W3C 标准，旨在让非技术背景的文化遗产专家能够轻松地对 3D 数字资产进行语义标注与协作。


<details>
  <summary>Details</summary>
Motivation: 现有的 3D 可视化工具缺乏对 3D 文物特定区域进行标注和元数据管理的高级功能。此外，现有解决方案往往局限于特定应用，缺乏通用性和跨平台的互操作性，且对非技术人员（如修复师、馆员）不够友好。

Method: 开发了名为 ART3mis 的通用型交互式 Web 标注工具。该工具遵循 W3C Web 标注数据模型（Web Annotation Data Model），支持对 3D 模型进行区域分割、文本标注和元数据挂载。

Result: 实现了一个功能丰富且用户友好的 Web 平台。即便是不具备 3D 成像和图形学专业知识的用户，也能轻松地对 3D 数字副本进行操作、分割和标注。

Conclusion: ART3mis 解决了 3D 文物信息的传播、分发和重用问题。通过遵循 W3C 标准，该工具提升了数据的互操作性，为文化遗产的数字化保护与研究提供了易用且高效的解决方案。

Abstract: Archaeologists, as well as specialists and practitioners in cultural heritage, require applications with additional functions, such as the annotation and attachment of metadata to specific regions of the 3D digital artifacts, to go beyond the simplistic three-dimensional (3D) visualization. Different strategies addressed this issue, most of which are excellent in their particular area of application, but their capacity is limited to their design's purpose; they lack generalization and interoperability. This paper introduces ART3mis, a general-purpose, user-friendly, feature-rich, interactive web-based textual annotation tool for 3D objects. Moreover, it enables the communication, distribution, and reuse of information as it complies with the W3C Web Annotation Data Model. It is primarily designed to help cultural heritage conservators, restorers, and curators who lack technical expertise in 3D imaging and graphics, handle, segment, and annotate 3D digital replicas of artifacts with ease.

</details>


### [114] [PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion](https://arxiv.org/abs/2602.12769)
*Hong-Phuc Lai,Phong Nguyen,Anh Tran*

Main category: cs.CV

TL;DR: PixelRush 是首个实用的免调优高分辨率图像生成框架，通过改进分块去噪与融合技术，将 4K 图像的生成速度提升了 10-35 倍，仅需 20 秒即可完成生成。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练扩散模型在图像生成上表现优异，但受限于训练时的原生分辨率。现有的免训练（training-free）高分辨率生成方法通常计算开销巨大（生成 4K 图像需 5 分钟以上），难以满足实际应用对效率的需求。

Method: PixelRush 采用了一种高效的分块推理（patch-based inference）架构：首先，它取消了传统方法中复杂的多次反演与再生循环，实现了低步数下的快速去噪；其次，针对少步数生成带来的分块痕迹，提出了“无缝融合策略（seamless blending strategy）”；最后，通过“噪声注入机制（noise injection mechanism）”来解决画面过度平滑的问题。

Result: 实验表明，PixelRush 能够在约 20 秒内生成 4K 分辨率图像，相比现有最先进（SOTA）方法实现了 10 倍至 35 倍的加速，同时在视觉保真度和图像质量上保持了领先水平。

Conclusion: PixelRush 证明了通过优化分块推理流程和融合机制，可以在不进行额外训练的情况下，在极短时间内实现高质量、超高分辨率的图像生成，为该领域的实际应用提供了高效的基准方案。

Abstract: Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\times$ to 35$\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.

</details>


### [115] [Bootstrapping MLLM for Weakly-Supervised Class-Agnostic Object Counting](https://arxiv.org/abs/2602.12774)
*Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Qijun Chen,Miaojing Shi*

Main category: cs.CV

TL;DR: 本文提出WS-COC，一种基于多模态大模型（MLLM）的弱监督类别无关物体计数框架，通过多轮对话推理和排序优化，在仅使用图像级标签的情况下实现了领先的全监督计数性能。


<details>
  <summary>Details</summary>
Motivation: 现有的全监督物体计数方法依赖高成本的点级标注，而目前的弱监督方法多局限于特定类别（如人脸/行人）。此外，直接微调多模态大模型（MLLM）进行计数存在模态鸿沟，难以直接准确预测数值。

Method: 提出WS-COC框架，包含三大核心策略：1. **分治识别对话微调（Divide-and-discern）**：通过多轮对话引导模型判断数量区间并逐步缩小范围；2. **比较排序计数优化（Compare-and-rank）**：训练模型学习多幅图像间物体数量的相对排序；3. **全局与局部计数增强（Global-and-local）**：融合全局视野与局部细节的预测结果以应对密集场景。

Result: 在FSC-147、CARPK、PUCPR+和ShanghaiTech等多个主流数据集上的实验结果显示，WS-COC在显著降低标注成本的前提下，性能匹配甚至超越了许多最先进的（SOTA）全监督计数方法。

Conclusion: WS-COC是首个利用MLLM驱动的弱监督类别无关计数框架，证明了在无需昂贵点级标注的情况下，通过合理利用大语言模型的推理和交互能力，可以达到甚至超越全监督计数方法的水平。

Abstract: Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object. Few weakly-supervised methods leverage only image-level object counts as supervision and achieve fairly promising results. They are, however, often limited to counting a single category, e.g. person. In this paper, we propose WS-COC, the first MLLM-driven weakly-supervised framework for class-agnostic object counting. Instead of directly fine-tuning MLLMs to predict object counts, which can be challenging due to the modality gap, we incorporate three simple yet effective strategies to bootstrap the counting paradigm in both training and testing: First, a divide-and-discern dialogue tuning strategy is proposed to guide the MLLM to determine whether the object count falls within a specific range and progressively break down the range through multi-round dialogue. Second, a compare-and-rank count optimization strategy is introduced to train the MLLM to optimize the relative ranking of multiple images according to their object counts. Third, a global-and-local counting enhancement strategy aggregates and fuses local and global count predictions to improve counting performance in dense scenes. Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech show that WS-COC matches or even surpasses many state-of-art fully-supervised methods while significantly reducing annotation costs. Code is available at https://github.com/viscom-tongji/WS-COC.

</details>


### [116] [Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation](https://arxiv.org/abs/2602.12843)
*Yichen Zhao,Zelin Peng,Piao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: 本文推出了 MMRad-IVL-22K，这是首个模拟放射科医师交错视觉语言推理过程的大规模胸部X射线数据集，旨在通过原生交错的视觉-语言链提升医疗AI的诊断准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的医学大视觉语言模型（LVLMs）通常只进行一次视觉检查，随后依赖纯文本思维链（CoT）推理，这容易导致模型产生幻觉。尽管一些方法引入了坐标信息，但仍无法捕捉纹理、密度等关键视觉细节。本文旨在模拟放射科医生在诊断中交替进行视觉观察和语言推理的真实工作流。

Method: 构建了首个大规模原生交错视觉语言推理数据集 MMRad-IVL-22K，包含21,994条诊断轨迹，覆盖35个解剖区域。该数据集通过视觉证据（Visual Rationales）与文本描述相结合，模拟放射科医生在诊断过程中重复进行的“观察-推理”循环，使模型在每一步推理中都能基于高保真视觉细节（而非简单的坐标）进行判断。

Result: 1. 在先进闭源模型上，多模态CoT引导的报告生成在临床准确度上显著优于纯文本CoT，其中 RadGraph 指标提升了 6%。
2. 在7个开源模型上的基准测试显示，在 MMRad-IVL-22K 上微调后的模型在推理一致性和报告质量上均优于现有的通用及医疗专用 LVLMs。

Conclusion: 高保真的交错式视觉语言证据是构建可靠医疗人工智能不可或缺的组件。MMRad-IVL-22K数据集证明了通过模拟放射科医师“视觉-语言交替”的思维模式，可以有效减少幻觉并提升临床诊断报告的准确性。

Abstract: Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.

</details>


### [117] [RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads](https://arxiv.org/abs/2602.12877)
*Vijayasri Iyer,Maahin Rathinagiriswaran,Jyothikamalesh S*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding road scenes is essential for autonomous driving, as it enables systems to interpret visual surroundings to aid in effective decision-making. We present Roadscapes, a multitask multimodal dataset consisting of upto 9,000 images captured in diverse Indian driving environments, accompanied by manually verified bounding boxes. To facilitate scalable scene understanding, we employ rule-based heuristics to infer various scene attributes, which are subsequently used to generate question-answer (QA) pairs for tasks such as object grounding, reasoning, and scene understanding. The dataset includes a variety of scenes from urban and rural India, encompassing highways, service roads, village paths, and congested city streets, captured in both daytime and nighttime settings. Roadscapes has been curated to advance research on visual scene understanding in unstructured environments. In this paper, we describe the data collection and annotation process, present key dataset statistics, and provide initial baselines for image QA tasks using vision-language models.

</details>


### [118] [RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training](https://arxiv.org/abs/2602.12892)
*Yunshuang Nie,Bingqian Lin,Minzhe Niu,Kun Xiang,Jianhua Han,Guowei Huang,Xingyue Quan,Hang Xu,Bokui Chen,Xiaodan Liang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bottlenecks. Current evaluation primarily relies on testing after supervised fine-tuning, which introduces laborious additional training and autoregressive decoding costs. Meanwhile, common pre-training metrics cannot quantify a model's perception and reasoning abilities in a disentangled manner. Furthermore, existing evaluation benchmarks are typically limited in scale or misaligned with pre-training objectives. Thus, we propose RADAR, an efficient ability-centric evaluation framework for Revealing Asymmetric Development of Abilities in MLLM pRe-training. RADAR involves two key components: (1) Soft Discrimination Score, a novel metric for robustly tracking ability development without fine-tuning, based on quantifying nuanced gradations of the model preference for the correct answer over distractors; and (2) Multi-Modal Mixture Benchmark, a new 15K+ sample benchmark for comprehensively evaluating pre-trained MLLMs' perception and reasoning abilities in a 0-shot manner, where we unify authoritative benchmark datasets and carefully collect new datasets, extending the evaluation scope and addressing the critical gaps in current benchmarks. With RADAR, we comprehensively reveal the asymmetric development of perceptual and reasoning capabilities in pretrained MLLMs across diverse factors, including data volume, model size, and pretraining strategy. Our RADAR underscores the need for a decomposed perspective on pre-training ability bottlenecks, informing targeted interventions to advance MLLMs efficiently. Our code is publicly available at https://github.com/Nieysh/RADAR.

</details>


### [119] [Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis](https://arxiv.org/abs/2602.13028)
*Runzhou Liu,Hailey Weingord,Sejal Mittal,Prakhar Dungarwal,Anusha Nandula,Bo Ni,Samyadeep Basu,Hongjie Chen,Nesreen K. Ahmed,Li Li,Jiayi Zhang,Koustava Goswami,Subhojyoti Mukherjee,Branislav Kveton,Puneet Mathur,Franck Dernoncourt,Yue Zhao,Yu Wang,Ryan A. Rossi,Zhengzhong Tu,Hongru Du*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大模型（MLLM）的细粒度图像编辑评估框架及人类验证基准，通过12个解释因子解决了传统指标不可靠的问题，实现了与人类偏好高度一致的自动化评估。


<details>
  <summary>Details</summary>
Motivation: 传统的图像编辑评估指标粒度较粗且缺乏可解释性，往往只关注视觉上的合理性，而无法有效衡量模型的可控性、编辑局部性以及对用户指令的忠实度。

Method: 提出了一种 MLLM-as-a-Judge（多模态大模型作为评判者）框架，将评估维度分解为涵盖图像保留、编辑质量和指令忠实度的12个细粒度解释因子。同时，构建了一个包含人类标注、MLLM 评估结果及多种模型输出的新型基准数据集。

Result: 实验证明，MLLM 评判者在细粒度上与人类评价高度一致，表现出可靠的评估能力。传统指标被证明难以区分过度编辑或语义不准确的输出，而 MLLM 框架在离线和在线设置下均能提供更直观、更具信息量的评估。

Conclusion: 该研究为图像编辑领域提供了一套标准化的细粒度评估基准和原则性的因子分解方法，为未来图像编辑模型的系统化比较和性能提升奠定了实践基础。

Abstract: Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators. We further demonstrate that traditional image editing metrics are often poor proxies for these factors, failing to distinguish over-edited or semantically imprecise outputs, whereas our judges provide more intuitive and informative assessments in both offline and online settings. Together, this work introduces a benchmark, a principled factorization, and empirical evidence positioning fine-grained MLLM judges as a practical foundation for studying, comparing, and improving image editing approaches.

</details>


### [120] [Robustness of Object Detection of Autonomous Vehicles in Adverse Weather Conditions](https://arxiv.org/abs/2602.12902)
*Fox Pettersen,Hong Zhu*

Main category: cs.CV

TL;DR: 本文提出一种通过逐步增强合成数据强度来评估自动驾驶目标检测模型鲁棒性的方法，并利用 AFFC 指标量化模型在恶劣天气和光照下的失效阈值。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的普及，确定模型在多变环境下的安全运行阈值至关重要。研究旨在评估目标检测模型在恶劣天气和光照条件下的鲁棒性，以确保公共安全。

Method: 提出一种基于数据增强的鲁棒性评估方法。通过对基准图像应用 7 种模拟恶劣环境（雾、雨、雪及 4 种光照变化）的增强算子，生成不同强度梯度的合成数据。引入“平均首次失效系数（AFFC）”作为度量指标，确定模型在特定环境条件下失效的最低强度阈值。

Result: 实验对比了 YOLOv5s、YOLOv11s、Faster R-CNN 和 Detectron2 四种模型。结果显示 Faster R-CNN 的鲁棒性最高（总平均 AFFC 为 71.9%），而 YOLO 变体的 AFFC 约为 43%。研究还发现，使用合成数据训练可提升鲁棒性，但过拟合可能导致性能边际递减或对原始环境的鲁棒性下降（遗忘现象）。

Conclusion: 该评估方法对于比较不同目标检测模型的鲁棒性是可行且高效的。此外，针对恶劣条件的合成数据训练能显著提升模型韧性，但需注意过度训练带来的边际递减效应和“遗忘现象”。

Abstract: As self-driving technology advances toward widespread adoption, determining safe operational thresholds across varying environmental conditions becomes critical for public safety. This paper proposes a method for evaluating the robustness of object detection ML models in autonomous vehicles under adverse weather conditions. It employs data augmentation operators to generate synthetic data that simulates different severance degrees of the adverse operation conditions at progressive intensity levels to find the lowest intensity of the adverse conditions at which the object detection model fails. The robustness of the object detection model is measured by the average first failure coefficients (AFFC) over the input images in the benchmark. The paper reports an experiment with four object detection models: YOLOv5s, YOLOv11s, Faster R-CNN, and Detectron2, utilising seven data augmentation operators that simulate weather conditions fog, rain, and snow, and lighting conditions of dark, bright, flaring, and shadow. The experiment data show that the method is feasible, effective, and efficient to evaluate and compare the robustness of object detection models in various adverse operation conditions. In particular, the Faster R-CNN model achieved the highest robustness with an overall average AFFC of 71.9% over all seven adverse conditions, while YOLO variants showed the AFFC values of 43%. The method is also applied to assess the impact of model training that targets adverse operation conditions using synthetic data on model robustness. It is observed that such training can improve robustness in adverse conditions but may suffer from diminishing returns and forgetting phenomena (i.e., decline in robustness) if overtrained.

</details>


### [121] [CoPE-VideoLM: Codec Primitives For Efficient Video Language Models](https://arxiv.org/abs/2602.13191)
*Sayan Deb Sarkar,Rémi Pautrat,Ondrej Miksik,Marc Pollefeys,Iro Armeni,Mahdi Rad,Mihai Dusmanu*

Main category: cs.CV

TL;DR: 本文提出一种利用视频编解码器原语（运动矢量和残差）的视频语言模型，通过轻量化编码器在降低 93% Token 消耗和 86% 首字延迟的同时，保持并优化了视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 目前的视频语言模型（VideoLMs）主要依赖稀疏的关键帧采样，这容易导致宏观事件和微观细节的丢失。此外，对每一帧进行全图像编码和处理会产生巨大的计算开销和 Token 负担，限制了长视频及高频信息的处理能力。

Method: 1. **利用编解码原语**：直接提取视频编解码器中的运动矢量（Motion Vectors）和残差（Residuals），利用其天然的去冗余特性。2. **轻量化编码器**：引入基于 Transformer 的轻量化编码器来聚合这些原语。3. **对齐预训练**：通过预训练策略将原语表示与图像编码器（Image Encoder）的嵌入进行空间对齐，从而加速端到端微调的收敛过程。

Result: 1. **效率显著提升**：与标准模型相比，首个 Token 生成时间（TTFT）缩短了 86%，Token 使用量减少了 93%。2. **性能卓越**：在涵盖通用问答、时间推理、长视频理解和空间场景理解的 14 个基准测试中，性能达到或超过了现有水平。

Conclusion: 本研究证明了利用视频编解码原语（而非原始像素）是提升视频语言模型效率的有效路径，在显著降低推理延迟和计算成本的同时，能够捕获更细致的视频时空特征。

Abstract: Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.

</details>


### [122] [Reliable Thinking with Images](https://arxiv.org/abs/2602.12916)
*Haobin Li,Yutong Yang,Yijie Lin,Dai Xiang,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: 本文研究了多模态推理中因视觉线索和推理过程不完美导致的“噪声思考”问题，并提出了 RTWI 方法，通过统一的可靠性评估、过滤及投票机制，有效提升了多模态大模型在图像思考过程中的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的“图像思考”（TWI）方法（如多模态 CoT）通常假设推理过程中生成的图文交织信息是完美的。然而在实际应用中，复杂的视觉理解常导致错误的视觉线索挖掘和推理过程，即**噪声思考（Noisy Thinking, NT）**。这种“一步错步步错”的误差累积会严重降低多模态大语言模型（MLLM）的性能。

Method: 提出了一种名为 **RTWI (Reliable Thinking with Images)** 的新方法：
1. **统一可靠性评估**：以文本为中心（text-centric）的方式，同时评估视觉线索和文本 CoT 的可靠性。
2. **过滤与投票模块**：利用稳健的过滤机制剔除低质量线索，并通过投票模块整合高置信度推理结果，防止噪声信息污染最终预测。

Result: 在 7 个基准测试集上的广泛实验表明，RTWI 能够有效应对噪声思考问题，在多种多模态推理任务中均表现出优于现有 TWI 方法的性能和稳健性。

Conclusion: RTWI 成功解决了 TWI 过程中被忽视的“噪声思考”问题。通过引入可靠性评估机制，该方法能显著减少多模态推理中的错误累积，为提升 MLLM 在复杂场景下的鲁棒性提供了有效方案。

Abstract: As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.

</details>


### [123] [EPRBench: A High-Quality Benchmark Dataset for Event Stream Based Visual Place Recognition](https://arxiv.org/abs/2602.12919)
*Xiao Wang,Xingxing Xiong,Jinfeng Gao,Xufeng Lou,Bo Jiang,Si-bao Chen,Yaowei Wang,Yonghong Tian*

Main category: cs.CV

TL;DR: 本文发布了首个高质量事件流视觉地点识别基准 EPRBench，并提出一种结合 LLM 文本生成的跨模态特征融合框架，旨在解决极端环境下 VPR 的性能与可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 传统可见光相机在低光照、曝光过度和高速运动等极端条件下鲁棒性不足。虽然事件相机能应对这些挑战，但该领域缺乏专用的大规模数据集，且现有 VPR 算法在语义感知和可解释性方面存在欠缺。

Method: 1. 构建 EPRBench 数据集：包含 1 万条事件序列和 6.5 万帧图像，涵盖多种拍摄设备、视角及气象光照条件；2. 语义集成：利用 LLM 生成场景描述并经人工校准；3. 基准测试：复现并评估了 15 种先进的 VPR 算法；4. 提出新模型：开发了一种多模态融合范式，利用 LLM 生成的文本引导空间注意力令牌选择、跨模态融合及多尺度特征学习。

Result: 建立了 15 种 SOTA 算法的性能基准；提出的多模态框架在实现高精度地点识别的同时，能够生成可解释的推理过程，显著提升了模型的透明度。

Conclusion: EPRBench 为事件流 VPR 研究提供了关键的基准测试平台；同时，结合 LLM 的多模态融合方案被证明能显著提升系统在复杂环境下的感知能力、识别精度及决策透明度。

Abstract: Event stream-based Visual Place Recognition (VPR) is an emerging research direction that offers a compelling solution to the instability of conventional visible-light cameras under challenging conditions such as low illumination, overexposure, and high-speed motion. Recognizing the current scarcity of dedicated datasets in this domain, we introduce EPRBench, a high-quality benchmark specifically designed for event stream-based VPR. EPRBench comprises 10K event sequences and 65K event frames, collected using both handheld and vehicle-mounted setups to comprehensively capture real-world challenges across diverse viewpoints, weather conditions, and lighting scenarios. To support semantic-aware and language-integrated VPR research, we provide LLM-generated scene descriptions, subsequently refined through human annotation, establishing a solid foundation for integrating LLMs into event-based perception pipelines. To facilitate systematic evaluation, we implement and benchmark 15 state-of-the-art VPR algorithms on EPRBench, offering a strong baseline for future algorithmic comparisons. Furthermore, we propose a novel multi-modal fusion paradigm for VPR: leveraging LLMs to generate textual scene descriptions from raw event streams, which then guide spatially attentive token selection, cross-modal feature fusion, and multi-scale representation learning. This framework not only achieves highly accurate place recognition but also produces interpretable reasoning processes alongside its predictions, significantly enhancing model transparency and explainability. The dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [124] [Beyond Benchmarks of IUGC: Rethinking Requirements of Deep Learning Methods for Intrapartum Ultrasound Biometry from Fetal Ultrasound Videos](https://arxiv.org/abs/2602.12922)
*Jieyun Bai,Zihao Zhou,Yitong Tang,Jie Gan,Zhuonan Liang,Jianan Fan,Lisa B. Mcguire,Jillian L. Clarke,Weidong Cai,Jacaueline Spurway,Yubo Tang,Shiye Wang,Wenda Shen,Wangwang Yu,Yihao Li,Philippe Zhang,Weili Jiang,Yongjie Li,Salem Muhsin Ali Binqahal Al Nasim,Arsen Abzhanov,Numan Saeed,Mohammad Yaqub,Zunhui Xian,Hongxing Lin,Libin Lan,Jayroop Ramesh,Valentin Bacher,Mark Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana I. L. Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale,Assanali Serikbey,Jiankai Li,Sung-Liang Chen,Zicheng Hu,Nana Liu,Yian Deng,Wei Hu,Cong Tan,Wenfeng Zhang,Mai Tuyet Nhi,Gregor Koehler,Rapheal Stock,Klaus Maier-Hein,Marawan Elbatel,Xiaomeng Li,Saad Slimani,Victor M. Campello,Benard Ohene-Botwe,Isaac Khobo,Yuxin Huang,Zhenyan Han,Hongying Hou,Di Qiu,Zheng Zheng,Gongning Luo,Dong Ni,Yaosheng Lu,Karim Lekadir,Shuo Li*

Main category: cs.CV

TL;DR: 本文综述了MICCAI 2024 IUGC挑战赛，发布了最大规模的多中心产时超声数据集，并评估了多任务AI模型在自动化分娩监测中的表现及挑战。


<details>
  <summary>Details</summary>
Motivation: 中低收入国家分娩期间的产妇及新生儿死亡率较高，产时超声生物测量在监测分娩进程中至关重要。然而，由于缺乏训练有素的超声医生，资源受限地区无法常规开展超声检查，亟需自动化测量工具。

Method: 依托MICCAI 2024举办了分娩阵痛期超声大挑战（IUGC），引入了一个集标准平面分类、胎头-耻骨联合分割及生物测量于一体的多任务自动测量框架。该挑战发布了包含774个视频、共68,106帧图像的最大规模多中心数据集，并对8支参赛队伍的方法从预处理、数据增强、学习策略、模型架构和后处理五个维度进行了综述。

Result: 通过对基准测试结果的系统分析，识别了当前算法的主要瓶颈，探讨了潜在解决方案，并强调了未来研究的开放性挑战。研究展示了多任务学习在利用互补信息提高测量精度方面的潜力。

Conclusion: 尽管研究取得了令人鼓舞的成果，但该领域仍处于早期阶段，在大规模临床部署前仍需深入研究。目前已公开所有基准方案和完整数据集，以促进产前超声自动生物测量的持续进步。

Abstract: A substantial proportion (45\%) of maternal deaths, neonatal deaths, and stillbirths occur during the intrapartum phase, with a particularly high burden in low- and middle-income countries. Intrapartum biometry plays a critical role in monitoring labor progression; however, the routine use of ultrasound in resource-limited settings is hindered by a shortage of trained sonographers. To address this challenge, the Intrapartum Ultrasound Grand Challenge (IUGC), co-hosted with MICCAI 2024, was launched. The IUGC introduces a clinically oriented multi-task automatic measurement framework that integrates standard plane classification, fetal head-pubic symphysis segmentation, and biometry, enabling algorithms to exploit complementary task information for more accurate estimation. Furthermore, the challenge releases the largest multi-center intrapartum ultrasound video dataset to date, comprising 774 videos (68,106 frames) collected from three hospitals, providing a robust foundation for model training and evaluation. In this study, we present a comprehensive overview of the challenge design, review the submissions from eight participating teams, and analyze their methods from five perspectives: preprocessing, data augmentation, learning strategy, model architecture, and post-processing. In addition, we perform a systematic analysis of the benchmark results to identify key bottlenecks, explore potential solutions, and highlight open challenges for future research. Although encouraging performance has been achieved, our findings indicate that the field remains at an early stage, and further in-depth investigation is required before large-scale clinical deployment. All benchmark solutions and the complete dataset have been publicly released to facilitate reproducible research and promote continued advances in automatic intrapartum ultrasound biometry.

</details>


### [125] [Deep-Learning Atlas Registration for Melanoma Brain Metastases: Preserving Pathology While Enabling Cohort-Level Analyses](https://arxiv.org/abs/2602.12933)
*Nanna E. Wielenberg,Ilinca Popp,Oliver Blanck,Lucas Zander,Jan C. Peeken,Stephanie E. Combs,Anca-Ligia Grosu,Dimos Baltas,Tobias Fechter*

Main category: cs.CV

TL;DR: 本文开发了一种无需病灶掩膜的深度学习可变形配准框架，用于将黑色素瘤脑转移 MRI 图像映射到标准图谱，揭示了病灶在灰白质交界处的空间分布偏好。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤脑转移（MBM）具有显著的空间异质性，由于个体解剖差异和 MRI 协议的不同，进行队列水平的标准化分析十分困难。现有配准方法通常依赖于病灶掩膜或复杂的预处理步骤，限制了大规模多中心研究的开展。

Method: 提出一种全微分的深度学习可变形配准框架。该方法通过基于距离变换解剖标签的前向模型相似性度量来处理病灶导致的解剖对应缺失，并结合体积保留正则化项（volume-preserving regularization）以确保变形的合理性。该框架无需病灶掩膜或复杂的预处理即可将病理脑图像对齐到标准图谱。

Result: 在 209 名 MBM 患者的数据集上，该方法实现了高精度的配准（Dice 0.89-0.92），并有效保留了转移瘤体积。空间分布分析显示，MBM 在大脑皮层和壳核显著多发，在白质中分布较少，且表现出向灰白质交界处聚集的特征。体积校正后，未发现特定动脉供血区的转移频率有显著差异。

Conclusion: 该框架实现了无需病灶掩膜的鲁棒病理脑部 MRI 图像配准，支持可重复的多中心队列研究。研究结果确认并细化了 MBM 的空间分布规律，特别是其在灰白质交界处和皮层区域的接种偏好。公开的代码实现有助于该方法扩展至其他脑肿瘤和神经系统病变研究。

Abstract: Melanoma brain metastases (MBM) are common and spatially heterogeneous lesions, complicating cohort-level analyses due to anatomical variability and differing MRI protocols. We propose a fully differentiable, deep-learning-based deformable registration framework that aligns individual pathological brains to a common atlas while preserving metastatic tissue without requiring lesion masks or preprocessing.
  Missing anatomical correspondences caused by metastases are handled through a forward-model similarity metric based on distance-transformed anatomical labels, combined with a volume-preserving regularization term to ensure deformation plausibility. Registration performance was evaluated using Dice coefficient (DSC), Hausdorff distance (HD), average symmetric surface distance (ASSD), and Jacobian-based measures. The method was applied to 209 MBM patients from three centres, enabling standardized mapping of metastases to anatomical, arterial, and perfusion atlases.
  The framework achieved high registration accuracy across datasets (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) while preserving metastatic volumes. Spatial analysis demonstrated significant over-representation of MBM in the cerebral cortex and putamen, under-representation in white matter, and consistent localization near the gray-white matter junction. No arterial territory showed increased metastasis frequency after volume correction.
  This approach enables robust atlas registration of pathological brain MRI without lesion masks and supports reproducible multi-centre analyses. Applied to MBM, it confirms and refines known spatial predilections, particularly preferential seeding near the gray-white matter junction and cortical regions. The publicly available implementation facilitates reproducible research and extension to other brain tumours and neurological pathologies.

</details>


### [126] [Detecting Object Tracking Failure via Sequential Hypothesis Testing](https://arxiv.org/abs/2602.12983)
*Alejandro Monroy Muñoz,Rajeev Verma,Alexander Timans*

Main category: cs.CV

TL;DR: 本文提出一种基于 e-process 序贯检验的方法，为实时目标跟踪提供统计学安全保证。它能快速检测跟踪失败并控制误报率，无需重新训练且适用于各种模型。


<details>
  <summary>Details</summary>
Motivation: 现有的实时在线目标跟踪系统缺乏正式的安全保证，难以准确判断跟踪何时可靠或失效，通常仅依赖启发式的置信度度量，这在安全敏感的应用中（如机器人、监控）存在隐患。

Method: 将目标跟踪解释为序贯假设检验（Sequential Hypothesis Test），并利用 e-process 形式化。该方法通过随时间累积支持或反对跟踪失败的证据，快速识别故障。研究涵盖了监督（利用地面真值）和无监督（仅利用内部跟踪信息）两种变体。

Result: 该方法能快速识别跟踪失效，同时能够根据预设速率严格控制误报（False Alerts）。其计算开销极低，无需额外的训练或微调，具有模型无关性。在四项视频基准测试和两种主流跟踪模型上的实验证明了其有效性。

Conclusion: 序贯检验（尤其是基于 e-process 的方法）为实时目标跟踪系统提供了一种统计学上可靠且高效的安全保障机制，能够在不增加模型负担的情况下显著提升系统的可靠性。

Abstract: Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.

</details>


### [127] [Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding](https://arxiv.org/abs/2602.12957)
*Wenhui Liao,Hongliang Li,Pengyu Xie,Xinyu Cai,Yufan Shen,Yi Xin,Qi Qin,Shenglong Ye,Tianbin Li,Ming Hu,Junjun He,Yihao Liu,Wenhai Wang,Min Dou,Bin Fu,Botian Shi,Yu Qiao,Lianwen Jin*

Main category: cs.CV

TL;DR: 针对VLM文档解析推理慢的问题，提出一种结合推测解码与布局感知并行策略的训练加速方法，在长文档解析中实现高达4.89倍的无损加速。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型（VLM）的端到端文档解析方法在处理长文档时，由于需要自回归生成极长的Token序列，导致推理延迟巨大，限制了其在实际场景中的应用。

Method: 提出一种无需训练的加速方法：1. **推测解码（Speculative Decoding）**：利用轻量级文档解析流水线作为草稿模型（Draft Model）预预测Token，再由VLM作为验证者进行并行校对；2. **区域并行解码（Region Parallel Decoding）**：利用文档布局结构将页面划分为独立区域，对各区域同步执行推测解码，最后按阅读顺序重组结果。

Result: 在通用基准测试 OmniDocBench 上，该方法为 dots.ocr 模型带来了 2.42 倍的无损加速；在长文档解析任务中，加速比最高可达 4.89 倍。

Conclusion: 该研究证明了通过结合推测解码与布局感知的并行化策略，可以在保持模型性能（无损）的前提下，显著解决VLM在文档解析任务中的高延迟瓶颈，尤其在长文档处理场景下表现优异。

Abstract: Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.

</details>


### [128] [MASAR: Motion-Appearance Synergy Refinement for Joint Detection and Trajectory Forecasting](https://arxiv.org/abs/2602.13003)
*Mohammed Amine Bencheikh Lehocine,Julian Schmidt,Frank Moosmann,Dikshant Gupta,Fabian Flohr*

Main category: cs.CV

TL;DR: MASAR是一个全微分的端到端框架，通过“向后看以向前看”的策略，利用目标中心的时空机制联合编码外观与运动特征，显著优化了3D检测与长时轨迹预测任务。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶系统感知与预测模块间的硬性接口限制了信息流动并导致误差累积。现有端到端模型往往忽略了外观与运动线索的协同作用，且缺乏对长期时间依赖的有效利用。

Method: 提出MASAR框架，这是一种全微分的联合3D检测与轨迹预测方案。其核心是“目标中心的时空机制”，通过预测并利用外观线索优化“过去轨迹”，从而捕捉长期时间依赖关系。该框架具有通用性，可兼容任何基于Transformer的3D检测器。

Result: 在nuScenes数据集上，MASAR在维持稳健检测性能的前提下，将轨迹预测的关键指标minADE和minFDE提升了20%以上。

Conclusion: MASAR证明了通过显式建模历史轨迹并融合多维度线索（外观与运动），可以有效增强端到端自动驾驶系统中感知与预测任务的协同效应，显著提升了长时轨迹预测的准确性。

Abstract: Classical autonomous driving systems connect perception and prediction modules via hand-crafted bounding-box interfaces, limiting information flow and propagating errors to downstream tasks. Recent research aims to develop end-to-end models that jointly address perception and prediction; however, they often fail to fully exploit the synergy between appearance and motion cues, relying mainly on short-term visual features. We follow the idea of "looking backward to look forward", and propose MASAR, a novel fully differentiable framework for joint 3D detection and trajectory forecasting compatible with any transformer-based 3D detector. MASAR employs an object-centric spatio-temporal mechanism that jointly encodes appearance and motion features. By predicting past trajectories and refining them using guidance from appearance cues, MASAR captures long-term temporal dependencies that enhance future trajectory forecasting. Experiments conducted on the nuScenes dataset demonstrate MASAR's effectiveness, showing improvements of over 20% in minADE and minFDE while maintaining robust detection performance. Code and models are available at https://github.com/aminmed/MASAR.

</details>


### [129] [Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation](https://arxiv.org/abs/2602.13055)
*Florinel-Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu,Nicu Sebe,Mubarak Shah*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). However, neither RLHF nor DPO take into account the fact that learning certain preferences is more difficult than learning other preferences, rendering the optimization process suboptimal. To address this gap in text-to-image generation, we recently proposed Curriculum-DPO, a method that organizes image pairs by difficulty. In this paper, we introduce Curriculum-DPO++, an enhanced method that combines the original data-level curriculum with a novel model-level curriculum. More precisely, we propose to dynamically increase the learning capacity of the denoising network as training advances. We implement this capacity increase via two mechanisms. First, we initialize the model with only a subset of the trainable layers used in the original Curriculum-DPO. As training progresses, we sequentially unfreeze layers until the configuration matches the full baseline architecture. Second, as the fine-tuning is based on Low-Rank Adaptation (LoRA), we implement a progressive schedule for the dimension of the low-rank matrices. Instead of maintaining a fixed capacity, we initialize the low-rank matrices with a dimension significantly smaller than that of the baseline. As training proceeds, we incrementally increase their rank, allowing the capacity to grow until it converges to the same rank value as in Curriculum-DPO. Furthermore, we propose an alternative ranking strategy to the one employed by Curriculum-DPO. Finally, we compare Curriculum-DPO++ against Curriculum-DPO and other state-of-the-art preference optimization approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO.

</details>


### [130] [Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions](https://arxiv.org/abs/2602.13013)
*Yunheng Li,Hengrui Zhang,Meng-Hao Guo,Wenzhao Gao,Shaoyong Jia,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.

</details>


### [131] [Multimodal Classification via Total Correlation Maximization](https://arxiv.org/abs/2602.13015)
*Feng Yu,Xiangyu Wu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal learning integrates data from diverse sensors to effectively harness information from different modalities. However, recent studies reveal that joint learning often overfits certain modalities while neglecting others, leading to performance inferior to that of unimodal learning. Although previous efforts have sought to balance modal contributions or combine joint and unimodal learning, thereby mitigating the degradation of weaker modalities with promising outcomes, few have examined the relationship between joint and unimodal learning from an information-theoretic perspective. In this paper, we theoretically analyze modality competition and propose a method for multimodal classification by maximizing the total correlation between multimodal features and labels. By maximizing this objective, our approach alleviates modality competition while capturing inter-modal interactions via feature alignment. Building on Mutual Information Neural Estimation (MINE), we introduce Total Correlation Neural Estimation (TCNE) to derive a lower bound for total correlation. Subsequently, we present TCMax, a hyperparameter-free loss function that maximizes total correlation through variational bound optimization. Extensive experiments demonstrate that TCMax outperforms state-of-the-art joint and unimodal learning approaches. Our code is available at https://github.com/hubaak/TCMax.

</details>


### [132] [DynaGuide: A Generalizable Dynamic Guidance Framework for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2602.13020)
*Boujemaa Guermazi,Riadh Ksantini,Naimul Khan*

Main category: cs.CV

TL;DR: 本文提出 DynaGuide，一种通过全局伪标签引导与局部 CNN 边界细化相结合的无监督图像分割框架，利用动态损失优化显著提升了场景理解的语义精度和边界清晰度。


<details>
  <summary>Details</summary>
Motivation: 针对无监督图像分割中，现有方法难以兼顾“全局语义结构”与“精细边界准确性”的问题，同时解决特定领域标注数据匮乏的挑战。

Method: 提出 DynaGuide 框架，核心是“双重引导策略”和“动态损失优化”：
1. **双重引导**：结合零样本模型（如 DiffSeg、SegFormer）提供的全局伪标签，以及由轻量级 CNN（从零训练）实现的局部边界细化。
2. **动态损失函数**：设计了多分量损失，动态平衡特征相似性、Huber 平滑的空间连续性（包含对角线关系）以及与全局伪标签的语义对齐。

Result: 在多个主流数据集上达到 SOTA 性能：
- **BSD500**：mIoU 提升 17.5%；
- **PASCAL VOC2012**：mIoU 提升 3.1%；
- **COCO**：mIoU 提升 11.66%。

Conclusion: DynaGuide 提供了一种可扩展且实用的无监督分割解决方案。其模块化设计允许即插即用不同的引导源，且在无需 ground-truth 标签的情况下，凭借其强大的泛化能力和极低的计算开销，在实际应用场景中具有显著优势。

Abstract: Unsupervised image segmentation is a critical task in computer vision. It enables dense scene understanding without human annotations, which is especially valuable in domains where labelled data is scarce. However, existing methods often struggle to reconcile global semantic structure with fine-grained boundary accuracy. This paper introduces DynaGuide, an adaptive segmentation framework that addresses these challenges through a novel dual-guidance strategy and dynamic loss optimization. Building on our previous work, DynaSeg, DynaGuide combines global pseudo-labels from zero-shot models such as DiffSeg or SegFormer with local boundary refinement using a lightweight CNN trained from scratch. This synergy allows the model to correct coarse or noisy global predictions and produce high-precision segmentations. At the heart of DynaGuide is a multi-component loss that dynamically balances feature similarity, Huber-smoothed spatial continuity, including diagonal relationships, and semantic alignment with the global pseudo-labels. Unlike prior approaches, DynaGuide trains entirely without ground-truth labels in the target domain and supports plug-and-play integration of diverse guidance sources. Extensive experiments on BSD500, PASCAL VOC2012, and COCO demonstrate that DynaGuide achieves state-of-the-art performance, improving mIoU by 17.5% on BSD500, 3.1% on PASCAL VOC2012, and 11.66% on COCO. With its modular design, strong generalization, and minimal computational footprint, DynaGuide offers a scalable and practical solution for unsupervised segmentation in real-world settings. Code available at: https://github.com/RyersonMultimediaLab/DynaGuide

</details>


### [133] [Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels](https://arxiv.org/abs/2602.13022)
*Julius Pesonen,Stefan Rua,Josef Taher,Niko Koivumäki,Xiaowei Yu,Eija Honkavaara*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.

</details>


### [134] [FedHENet: A Frugal Federated Learning Framework for Heterogeneous Environments](https://arxiv.org/abs/2602.13024)
*Alejandro Dopico-Castro,Oscar Fontenla-Romero,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos,Iván Pérez Digón*

Main category: cs.CV

TL;DR: FedHENet 通过固定特征提取器和单轮同态加密解析聚合，实现了一种无需迭代、无需调参且高能效的隐私保护联邦图像分类方法。


<details>
  <summary>Details</summary>
Motivation: 针对传统联邦学习（FL）存在的痛点：迭代优化成本高昂、共享梯度存在隐私泄露风险、超参数调优导致的高能耗（碳足迹）以及模型训练的不稳定性。

Method: 提出 FedHENet 框架：1) 采用固定的预训练特征提取器，避免本地深度微调；2) 仅对最后一层输出层进行建模；3) 利用同态加密（HE）技术，通过单轮通信对各客户端知识进行解析式（Analytical）聚合。

Result: 实验表明，FedHENet 在图像分类任务中达到了与主流迭代式 FL 基准相当的准确率；其能源效率提升了高达 70%；具备更强的稳定性，且完全免去了超参数调优的过程。

Conclusion: FedHENet 证明了在联邦学习中，通过单轮解析聚合结合同态加密，可以在不牺牲准确性的前提下，显著降低计算成本和碳排放，是传统迭代式联邦学习的一种高效、隐私安全的替代方案。

Abstract: Federated Learning (FL) enables collaborative training without centralizing data, essential for privacy compliance in real-world scenarios involving sensitive visual information. Most FL approaches rely on expensive, iterative deep network optimization, which still risks privacy via shared gradients. In this work, we propose FedHENet, extending the FedHEONN framework to image classification. By using a fixed, pre-trained feature extractor and learning only a single output layer, we avoid costly local fine-tuning. This layer is learned by analytically aggregating client knowledge in a single round of communication using homomorphic encryption (HE). Experiments show that FedHENet achieves competitive accuracy compared to iterative FL baselines while demonstrating superior stability performance and up to 70\% better energy efficiency. Crucially, our method is hyperparameter-free, removing the carbon footprint associated with hyperparameter tuning in standard FL. Code available in https://github.com/AlejandroDopico2/FedHENet/

</details>


### [135] [Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images](https://arxiv.org/abs/2602.13041)
*Yuhao Chen,Gautham Vinod,Siddeshwar Raghavan,Talha Ibn Mahmud,Bruce Coburn,Jinge Ma,Fengqing Zhu,Jiangpeng He*

Main category: cs.CV

TL;DR: 本文提出了一个单目多食物图像的隐式比例3D重建基准数据集，旨在通过利用餐具等上下文线索解决尺度歧义问题，从而提升食物分量估计的几何推理精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的饮食评估方法（包括最新的视觉语言模型）主要依赖单图分析或外观推断，缺乏显式的几何推理能力，且对尺度歧义（Scale Ambiguity）高度敏感。此外，真实餐饮场景通常缺乏精确的度量标注或物理参照物。

Method: 将食物分量估计重新定义为单目观测下的“隐式比例3D重建”任务。数据集移除了显式物理参考，转而利用盘子、餐具等上下文物体作为比例推断的隐含线索。该基准涵盖了多食物场景、多样化几何形态、频繁遮挡及复杂的空间布局。

Result: 在MetaFood 2025 Workshop挑战赛中，几何重建方法表现优于视觉语言模型基准。顶级方案在体积估计上达到了0.21 MAPE（平均绝对百分比误差），在几何精度上达到了5.7 L1 Chamfer距离。

Conclusion: 基于几何的重建方法在食物分量估计中比纯视觉语言模型更具准确性和鲁棒性。实验证明，在缺乏显式物理参照的情况下，利用环境上下文进行隐式比例推断是实现高精度分量估计的有效途径。

Abstract: We present Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images, a benchmark dataset designed to advance geometry-based food portion estimation in realistic dining scenarios. Existing dietary assessment methods largely rely on single-image analysis or appearance-based inference, including recent vision-language models, which lack explicit geometric reasoning and are sensitive to scale ambiguity. This benchmark reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. To reflect real-world conditions, explicit physical references and metric annotations are removed; instead, contextual objects such as plates and utensils are provided, requiring algorithms to infer scale from implicit cues and prior knowledge. The dataset emphasizes multi-food scenes with diverse object geometries, frequent occlusions, and complex spatial arrangements. The benchmark was adopted as a challenge at the MetaFood 2025 Workshop, where multiple teams proposed reconstruction-based solutions. Experimental results show that while strong vision--language baselines achieve competitive performance, geometry-based reconstruction methods provide both improved accuracy and greater robustness, with the top-performing approach achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.

</details>


### [136] [A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models](https://arxiv.org/abs/2602.13066)
*Yash Deo,Yan Jia,Toni Lassila,Victoria J Hodge,Alejandro F Frang,Chenghao Qian,Siyuan Kang,Ibrahim Habli*

Main category: cs.CV

TL;DR: 本文提出了一种基于MRI基础模型的校准指标（ONI/MI），旨在通过多层相似度分析，精准检测医学图像生成模型对训练数据的记忆与重复问题，以保障隐私安全。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型在生成医学影像时容易复制训练数据（即产生记忆效应），这可能导致严重的个人隐私泄露风险。

Method: 提出一种逐样本校准指标，利用MRI基础模型提取特征，通过聚合多层白化最近邻（Whitened Nearest-Neighbor）相似度，计算得到有界的“过拟合/新颖性指数”（ONI）和“记忆指数”（MI）。

Result: 在三个包含受控重复样本的MRI数据集上，该指标展现出极强的鲁棒性和跨数据集的一致性，在样本层级的重复检测准确率接近完美。

Conclusion: 该指标为医学影像生成模型的隐私合规性和生成质量评估提供了一个可靠的量化工具，能有效识别数据泄露风险。

Abstract: Image generative models are known to duplicate images from the training data as part of their outputs, which can lead to privacy concerns when used for medical image generation. We propose a calibrated per-sample metric for detecting memorization and duplication of training data. Our metric uses image features extracted using an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to a bounded \emph{Overfit/Novelty Index} (ONI) and \emph{Memorization Index} (MI) scores. Across three MRI datasets with controlled duplication percentages and typical image augmentations, our metric robustly detects duplication and provides more consistent metric values across datasets. At the sample level, our metric achieves near-perfect detection of duplicates.

</details>


### [137] [SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery](https://arxiv.org/abs/2602.13067)
*Chunming Li,Shidong Wang,Tong Xin,Haofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出 SIEFormer，一种通过隐式图信号处理和显式傅里叶频域滤波来增强 Vision Transformer 特征表达能力的模型，在通用类别发现（GCD）任务中取得了 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过频谱分析重新解释视觉 Transformer（ViT）中的自注意力机制，并解决通用类别发现（GCD）任务中特征自适应能力不足的问题。

Method: 提出 SIEFormer 架构，包含两个核心分支：
1. **隐式分支（Implicit Branch）**：利用不同类型的图拉普拉斯算子建模 Token 间的局部结构相关性，并引入带通/带阻可调的带自适应滤波器（BaF）。
2. **显式分支（Explicit Branch）**：引入可操作滤波层（MFL），通过傅里叶变换将“Value”特征转至频域，利用可学习参数进行全局依赖调制，再通过逆变换恢复增强特征。

Result: 在多个图像识别数据集上达到了当前最先进（SOTA）的性能水平，消融实验和可视化结果进一步验证了该频谱增强方法的有效性。

Conclusion: SIEFormer 通过结合图拉普拉斯和傅里叶变换，为视觉 Transformer 提供了一个更具解释性且性能更强的频谱分析框架，显著提升了模型在复杂分类任务中的特征建模能力。

Abstract: This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value" features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.

</details>


### [138] [Universal Transformation of One-Class Classifiers for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.13091)
*Declan McIntosh,Alexandra Branzan Albu*

Main category: cs.CV

TL;DR: 本文提出一种通过“数据集折叠”将任意单类异常检测器转化为全无监督模型的方法，在无需修改原算法的前提下，利用多模型过滤机制剔除训练集噪声，并在多个权威数据集上取得 SOTA 效果。


<details>
  <summary>Details</summary>
Motivation: 传统的异常检测通常依赖于纯净的单类训练数据（即假设训练集全为正常样本），但在实际场景中，训练数据往往不可避免地包含噪声（异常值）。现有的单类分类方法对这类标签噪声非常敏感，因此需要一种能直接在含有噪声的数据集上进行训练的全无监督方法。

Method: 提出了一种名为“数据集折叠”（Dataset Folding）的方法。该方法基于“异常是稀少且异质的”这一假设，通过构建多个独立训练的单类分类器（OCC）实例来对原始训练集进行迭代过滤。该过程无需修改底层检测算法，仅通过算法化选择数据子集，即可将单类分类器转化为全无监督模型。

Result: 该方法在 MVTec AD、ViSA 和 MVTec LOCO AD 等主流数据集上达到了最先进（SOTA）的无监督检测性能，并成功创制了首个无监督逻辑异常检测器。实验证明，该框架能有效提升多种图像和视频异常检测器在无监督场景下的鲁棒性。

Conclusion: 该研究成功建立了单类分类与无监督异常检测之间的技术桥梁。其核心价值在于提供了一个通用的元算法框架，使得任何单类分类器的技术进步都能直接转化为无监督领域的性能提升，具有极强的泛化性和实用价值。

Abstract: Detecting anomalies in images and video is an essential task for multiple real-world problems, including industrial inspection, computer-assisted diagnosis, and environmental monitoring. Anomaly detection is typically formulated as a one-class classification problem, where the training data consists solely of nominal values, leaving methods built on this assumption susceptible to training label noise. We present a dataset folding method that transforms an arbitrary one-class classifier-based anomaly detector into a fully unsupervised method. This is achieved by making a set of key weak assumptions: that anomalies are uncommon in the training dataset and generally heterogeneous. These assumptions enable us to utilize multiple independently trained instances of a one-class classifier to filter the training dataset for anomalies. This transformation requires no modifications to the underlying anomaly detector; the only changes are algorithmically selected data subsets used for training. We demonstrate that our method can transform a wide variety of one-class classifier anomaly detectors for both images and videos into unsupervised ones. Our method creates the first unsupervised logical anomaly detectors by transforming existing methods. We also demonstrate that our method achieves state-of-the-art performance for unsupervised anomaly detection on the MVTec AD, ViSA, and MVTec Loco AD datasets. As improvements to one-class classifiers are made, our method directly transfers those improvements to the unsupervised domain, linking the domains.

</details>


### [139] [LongStream: Long-Sequence Streaming Autoregressive Visual Geometry](https://arxiv.org/abs/2602.13172)
*Chong Cheng,Xianda Chen,Tao Xie,Wei Yin,Weiqiang Ren,Qian Zhang,Xiaoyuang Guo,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出 LongStream，通过关键帧位姿解耦、尺度学习优化及 Transformer 缓存管理，解决了长序列流式 3D 重建中的漂移与衰减问题，实现了公里级场景的实时高精度重建。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在处理长序列流式 3D 重建时，由于过度依赖首帧锚定，容易导致注意力衰减、尺度漂移（Scale drift）以及外推误差，难以支持超长距离的实时重建。

Method: 本文提出 LongStream 模型，核心方法包括：1. **规范解耦位姿预测（Gauge-decoupled Poses）**：舍弃首帧锚定，采用关键帧相对位姿预测，将长程外推转化为常数难度的局部任务；2. **正交尺度学习（Orthogonal Scale Learning）**：将几何特征与尺度估计完全解耦以抑制漂移；3. **Transformer 缓存优化**：引入缓存一致性训练（Cache-consistent training）与周期性缓存刷新机制，解决注意力汇聚（Attention-sink）依赖和 KV 缓存污染问题。

Result: 实验证明，LongStream 达到了 SOTA 性能，能够在公里级序列上实现稳定的、度量尺度的（Metric-scale）场景重建，且推理速度可达 18 FPS。

Conclusion: LongStream 通过规范解耦机制与高效的缓存管理策略，有效克服了超长序列 3D 重建中的注意力退化与尺度漂移瓶颈，缩小了模型训练与推理之间的表现差距。

Abstract: Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/

</details>


### [140] [Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace](https://arxiv.org/abs/2602.13176)
*Seth Donahue,J. D. Peiffer,R. Tyler Richardson,Yishan Zhong,Shaun Q. Y. Tan,Benoit Marteau,Stephanie R. Russo,May D. Wang,R. James Cotton,Ross Chafetz*

Main category: cs.CV

TL;DR: 本研究验证了利用单目相机结合AI无标记动捕技术进行上肢可及空间（UERW）评估的临床可行性，发现正面视角拍摄能达到与传统标记系统相近的精度，大大降低了临床评估的成本和复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统的上肢可及空间（UERW）定量分析依赖复杂且昂贵的动作捕捉系统，限制了其在临床环境中的普及。本研究旨在验证基于单目相机和AI驱动的无标记动作捕捉（MMC）技术在临床UERW评估中的准确性与可行性。

Method: 招募9名无障碍成年受试者，佩戴VR头显执行标准UERW任务（触碰虚拟球面目标）。实验同步采用基于标记的动作捕捉系统（参考基准）和8台FLIR相机，通过AI驱动的无标记动捕（MMC）分析其中正面（Frontal）和偏置（Offset）两种单目视角，并进行对比验证。

Result: 正面相机配置与基于标记的基准系统表现出极强的一致性，每个象限的平均偏差仅为 $0.61 \pm 0.12\%$。相比之下，侧偏相机视角低估了可及空间百分比（偏差为 $-5.66 \pm 0.45\%$）。这是首个验证单目MMC系统用于分析UERW任务的研究。

Conclusion: 本研究证明了使用正面单目相机配置进行UERW评估的可行性，尤其在评估前侧空间时与基于标记的系统具有高度一致性。该方法显著降低了定量评估的技术门槛和复杂性，为临床上广泛开展上肢活动能力定量分析提供了可能。

Abstract: To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \pm 0.12$ \% reachspace reached per octanct (mean $\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \pm 0.45$ \% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.

</details>


### [141] [FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control](https://arxiv.org/abs/2602.13185)
*Mingzhi Sheng,Zekai Gu,Peng Li,Cheng Lin,Hao-Xiang Guo,Ying-Cong Chen,Yuan Liu*

Main category: cs.CV

TL;DR: FlexAM 是一种基于 3D 点云控制信号的统一视频生成框架，通过解耦外观与运动，实现了在 I2V、V2V、相机控制等多种任务上的高精度、通用化控制。


<details>
  <summary>Details</summary>
Motivation: 针对现有视频生成控制方法依赖模糊或特定任务信号、缺乏通用性的问题，探索通过解耦“外观”与“运动”来实现更鲁棒、可扩展的视频控制。

Method: 提出 FlexAM 框架，其核心是基于 3D 点云的视频动力学表征。技术创新包括：1. 多频率位置编码（用于区分细粒度运动）；2. 深度感知位置编码；3. 灵活控制信号机制（平衡生成精度与质量）。

Result: 实验结果表明，FlexAM 在图像转视频（I2V）、视频编辑（V2V）、相机控制及空间物体编辑等多项任务中均取得了优于现有方法的性能表现。

Conclusion: FlexAM 证明了通过 3D 点云表征实现“外观”与“运动”的解耦，是实现通用且高性能视频生成控制的有效途径，为该领域提供了新的研究方向。

Abstract: Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of "appearance" and "motion" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.

</details>


### [142] [Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision](https://arxiv.org/abs/2602.13195)
*Aadarsh Sahoo,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出了一种新的“会话式图像分割（CIS）”任务及其基准 ConverSeg，旨在解决包含功能和物理推理的抽象指令分割问题。通过配套的 ConverSeg-Net 模型和自动数据生成引擎，显著提升了模型在复杂意图理解下的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有的指代性图像分割（Referring Image Grounding）研究主要集中在类别和空间查询（如“最左边的苹果”），忽视了对功能、物理推理及意图驱动概念（如“哪里可以安全存放刀具”）的理解，导致模型在处理复杂的实际对话场景时能力受限。

Method: 1. 定义了会话式图像分割（CIS）任务；2. 构建了 **ConverSeg** 基准测试集，涵盖实体、空间、意图、功能、安全和物理推理等维度；3. 提出了 **ConverSeg-Net** 架构，将强大的分割先验与语言理解能力相结合；4. 开发了一个 **AI 驱动的数据引擎**，可在无需人工监督的情况下生成“提示-掩码”对。

Result: 实验结果表明，现有的语言引导分割模型在 CIS 任务上表现不足。而经过 AI 数据引擎训练的 ConverSeg-Net 在 ConverSeg 基准测试中取得了显著提升，同时在现有的传统分割基准上也保持了较强的性能。

Conclusion: 本研究通过引入新的基准测试 ConverSeg 和高性能模型 ConverSeg-Net，成功将图像分割从简单的指代定位扩展到了复杂的意图和功能推理层面，证明了结合语言理解与分割先验以及使用合成数据训练是解决会话式图像分割（CIS）的有效途径。

Abstract: Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., "left-most apple") and overlooks functional and physical reasoning (e.g., "where can I safely store the knife?"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/

</details>
